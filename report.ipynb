{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0b8e1d",
   "metadata": {},
   "source": [
    "# Rapport du projet de résolution de problème\n",
    "\n",
    "- Paul Achard\n",
    "- Julien Faure\n",
    "\n",
    "    \n",
    "- *Date : 20/01/2022*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1070770",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42622bf2",
   "metadata": {},
   "source": [
    "## Problématique\n",
    "\n",
    "Notre problématique est d'identifier un chiffre à partir d'une image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3a0c6",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "- Trouver un modèle permettant d'identifier un chiffre à partir d'une image\n",
    "- Comparer différentes stratégies de solveur pour le modèle trouvé\n",
    "\n",
    "## Analyse du dataset\n",
    "\n",
    "Identifier un chiffre à partir d'une image est une tache qui peut s'avérer très complexe. Afin d'avoir une difficulté raisonnable et adapté au contexte de ce projet, nous avons fixé certains paramètres dans notre dataset.\n",
    "\n",
    "- La résolution de nos images est identiques pour tout le dataset. Cette résolution est **8 pixels par 8 pixels**.\n",
    "- Chaque pixel est codé sur **4 bits**.\n",
    "- Les images contiennent uniquement un chiffre sans élément parasite, sans effet et sans traitement.\n",
    "\n",
    "Nous utilisons le dataset `digits` de `sklearn`.\n",
    "\n",
    "### Forme du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894968c2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data Shape (1797, 64)\n",
      "Label Data Shape (1797,)\n"
     ]
    }
   ],
   "source": [
    "# Import du dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Affiche le nombre d'images et leur format\n",
    "print(\"Image Data Shape\" , digits.data.shape)\n",
    "\n",
    "# Affiche le nombre de labels\n",
    "print(\"Label Data Shape\", digits.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311b24e7",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir ci-dessus, le dataset est composé de **1797** images labellisées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb1ba9",
   "metadata": {},
   "source": [
    "### Répartition des images du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1826bdc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Text(0.5, 0, 'n° labellisé'), Text(0, 0.5, 'Occurrence')]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphique = sns.countplot(x=digits.target)\n",
    "graphique.set(xlabel=\"n° labellisé\", ylabel = \"Occurrence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee617b1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Les données labellisées sont équitablement distribuées (environ 175 par label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc387e8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Représentation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d087b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x216 with 10 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Création de 10 figures de 10 par 3 pixels\n",
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 3))\n",
    "\n",
    "# Pour chaque figure, on affiche l'image du chiffre et son label en titre\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "    ax.set_title(\"Label: %i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e6cc1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Représentation des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb283e09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "\n",
      " Type de chaque valeur : <class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "# Affiche le tableau représentant la première image\n",
    "print(digits.images[0])\n",
    "print(\"\\n Type de chaque valeur :\", type(digits.images[0][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa72f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Comme nous l'avons vu précédemment, chaque image possède une résolution de 8x8 pixels en niveaux de gris codés sur 4bits par pixel.\n",
    "\n",
    "Dans notre programme, une image est représentée par une matrice de dimension 8x8. Chaque élément représente un pixel avec un niveau de gris codé sur 4bits (de 0 à 15). Plus la valeur est élevée, plus la couleur est foncée.\n",
    "\n",
    "> Exemple :\n",
    "* 0 : Blanc\n",
    "* 15 : Noir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3675a",
   "metadata": {},
   "source": [
    "\n",
    "## Modèles\n",
    "\n",
    "Pour résoudre notre problème, nous allons essayer les modèles suivants :\n",
    "* [Modèle de regréssion logistique multiclasse](#Mod%C3%A8le-de-regr%C3%A9ssion-logistique-multiclasse)\n",
    "* [Modèle de Bayes](#Mod%C3%A8le-de-Bayes)\n",
    "\n",
    "### Mise en forme des données d'entrées\n",
    "\n",
    "Pour commencer, nous devons redimenssionner nos données d'entrées.\n",
    "\n",
    "Actuellement, nous avons des données sous la forme d'un tableau de matrice.\n",
    "\n",
    "Il nous faut les mettre sous forme d'une matrice où chaque vecteur correspond aux pixels de l'image.\n",
    "\n",
    "Donc si nous avons 1797 images, la matrice d'entrée est composée de 1797 vecteurs.\n",
    "\n",
    "Avec une résolution de 8x8, la taille d'un vecteur est de 64 valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74b2574d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Redimensionne la matrice en vecteur\n",
    "print(digits.images[0].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e10f7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
    "\n",
    "# Redimenssionne le tableau de matrice en matrice\n",
    "x = digits.images.reshape((len(digits.images), -1))\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9331fa9",
   "metadata": {},
   "source": [
    "### Jeu de test et jeu d'entraînement\n",
    "\n",
    "Il nous faut maintenant séparer notre jeu de test et notre jeu d'entraînement du dataset.\n",
    "\n",
    "Pour choisir la taille de notre jeu de test, il est nécessaire de faire attention à plusieurs points :\n",
    "* Le temps d'entrainement de notre modèle\n",
    "* La taille de notre dataset\n",
    "\n",
    "Plus la proportion du jeu d'entraînement est faible, plus notre modèle à un niveau de variance élevé. Ainsi, on augmente les chances d'avoir de l'*over fitting*.\n",
    "\n",
    "A contrario, plus la proportion du jeu de test est faible, plus notre modèle à un niveau de variance faible. Ainsi, on augmente les chances d'avoir de l'*under fitting*.\n",
    "\n",
    "Le but est donc de trouver la valeur qui nous permet d'avoir le taux de variance optimal.\n",
    "\n",
    "\n",
    "Pour trouver cette valeur, nous avons appliqué la procédure suivante, nous avons essayé avec plusieurs valeurs en partant de 20% jusqu'à 80% avec un pas de 5%. Nous avons déterminé que la meilleure valeur est **35%**.\n",
    "\n",
    "\n",
    "> [https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio](https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de94e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# séparation du dataset en données \"d'apprentissage\" et de test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.35, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Création du tableau de résultat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                               Best Mean Test Score\nLogistic Regression (LR)                        0.0\nNaïve Bayes Classifier (NB)                     0.0\nDecision Tree Classifier (DT)                   0.0\nGradient Boosting (GB)                          0.0\nK Nearest Neighbours (KNN)                      0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Best Mean Test Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression (LR)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Naïve Bayes Classifier (NB)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Decision Tree Classifier (DT)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting (GB)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>K Nearest Neighbours (KNN)</th>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluation_Results = pd.DataFrame(np.zeros((5, 1)), columns=['Best Mean Test Score'])\n",
    "Evaluation_Results.index = ['Logistic Regression (LR)', 'Naïve Bayes Classifier (NB)',\n",
    "                            'Decision Tree Classifier (DT)', 'Gradient Boosting (GB)',\n",
    "                            'K Nearest Neighbours (KNN)']\n",
    "Evaluation_Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "334ad595",
   "metadata": {},
   "source": [
    "### Modèle de regréssion logistique multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc863948",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 18 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.97231332 0.97231332 0.96489613 0.96659819 0.95546507        nan\n",
      " 0.96917575 0.96946187 0.95748138 0.96659819 0.95546507        nan\n",
      " 0.96917697 0.96860595 0.95633934 0.96659819 0.95546507        nan]\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [1.         1.         0.99600438 1.         1.                nan\n",
      " 1.         1.         0.99978594 1.         1.                nan\n",
      " 1.         1.         1.         1.         1.                nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.958664546899841"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création du modèle de régression\n",
    "LR_model = LogisticRegression(verbose=False, max_iter=10000)\n",
    "\n",
    "# Dictionnaire contenant les différents paramètres à essayer\n",
    "LR_params = dict()\n",
    "LR_params['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "LR_params['penalty'] = ['l2', 'none']\n",
    "LR_params['C'] = [0.1, 5, 10]\n",
    "\n",
    "# Création de nos itérations\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "# Création de notre modèle de validation croisée\n",
    "model_cv = GridSearchCV(estimator=LR_model,\n",
    "                        param_grid=LR_params,\n",
    "                        scoring='accuracy',\n",
    "                        cv=kFold,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True)\n",
    "\n",
    "# Entrainement du modèle\n",
    "model_cv.fit(x_train, y_train)\n",
    "# cv results\n",
    "pd.set_option('display.max_columns', None)\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[0]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31144b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "La validation croisée nous permet de determiner les paramètres optimaux pour notre modèle d'apprentissage.\n",
    "Les résultats de nos tests avec de nombreux paramètres différents (dont la plupart ne sont pas présent dans l'exemple ci-dessus).\n",
    "* Le **paramètre C** correspond à l'inverse de la force de régularisation des données. Plus ce paramètre est grand, plus le risque d'overfitting est grand. Une valeur de 0.1 semble être optimale.\n",
    "* Le **paramètre de pénalité** permet de réduire les coefficients θ. On remarque une perte de précision si l'on n'applique pas de pénalité. La meilleure pénalité semble être la norme L2.\n",
    "* Le **solveur** correspond à l'algorithme d'optimisation utilisé pour l'entrainement. Dans notre cas, lbfgs permet d'obtenir la precision la plus élevée malgré un temps d'entrainement significativement plus long que ses concurrents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b3aae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Classification naïve bayésienne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9833fcac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n0       0.008136      0.007135         0.000731        0.000441           0   \n1       0.003264      0.000440         0.000536        0.000501      0.0001   \n2       0.004066      0.002234         0.001198        0.000542       0.001   \n3       0.005335      0.006183         0.000733        0.000442        0.01   \n4       0.002932      0.001179         0.000799        0.000541         0.1   \n5       0.006201      0.005811         0.001865        0.002919         0.5   \n6       0.003132      0.000340         0.000801        0.000401         1.0   \n\n              params  split0_test_score  split1_test_score  split2_test_score  \\\n0       {'alpha': 0}           0.837607           0.871795           0.854701   \n1  {'alpha': 0.0001}           0.841880           0.884615           0.854701   \n2   {'alpha': 0.001}           0.846154           0.884615           0.854701   \n3    {'alpha': 0.01}           0.850427           0.884615           0.854701   \n4     {'alpha': 0.1}           0.854701           0.880342           0.854701   \n5     {'alpha': 0.5}           0.850427           0.876068           0.850427   \n6     {'alpha': 1.0}           0.850427           0.871795           0.850427   \n\n   split3_test_score  split4_test_score  split5_test_score  split6_test_score  \\\n0           0.836910           0.845494           0.871795           0.850427   \n1           0.849785           0.854077           0.876068           0.880342   \n2           0.849785           0.858369           0.876068           0.884615   \n3           0.849785           0.858369           0.867521           0.876068   \n4           0.849785           0.858369           0.863248           0.871795   \n5           0.845494           0.845494           0.858974           0.867521   \n6           0.841202           0.845494           0.858974           0.867521   \n\n   split7_test_score  split8_test_score  split9_test_score  \\\n0           0.854701           0.819742           0.875536   \n1           0.863248           0.828326           0.879828   \n2           0.867521           0.836910           0.875536   \n3           0.867521           0.841202           0.875536   \n4           0.867521           0.841202           0.884120   \n5           0.867521           0.836910           0.884120   \n6           0.867521           0.832618           0.879828   \n\n   split10_test_score  split11_test_score  split12_test_score  \\\n0            0.833333            0.854701            0.858974   \n1            0.854701            0.858974            0.867521   \n2            0.854701            0.858974            0.867521   \n3            0.858974            0.858974            0.863248   \n4            0.867521            0.858974            0.854701   \n5            0.871795            0.850427            0.854701   \n6            0.871795            0.841880            0.854701   \n\n   split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0            0.866953            0.871245         0.853594        0.015985   \n1            0.871245            0.884120         0.863296        0.015941   \n2            0.871245            0.884120         0.864722        0.014346   \n3            0.871245            0.884120         0.864154        0.012277   \n4            0.871245            0.888412         0.864442        0.012759   \n5            0.854077            0.875536         0.859300        0.013215   \n6            0.854077            0.875536         0.857587        0.013731   \n\n   rank_test_score  \n0                7  \n1                4  \n2                1  \n3                3  \n4                2  \n5                5  \n6                6  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_alpha</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.008136</td>\n      <td>0.007135</td>\n      <td>0.000731</td>\n      <td>0.000441</td>\n      <td>0</td>\n      <td>{'alpha': 0}</td>\n      <td>0.837607</td>\n      <td>0.871795</td>\n      <td>0.854701</td>\n      <td>0.836910</td>\n      <td>0.845494</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.854701</td>\n      <td>0.819742</td>\n      <td>0.875536</td>\n      <td>0.833333</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.866953</td>\n      <td>0.871245</td>\n      <td>0.853594</td>\n      <td>0.015985</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.003264</td>\n      <td>0.000440</td>\n      <td>0.000536</td>\n      <td>0.000501</td>\n      <td>0.0001</td>\n      <td>{'alpha': 0.0001}</td>\n      <td>0.841880</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.854077</td>\n      <td>0.876068</td>\n      <td>0.880342</td>\n      <td>0.863248</td>\n      <td>0.828326</td>\n      <td>0.879828</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.863296</td>\n      <td>0.015941</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.004066</td>\n      <td>0.002234</td>\n      <td>0.001198</td>\n      <td>0.000542</td>\n      <td>0.001</td>\n      <td>{'alpha': 0.001}</td>\n      <td>0.846154</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.876068</td>\n      <td>0.884615</td>\n      <td>0.867521</td>\n      <td>0.836910</td>\n      <td>0.875536</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.864722</td>\n      <td>0.014346</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.005335</td>\n      <td>0.006183</td>\n      <td>0.000733</td>\n      <td>0.000442</td>\n      <td>0.01</td>\n      <td>{'alpha': 0.01}</td>\n      <td>0.850427</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.867521</td>\n      <td>0.876068</td>\n      <td>0.867521</td>\n      <td>0.841202</td>\n      <td>0.875536</td>\n      <td>0.858974</td>\n      <td>0.858974</td>\n      <td>0.863248</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.864154</td>\n      <td>0.012277</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.002932</td>\n      <td>0.001179</td>\n      <td>0.000799</td>\n      <td>0.000541</td>\n      <td>0.1</td>\n      <td>{'alpha': 0.1}</td>\n      <td>0.854701</td>\n      <td>0.880342</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.863248</td>\n      <td>0.871795</td>\n      <td>0.867521</td>\n      <td>0.841202</td>\n      <td>0.884120</td>\n      <td>0.867521</td>\n      <td>0.858974</td>\n      <td>0.854701</td>\n      <td>0.871245</td>\n      <td>0.888412</td>\n      <td>0.864442</td>\n      <td>0.012759</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.006201</td>\n      <td>0.005811</td>\n      <td>0.001865</td>\n      <td>0.002919</td>\n      <td>0.5</td>\n      <td>{'alpha': 0.5}</td>\n      <td>0.850427</td>\n      <td>0.876068</td>\n      <td>0.850427</td>\n      <td>0.845494</td>\n      <td>0.845494</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.867521</td>\n      <td>0.836910</td>\n      <td>0.884120</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.854701</td>\n      <td>0.854077</td>\n      <td>0.875536</td>\n      <td>0.859300</td>\n      <td>0.013215</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.003132</td>\n      <td>0.000340</td>\n      <td>0.000801</td>\n      <td>0.000401</td>\n      <td>1.0</td>\n      <td>{'alpha': 1.0}</td>\n      <td>0.850427</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.841202</td>\n      <td>0.845494</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.867521</td>\n      <td>0.832618</td>\n      <td>0.879828</td>\n      <td>0.871795</td>\n      <td>0.841880</td>\n      <td>0.854701</td>\n      <td>0.854077</td>\n      <td>0.875536</td>\n      <td>0.857587</td>\n      <td>0.013731</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "NB_model = BernoulliNB()\n",
    "\n",
    "NB_params = {'alpha': [0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0]}\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(NB_model, NB_params, n_iter=7, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "# NB = RCV.fit(x_train, y_train).best_estimator_\n",
    "RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(RCV.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[1]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le seul paramètre testé dans ce modèle, alpha, correspond à la force du lissage de Laplace a appliqué au modèle.\n",
    "En d'autres termes, ce paramètre permet de palier la présence d'une caractéristique dans le jeu de test qui n'existe pas dans le jeu d'entrainement.\n",
    "> https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece\n",
    "\n",
    "Au vu des résultats, le paramètre alpha semble être optimal autour de 0.001\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification par arbre de décision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        0.006196      0.001103         0.000735        0.000443   \n1        0.002732      0.000998         0.000468        0.000500   \n2        0.005932      0.001063         0.000467        0.000499   \n3        0.002399      0.000489         0.000733        0.000929   \n4        0.005599      0.000489         0.000600        0.000611   \n5        0.002465      0.000621         0.000669        0.000473   \n6        0.005796      0.000543         0.000334        0.000473   \n7        0.002596      0.000800         0.000468        0.000501   \n8        0.015198      0.003101         0.000667        0.000597   \n9        0.005266      0.000574         0.000600        0.000490   \n10       0.014064      0.003957         0.000602        0.000613   \n11       0.006066      0.004555         0.000533        0.000499   \n12       0.015130      0.004557         0.000268        0.000445   \n13       0.004732      0.000442         0.000465        0.000497   \n14       0.012198      0.000656         0.000335        0.000473   \n15       0.004731      0.000573         0.000401        0.000491   \n16       0.007067      0.000570         0.000532        0.000498   \n17       0.002734      0.000571         0.000201        0.000401   \n18       0.007998      0.002634         0.000533        0.000498   \n19       0.002132      0.000339         0.000666        0.000471   \n20       0.007399      0.000609         0.000201        0.000401   \n21       0.002733      0.000574         0.000133        0.000339   \n22       0.007601      0.001145         0.000467        0.000499   \n23       0.002266      0.000442         0.000266        0.000440   \n24       0.021533      0.005377         0.000868        0.001456   \n25       0.008395      0.003931         0.000334        0.000472   \n26       0.017797      0.002836         0.000467        0.000499   \n27       0.005602      0.000490         0.000467        0.000499   \n28       0.016263      0.001569         0.000801        0.000401   \n29       0.005799      0.001760         0.000599        0.000489   \n30       0.016465      0.002161         0.000401        0.000492   \n31       0.005465      0.001088         0.000533        0.000499   \n\n   param_splitter param_min_samples_leaf param_max_depth param_criterion  \\\n0            best                      1               3            gini   \n1          random                      1               3            gini   \n2            best                      2               3            gini   \n3          random                      2               3            gini   \n4            best                      3               3            gini   \n5          random                      3               3            gini   \n6            best                      4               3            gini   \n7          random                      4               3            gini   \n8            best                      1            None            gini   \n9          random                      1            None            gini   \n10           best                      2            None            gini   \n11         random                      2            None            gini   \n12           best                      3            None            gini   \n13         random                      3            None            gini   \n14           best                      4            None            gini   \n15         random                      4            None            gini   \n16           best                      1               3         entropy   \n17         random                      1               3         entropy   \n18           best                      2               3         entropy   \n19         random                      2               3         entropy   \n20           best                      3               3         entropy   \n21         random                      3               3         entropy   \n22           best                      4               3         entropy   \n23         random                      4               3         entropy   \n24           best                      1            None         entropy   \n25         random                      1            None         entropy   \n26           best                      2            None         entropy   \n27         random                      2            None         entropy   \n28           best                      3            None         entropy   \n29         random                      3            None         entropy   \n30           best                      4            None         entropy   \n31         random                      4            None         entropy   \n\n                                               params  split0_test_score  \\\n0   {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.410256   \n1   {'splitter': 'random', 'min_samples_leaf': 1, ...           0.431624   \n2   {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.410256   \n3   {'splitter': 'random', 'min_samples_leaf': 2, ...           0.414530   \n4   {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.410256   \n5   {'splitter': 'random', 'min_samples_leaf': 3, ...           0.547009   \n6   {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.410256   \n7   {'splitter': 'random', 'min_samples_leaf': 4, ...           0.461538   \n8   {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.773504   \n9   {'splitter': 'random', 'min_samples_leaf': 1, ...           0.752137   \n10  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.747863   \n11  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.764957   \n12  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.739316   \n13  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.799145   \n14  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.739316   \n15  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.756410   \n16  {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.495726   \n17  {'splitter': 'random', 'min_samples_leaf': 1, ...           0.440171   \n18  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.495726   \n19  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.551282   \n20  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.495726   \n21  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.504274   \n22  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.495726   \n23  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.508547   \n24  {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.799145   \n25  {'splitter': 'random', 'min_samples_leaf': 1, ...           0.790598   \n26  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.747863   \n27  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.773504   \n28  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.760684   \n29  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.760684   \n30  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.769231   \n31  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.777778   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.470085           0.410256           0.484979   \n1            0.487179           0.461538           0.493562   \n2            0.470085           0.410256           0.484979   \n3            0.491453           0.564103           0.506438   \n4            0.470085           0.410256           0.484979   \n5            0.491453           0.457265           0.570815   \n6            0.470085           0.410256           0.484979   \n7            0.559829           0.619658           0.450644   \n8            0.858974           0.854701           0.806867   \n9            0.863248           0.833333           0.811159   \n10           0.850427           0.854701           0.798283   \n11           0.837607           0.854701           0.772532   \n12           0.846154           0.837607           0.793991   \n13           0.807692           0.841880           0.819742   \n14           0.833333           0.837607           0.793991   \n15           0.786325           0.829060           0.802575   \n16           0.529915           0.555556           0.575107   \n17           0.525641           0.487179           0.536481   \n18           0.529915           0.555556           0.575107   \n19           0.470085           0.542735           0.587983   \n20           0.529915           0.555556           0.575107   \n21           0.581197           0.534188           0.489270   \n22           0.529915           0.555556           0.575107   \n23           0.594017           0.525641           0.493562   \n24           0.884615           0.841880           0.854077   \n25           0.867521           0.876068           0.811159   \n26           0.846154           0.850427           0.845494   \n27           0.837607           0.829060           0.828326   \n28           0.863248           0.807692           0.832618   \n29           0.794872           0.841880           0.806867   \n30           0.871795           0.816239           0.832618   \n31           0.824786           0.799145           0.798283   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.463519           0.423077           0.457265   \n1            0.587983           0.594017           0.431624   \n2            0.463519           0.423077           0.457265   \n3            0.407725           0.495726           0.521368   \n4            0.463519           0.423077           0.457265   \n5            0.484979           0.534188           0.517094   \n6            0.463519           0.423077           0.457265   \n7            0.540773           0.495726           0.491453   \n8            0.806867           0.824786           0.811966   \n9            0.789700           0.837607           0.850427   \n10           0.793991           0.816239           0.846154   \n11           0.811159           0.820513           0.837607   \n12           0.776824           0.841880           0.833333   \n13           0.824034           0.799145           0.841880   \n14           0.755365           0.829060           0.824786   \n15           0.742489           0.773504           0.807692   \n16           0.553648           0.576923           0.538462   \n17           0.506438           0.542735           0.555556   \n18           0.553648           0.576923           0.538462   \n19           0.506438           0.500000           0.555556   \n20           0.553648           0.576923           0.538462   \n21           0.497854           0.542735           0.555556   \n22           0.553648           0.576923           0.538462   \n23           0.493562           0.572650           0.500000   \n24           0.815451           0.850427           0.811966   \n25           0.798283           0.850427           0.794872   \n26           0.819742           0.841880           0.816239   \n27           0.858369           0.837607           0.858974   \n28           0.845494           0.846154           0.790598   \n29           0.802575           0.850427           0.820513   \n30           0.819742           0.837607           0.803419   \n31           0.802575           0.803419           0.841880   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.457265           0.472103           0.467811   \n1            0.427350           0.527897           0.545064   \n2            0.457265           0.472103           0.467811   \n3            0.508547           0.510730           0.412017   \n4            0.457265           0.472103           0.467811   \n5            0.478632           0.420601           0.510730   \n6            0.457265           0.472103           0.472103   \n7            0.547009           0.515021           0.480687   \n8            0.799145           0.776824           0.772532   \n9            0.833333           0.789700           0.819742   \n10           0.824786           0.751073           0.759657   \n11           0.854701           0.793991           0.772532   \n12           0.799145           0.755365           0.763948   \n13           0.846154           0.824034           0.751073   \n14           0.803419           0.772532           0.755365   \n15           0.820513           0.721030           0.789700   \n16           0.581197           0.532189           0.566524   \n17           0.534188           0.536481           0.506438   \n18           0.581197           0.532189           0.566524   \n19           0.568376           0.519313           0.570815   \n20           0.581197           0.532189           0.566524   \n21           0.512821           0.489270           0.502146   \n22           0.581197           0.532189           0.566524   \n23           0.581197           0.506438           0.557940   \n24           0.850427           0.772532           0.849785   \n25           0.858974           0.781116           0.828326   \n26           0.863248           0.751073           0.793991   \n27           0.841880           0.824034           0.845494   \n28           0.846154           0.768240           0.836910   \n29           0.807692           0.785408           0.828326   \n30           0.846154           0.746781           0.828326   \n31           0.803419           0.768240           0.836910   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.423077            0.452991            0.465812   \n1             0.444444            0.457265            0.602564   \n2             0.423077            0.452991            0.465812   \n3             0.525641            0.478632            0.452991   \n4             0.423077            0.452991            0.465812   \n5             0.414530            0.478632            0.461538   \n6             0.423077            0.452991            0.465812   \n7             0.457265            0.491453            0.482906   \n8             0.794872            0.841880            0.833333   \n9             0.816239            0.816239            0.858974   \n10            0.790598            0.837607            0.829060   \n11            0.777778            0.752137            0.811966   \n12            0.743590            0.820513            0.829060   \n13            0.790598            0.824786            0.829060   \n14            0.747863            0.811966            0.807692   \n15            0.794872            0.799145            0.829060   \n16            0.559829            0.576923            0.581197   \n17            0.542735            0.504274            0.491453   \n18            0.559829            0.576923            0.581197   \n19            0.529915            0.500000            0.482906   \n20            0.559829            0.576923            0.581197   \n21            0.491453            0.581197            0.500000   \n22            0.559829            0.576923            0.581197   \n23            0.461538            0.500000            0.521368   \n24            0.769231            0.901709            0.799145   \n25            0.807692            0.863248            0.858974   \n26            0.773504            0.884615            0.756410   \n27            0.811966            0.867521            0.850427   \n28            0.769231            0.905983            0.799145   \n29            0.824786            0.841880            0.833333   \n30            0.782051            0.871795            0.777778   \n31            0.803419            0.829060            0.799145   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.493562            0.446352         0.453227        0.024958   \n1             0.519313            0.480687         0.499474        0.058809   \n2             0.493562            0.446352         0.453227        0.024958   \n3             0.545064            0.407725         0.482846        0.050268   \n4             0.493562            0.446352         0.453227        0.024958   \n5             0.510730            0.540773         0.494598        0.043411   \n6             0.493562            0.446352         0.453514        0.025147   \n7             0.536481            0.459227         0.505978        0.045478   \n8             0.854077            0.815451         0.815052        0.028252   \n9             0.824034            0.802575         0.819897        0.028105   \n10            0.849785            0.793991         0.809614        0.035400   \n11            0.798283            0.751073         0.800769        0.034105   \n12            0.828326            0.785408         0.799631        0.035980   \n13            0.811159            0.768240         0.811908        0.026065   \n14            0.849785            0.776824         0.795927        0.034777   \n15            0.832618            0.785408         0.791360        0.031410   \n16            0.549356            0.545064         0.554508        0.023036   \n17            0.484979            0.527897         0.514843        0.029262   \n18            0.549356            0.545064         0.554508        0.023036   \n19            0.540773            0.446352         0.524835        0.038952   \n20            0.549356            0.545064         0.554508        0.023036   \n21            0.523605            0.484979         0.519370        0.031536   \n22            0.549356            0.545064         0.554508        0.023036   \n23            0.587983            0.467811         0.524817        0.041950   \n24            0.854077            0.802575         0.830470        0.037271   \n25            0.824034            0.802575         0.827591        0.031059   \n26            0.828326            0.811159         0.815342        0.041196   \n27            0.798283            0.763948         0.828467        0.029276   \n28            0.828326            0.806867         0.820490        0.038463   \n29            0.862661            0.781116         0.816201        0.027378   \n30            0.871245            0.789700         0.817632        0.037858   \n31            0.841202            0.789700         0.807931        0.021578   \n\n    rank_test_score  \n0                30  \n1                26  \n2                30  \n3                28  \n4                30  \n5                27  \n6                29  \n7                25  \n8                 9  \n9                 5  \n10               11  \n11               13  \n12               14  \n13               10  \n14               15  \n15               16  \n16               17  \n17               24  \n18               17  \n19               21  \n20               17  \n21               23  \n22               17  \n23               22  \n24                1  \n25                3  \n26                8  \n27                2  \n28                4  \n29                7  \n30                6  \n31               12  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_splitter</th>\n      <th>param_min_samples_leaf</th>\n      <th>param_max_depth</th>\n      <th>param_criterion</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.006196</td>\n      <td>0.001103</td>\n      <td>0.000735</td>\n      <td>0.000443</td>\n      <td>best</td>\n      <td>1</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453227</td>\n      <td>0.024958</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.002732</td>\n      <td>0.000998</td>\n      <td>0.000468</td>\n      <td>0.000500</td>\n      <td>random</td>\n      <td>1</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.431624</td>\n      <td>0.487179</td>\n      <td>0.461538</td>\n      <td>0.493562</td>\n      <td>0.587983</td>\n      <td>0.594017</td>\n      <td>0.431624</td>\n      <td>0.427350</td>\n      <td>0.527897</td>\n      <td>0.545064</td>\n      <td>0.444444</td>\n      <td>0.457265</td>\n      <td>0.602564</td>\n      <td>0.519313</td>\n      <td>0.480687</td>\n      <td>0.499474</td>\n      <td>0.058809</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.005932</td>\n      <td>0.001063</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>2</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453227</td>\n      <td>0.024958</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.002399</td>\n      <td>0.000489</td>\n      <td>0.000733</td>\n      <td>0.000929</td>\n      <td>random</td>\n      <td>2</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.414530</td>\n      <td>0.491453</td>\n      <td>0.564103</td>\n      <td>0.506438</td>\n      <td>0.407725</td>\n      <td>0.495726</td>\n      <td>0.521368</td>\n      <td>0.508547</td>\n      <td>0.510730</td>\n      <td>0.412017</td>\n      <td>0.525641</td>\n      <td>0.478632</td>\n      <td>0.452991</td>\n      <td>0.545064</td>\n      <td>0.407725</td>\n      <td>0.482846</td>\n      <td>0.050268</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.005599</td>\n      <td>0.000489</td>\n      <td>0.000600</td>\n      <td>0.000611</td>\n      <td>best</td>\n      <td>3</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453227</td>\n      <td>0.024958</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.002465</td>\n      <td>0.000621</td>\n      <td>0.000669</td>\n      <td>0.000473</td>\n      <td>random</td>\n      <td>3</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.547009</td>\n      <td>0.491453</td>\n      <td>0.457265</td>\n      <td>0.570815</td>\n      <td>0.484979</td>\n      <td>0.534188</td>\n      <td>0.517094</td>\n      <td>0.478632</td>\n      <td>0.420601</td>\n      <td>0.510730</td>\n      <td>0.414530</td>\n      <td>0.478632</td>\n      <td>0.461538</td>\n      <td>0.510730</td>\n      <td>0.540773</td>\n      <td>0.494598</td>\n      <td>0.043411</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.005796</td>\n      <td>0.000543</td>\n      <td>0.000334</td>\n      <td>0.000473</td>\n      <td>best</td>\n      <td>4</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.472103</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453514</td>\n      <td>0.025147</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.002596</td>\n      <td>0.000800</td>\n      <td>0.000468</td>\n      <td>0.000501</td>\n      <td>random</td>\n      <td>4</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.461538</td>\n      <td>0.559829</td>\n      <td>0.619658</td>\n      <td>0.450644</td>\n      <td>0.540773</td>\n      <td>0.495726</td>\n      <td>0.491453</td>\n      <td>0.547009</td>\n      <td>0.515021</td>\n      <td>0.480687</td>\n      <td>0.457265</td>\n      <td>0.491453</td>\n      <td>0.482906</td>\n      <td>0.536481</td>\n      <td>0.459227</td>\n      <td>0.505978</td>\n      <td>0.045478</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.015198</td>\n      <td>0.003101</td>\n      <td>0.000667</td>\n      <td>0.000597</td>\n      <td>best</td>\n      <td>1</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.773504</td>\n      <td>0.858974</td>\n      <td>0.854701</td>\n      <td>0.806867</td>\n      <td>0.806867</td>\n      <td>0.824786</td>\n      <td>0.811966</td>\n      <td>0.799145</td>\n      <td>0.776824</td>\n      <td>0.772532</td>\n      <td>0.794872</td>\n      <td>0.841880</td>\n      <td>0.833333</td>\n      <td>0.854077</td>\n      <td>0.815451</td>\n      <td>0.815052</td>\n      <td>0.028252</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.005266</td>\n      <td>0.000574</td>\n      <td>0.000600</td>\n      <td>0.000490</td>\n      <td>random</td>\n      <td>1</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.752137</td>\n      <td>0.863248</td>\n      <td>0.833333</td>\n      <td>0.811159</td>\n      <td>0.789700</td>\n      <td>0.837607</td>\n      <td>0.850427</td>\n      <td>0.833333</td>\n      <td>0.789700</td>\n      <td>0.819742</td>\n      <td>0.816239</td>\n      <td>0.816239</td>\n      <td>0.858974</td>\n      <td>0.824034</td>\n      <td>0.802575</td>\n      <td>0.819897</td>\n      <td>0.028105</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.014064</td>\n      <td>0.003957</td>\n      <td>0.000602</td>\n      <td>0.000613</td>\n      <td>best</td>\n      <td>2</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.747863</td>\n      <td>0.850427</td>\n      <td>0.854701</td>\n      <td>0.798283</td>\n      <td>0.793991</td>\n      <td>0.816239</td>\n      <td>0.846154</td>\n      <td>0.824786</td>\n      <td>0.751073</td>\n      <td>0.759657</td>\n      <td>0.790598</td>\n      <td>0.837607</td>\n      <td>0.829060</td>\n      <td>0.849785</td>\n      <td>0.793991</td>\n      <td>0.809614</td>\n      <td>0.035400</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.006066</td>\n      <td>0.004555</td>\n      <td>0.000533</td>\n      <td>0.000499</td>\n      <td>random</td>\n      <td>2</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.764957</td>\n      <td>0.837607</td>\n      <td>0.854701</td>\n      <td>0.772532</td>\n      <td>0.811159</td>\n      <td>0.820513</td>\n      <td>0.837607</td>\n      <td>0.854701</td>\n      <td>0.793991</td>\n      <td>0.772532</td>\n      <td>0.777778</td>\n      <td>0.752137</td>\n      <td>0.811966</td>\n      <td>0.798283</td>\n      <td>0.751073</td>\n      <td>0.800769</td>\n      <td>0.034105</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.015130</td>\n      <td>0.004557</td>\n      <td>0.000268</td>\n      <td>0.000445</td>\n      <td>best</td>\n      <td>3</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.739316</td>\n      <td>0.846154</td>\n      <td>0.837607</td>\n      <td>0.793991</td>\n      <td>0.776824</td>\n      <td>0.841880</td>\n      <td>0.833333</td>\n      <td>0.799145</td>\n      <td>0.755365</td>\n      <td>0.763948</td>\n      <td>0.743590</td>\n      <td>0.820513</td>\n      <td>0.829060</td>\n      <td>0.828326</td>\n      <td>0.785408</td>\n      <td>0.799631</td>\n      <td>0.035980</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.004732</td>\n      <td>0.000442</td>\n      <td>0.000465</td>\n      <td>0.000497</td>\n      <td>random</td>\n      <td>3</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.799145</td>\n      <td>0.807692</td>\n      <td>0.841880</td>\n      <td>0.819742</td>\n      <td>0.824034</td>\n      <td>0.799145</td>\n      <td>0.841880</td>\n      <td>0.846154</td>\n      <td>0.824034</td>\n      <td>0.751073</td>\n      <td>0.790598</td>\n      <td>0.824786</td>\n      <td>0.829060</td>\n      <td>0.811159</td>\n      <td>0.768240</td>\n      <td>0.811908</td>\n      <td>0.026065</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.012198</td>\n      <td>0.000656</td>\n      <td>0.000335</td>\n      <td>0.000473</td>\n      <td>best</td>\n      <td>4</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.739316</td>\n      <td>0.833333</td>\n      <td>0.837607</td>\n      <td>0.793991</td>\n      <td>0.755365</td>\n      <td>0.829060</td>\n      <td>0.824786</td>\n      <td>0.803419</td>\n      <td>0.772532</td>\n      <td>0.755365</td>\n      <td>0.747863</td>\n      <td>0.811966</td>\n      <td>0.807692</td>\n      <td>0.849785</td>\n      <td>0.776824</td>\n      <td>0.795927</td>\n      <td>0.034777</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.004731</td>\n      <td>0.000573</td>\n      <td>0.000401</td>\n      <td>0.000491</td>\n      <td>random</td>\n      <td>4</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.756410</td>\n      <td>0.786325</td>\n      <td>0.829060</td>\n      <td>0.802575</td>\n      <td>0.742489</td>\n      <td>0.773504</td>\n      <td>0.807692</td>\n      <td>0.820513</td>\n      <td>0.721030</td>\n      <td>0.789700</td>\n      <td>0.794872</td>\n      <td>0.799145</td>\n      <td>0.829060</td>\n      <td>0.832618</td>\n      <td>0.785408</td>\n      <td>0.791360</td>\n      <td>0.031410</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.007067</td>\n      <td>0.000570</td>\n      <td>0.000532</td>\n      <td>0.000498</td>\n      <td>best</td>\n      <td>1</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.002734</td>\n      <td>0.000571</td>\n      <td>0.000201</td>\n      <td>0.000401</td>\n      <td>random</td>\n      <td>1</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.440171</td>\n      <td>0.525641</td>\n      <td>0.487179</td>\n      <td>0.536481</td>\n      <td>0.506438</td>\n      <td>0.542735</td>\n      <td>0.555556</td>\n      <td>0.534188</td>\n      <td>0.536481</td>\n      <td>0.506438</td>\n      <td>0.542735</td>\n      <td>0.504274</td>\n      <td>0.491453</td>\n      <td>0.484979</td>\n      <td>0.527897</td>\n      <td>0.514843</td>\n      <td>0.029262</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.007998</td>\n      <td>0.002634</td>\n      <td>0.000533</td>\n      <td>0.000498</td>\n      <td>best</td>\n      <td>2</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.002132</td>\n      <td>0.000339</td>\n      <td>0.000666</td>\n      <td>0.000471</td>\n      <td>random</td>\n      <td>2</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.551282</td>\n      <td>0.470085</td>\n      <td>0.542735</td>\n      <td>0.587983</td>\n      <td>0.506438</td>\n      <td>0.500000</td>\n      <td>0.555556</td>\n      <td>0.568376</td>\n      <td>0.519313</td>\n      <td>0.570815</td>\n      <td>0.529915</td>\n      <td>0.500000</td>\n      <td>0.482906</td>\n      <td>0.540773</td>\n      <td>0.446352</td>\n      <td>0.524835</td>\n      <td>0.038952</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.007399</td>\n      <td>0.000609</td>\n      <td>0.000201</td>\n      <td>0.000401</td>\n      <td>best</td>\n      <td>3</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.002733</td>\n      <td>0.000574</td>\n      <td>0.000133</td>\n      <td>0.000339</td>\n      <td>random</td>\n      <td>3</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.504274</td>\n      <td>0.581197</td>\n      <td>0.534188</td>\n      <td>0.489270</td>\n      <td>0.497854</td>\n      <td>0.542735</td>\n      <td>0.555556</td>\n      <td>0.512821</td>\n      <td>0.489270</td>\n      <td>0.502146</td>\n      <td>0.491453</td>\n      <td>0.581197</td>\n      <td>0.500000</td>\n      <td>0.523605</td>\n      <td>0.484979</td>\n      <td>0.519370</td>\n      <td>0.031536</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.007601</td>\n      <td>0.001145</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>4</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.002266</td>\n      <td>0.000442</td>\n      <td>0.000266</td>\n      <td>0.000440</td>\n      <td>random</td>\n      <td>4</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.508547</td>\n      <td>0.594017</td>\n      <td>0.525641</td>\n      <td>0.493562</td>\n      <td>0.493562</td>\n      <td>0.572650</td>\n      <td>0.500000</td>\n      <td>0.581197</td>\n      <td>0.506438</td>\n      <td>0.557940</td>\n      <td>0.461538</td>\n      <td>0.500000</td>\n      <td>0.521368</td>\n      <td>0.587983</td>\n      <td>0.467811</td>\n      <td>0.524817</td>\n      <td>0.041950</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.021533</td>\n      <td>0.005377</td>\n      <td>0.000868</td>\n      <td>0.001456</td>\n      <td>best</td>\n      <td>1</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.799145</td>\n      <td>0.884615</td>\n      <td>0.841880</td>\n      <td>0.854077</td>\n      <td>0.815451</td>\n      <td>0.850427</td>\n      <td>0.811966</td>\n      <td>0.850427</td>\n      <td>0.772532</td>\n      <td>0.849785</td>\n      <td>0.769231</td>\n      <td>0.901709</td>\n      <td>0.799145</td>\n      <td>0.854077</td>\n      <td>0.802575</td>\n      <td>0.830470</td>\n      <td>0.037271</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.008395</td>\n      <td>0.003931</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>random</td>\n      <td>1</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.790598</td>\n      <td>0.867521</td>\n      <td>0.876068</td>\n      <td>0.811159</td>\n      <td>0.798283</td>\n      <td>0.850427</td>\n      <td>0.794872</td>\n      <td>0.858974</td>\n      <td>0.781116</td>\n      <td>0.828326</td>\n      <td>0.807692</td>\n      <td>0.863248</td>\n      <td>0.858974</td>\n      <td>0.824034</td>\n      <td>0.802575</td>\n      <td>0.827591</td>\n      <td>0.031059</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.017797</td>\n      <td>0.002836</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>2</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.747863</td>\n      <td>0.846154</td>\n      <td>0.850427</td>\n      <td>0.845494</td>\n      <td>0.819742</td>\n      <td>0.841880</td>\n      <td>0.816239</td>\n      <td>0.863248</td>\n      <td>0.751073</td>\n      <td>0.793991</td>\n      <td>0.773504</td>\n      <td>0.884615</td>\n      <td>0.756410</td>\n      <td>0.828326</td>\n      <td>0.811159</td>\n      <td>0.815342</td>\n      <td>0.041196</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.005602</td>\n      <td>0.000490</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>random</td>\n      <td>2</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.773504</td>\n      <td>0.837607</td>\n      <td>0.829060</td>\n      <td>0.828326</td>\n      <td>0.858369</td>\n      <td>0.837607</td>\n      <td>0.858974</td>\n      <td>0.841880</td>\n      <td>0.824034</td>\n      <td>0.845494</td>\n      <td>0.811966</td>\n      <td>0.867521</td>\n      <td>0.850427</td>\n      <td>0.798283</td>\n      <td>0.763948</td>\n      <td>0.828467</td>\n      <td>0.029276</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.016263</td>\n      <td>0.001569</td>\n      <td>0.000801</td>\n      <td>0.000401</td>\n      <td>best</td>\n      <td>3</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.760684</td>\n      <td>0.863248</td>\n      <td>0.807692</td>\n      <td>0.832618</td>\n      <td>0.845494</td>\n      <td>0.846154</td>\n      <td>0.790598</td>\n      <td>0.846154</td>\n      <td>0.768240</td>\n      <td>0.836910</td>\n      <td>0.769231</td>\n      <td>0.905983</td>\n      <td>0.799145</td>\n      <td>0.828326</td>\n      <td>0.806867</td>\n      <td>0.820490</td>\n      <td>0.038463</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.005799</td>\n      <td>0.001760</td>\n      <td>0.000599</td>\n      <td>0.000489</td>\n      <td>random</td>\n      <td>3</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.760684</td>\n      <td>0.794872</td>\n      <td>0.841880</td>\n      <td>0.806867</td>\n      <td>0.802575</td>\n      <td>0.850427</td>\n      <td>0.820513</td>\n      <td>0.807692</td>\n      <td>0.785408</td>\n      <td>0.828326</td>\n      <td>0.824786</td>\n      <td>0.841880</td>\n      <td>0.833333</td>\n      <td>0.862661</td>\n      <td>0.781116</td>\n      <td>0.816201</td>\n      <td>0.027378</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.016465</td>\n      <td>0.002161</td>\n      <td>0.000401</td>\n      <td>0.000492</td>\n      <td>best</td>\n      <td>4</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.769231</td>\n      <td>0.871795</td>\n      <td>0.816239</td>\n      <td>0.832618</td>\n      <td>0.819742</td>\n      <td>0.837607</td>\n      <td>0.803419</td>\n      <td>0.846154</td>\n      <td>0.746781</td>\n      <td>0.828326</td>\n      <td>0.782051</td>\n      <td>0.871795</td>\n      <td>0.777778</td>\n      <td>0.871245</td>\n      <td>0.789700</td>\n      <td>0.817632</td>\n      <td>0.037858</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.005465</td>\n      <td>0.001088</td>\n      <td>0.000533</td>\n      <td>0.000499</td>\n      <td>random</td>\n      <td>4</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.777778</td>\n      <td>0.824786</td>\n      <td>0.799145</td>\n      <td>0.798283</td>\n      <td>0.802575</td>\n      <td>0.803419</td>\n      <td>0.841880</td>\n      <td>0.803419</td>\n      <td>0.768240</td>\n      <td>0.836910</td>\n      <td>0.803419</td>\n      <td>0.829060</td>\n      <td>0.799145</td>\n      <td>0.841202</td>\n      <td>0.789700</td>\n      <td>0.807931</td>\n      <td>0.021578</td>\n      <td>12</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT_model = DecisionTreeClassifier()\n",
    "\n",
    "DT_params = dict()\n",
    "DT_params['max_depth'] = [3, None]\n",
    "DT_params['min_samples_leaf'] = [1, 2, 3, 4]\n",
    "DT_params['criterion'] = [\"gini\", \"entropy\"]\n",
    "DT_params['splitter'] = [\"best\", \"random\"]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "RCV = RandomizedSearchCV(DT_model, DT_params, n_iter=32, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "DT = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(DT.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[2]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les paramètres testés pour ce modèle sont :\n",
    "-le max_depth, correspondant à la profondeur de l'arbre qui doit être illimité pour garantir une précision maximale du modèle.\n",
    "-le min_samples_leaf, correspondant au nombre d'échantillons minimal par feuille qui doit être de 1.\n",
    "-le criterion est une fonction permettant de mesurer la qualité d'un split, nous obtenons des meilleurs résultats avec 'entropy'.\n",
    "-le splitter est la méthode choisie pour selectionner le split à chaque noeud. Modifier ce paramètre ne semble pas impacter significativement les résultats."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification par boosting de gradient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "642551df",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        2.642961      0.349570         0.003692        0.004762   \n1        4.999359      0.203709         0.004941        0.003837   \n2       12.595467      0.721582         0.011132        0.002529   \n3        3.894051      0.036596         0.005349        0.004280   \n4        7.891996      0.278983         0.008004        0.003453   \n5       21.261279      2.110532         0.022539        0.005929   \n6        5.787422      0.297477         0.007823        0.003027   \n7       11.085659      0.292676         0.011759        0.002605   \n8       15.043124      0.331604         0.016522        0.003834   \n9        8.923743      0.106728         0.010022        0.002663   \n10      10.732419      0.190475         0.010971        0.002015   \n11      12.636336      0.264456         0.011763        0.002748   \n12       2.367153      0.038601         0.002167        0.002575   \n13       4.701224      0.035751         0.005713        0.004291   \n14      11.757734      0.088428         0.011210        0.002635   \n15       3.851834      0.020800         0.005251        0.003990   \n16       8.029278      0.260042         0.009908        0.003124   \n17      11.975694      0.463063         0.011637        0.001323   \n18       5.709525      0.210762         0.006322        0.003480   \n19       7.733753      0.593058         0.008706        0.002143   \n20      10.412810      1.213208         0.009183        0.003110   \n21       6.083747      0.500584         0.009866        0.010180   \n22       6.401049      0.198407         0.008371        0.002446   \n23       8.505841      0.260087         0.007739        0.003045   \n24       2.415644      0.044271         0.002429        0.003037   \n25       4.847136      0.052056         0.005031        0.003322   \n26      12.296079      0.364221         0.011093        0.002024   \n27       4.013767      0.035984         0.006011        0.004047   \n28       7.195184      0.266932         0.008840        0.002748   \n29       9.806637      0.370849         0.010576        0.001803   \n30       5.035684      0.179999         0.006923        0.004330   \n31       5.630193      0.120447         0.005871        0.004670   \n32       8.013972      0.100420         0.009025        0.002454   \n33       4.124968      0.099238         0.004156        0.003066   \n34       4.962349      0.169187         0.005773        0.003368   \n35       7.033018      0.105714         0.009071        0.002170   \n\n   param_n_estimators param_max_depth param_learning_rate  \\\n0                 100               1                 0.1   \n1                 200               1                 0.1   \n2                 500               1                 0.1   \n3                 100               2                 0.1   \n4                 200               2                 0.1   \n5                 500               2                 0.1   \n6                 100               3                 0.1   \n7                 200               3                 0.1   \n8                 500               3                 0.1   \n9                 100               5                 0.1   \n10                200               5                 0.1   \n11                500               5                 0.1   \n12                100               1                 0.2   \n13                200               1                 0.2   \n14                500               1                 0.2   \n15                100               2                 0.2   \n16                200               2                 0.2   \n17                500               2                 0.2   \n18                100               3                 0.2   \n19                200               3                 0.2   \n20                500               3                 0.2   \n21                100               5                 0.2   \n22                200               5                 0.2   \n23                500               5                 0.2   \n24                100               1                 0.3   \n25                200               1                 0.3   \n26                500               1                 0.3   \n27                100               2                 0.3   \n28                200               2                 0.3   \n29                500               2                 0.3   \n30                100               3                 0.3   \n31                200               3                 0.3   \n32                500               3                 0.3   \n33                100               5                 0.3   \n34                200               5                 0.3   \n35                500               5                 0.3   \n\n                                               params  split0_test_score  \\\n0   {'n_estimators': 100, 'max_depth': 1, 'learnin...           0.905983   \n1   {'n_estimators': 200, 'max_depth': 1, 'learnin...           0.923077   \n2   {'n_estimators': 500, 'max_depth': 1, 'learnin...           0.952991   \n3   {'n_estimators': 100, 'max_depth': 2, 'learnin...           0.961538   \n4   {'n_estimators': 200, 'max_depth': 2, 'learnin...           0.974359   \n5   {'n_estimators': 500, 'max_depth': 2, 'learnin...           0.970085   \n6   {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.961538   \n7   {'n_estimators': 200, 'max_depth': 3, 'learnin...           0.961538   \n8   {'n_estimators': 500, 'max_depth': 3, 'learnin...           0.970085   \n9   {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.897436   \n10  {'n_estimators': 200, 'max_depth': 5, 'learnin...           0.914530   \n11  {'n_estimators': 500, 'max_depth': 5, 'learnin...           0.905983   \n12  {'n_estimators': 100, 'max_depth': 1, 'learnin...           0.931624   \n13  {'n_estimators': 200, 'max_depth': 1, 'learnin...           0.957265   \n14  {'n_estimators': 500, 'max_depth': 1, 'learnin...           0.961538   \n15  {'n_estimators': 100, 'max_depth': 2, 'learnin...           0.974359   \n16  {'n_estimators': 200, 'max_depth': 2, 'learnin...           0.974359   \n17  {'n_estimators': 500, 'max_depth': 2, 'learnin...           0.974359   \n18  {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.974359   \n19  {'n_estimators': 200, 'max_depth': 3, 'learnin...           0.965812   \n20  {'n_estimators': 500, 'max_depth': 3, 'learnin...           0.965812   \n21  {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.914530   \n22  {'n_estimators': 200, 'max_depth': 5, 'learnin...           0.918803   \n23  {'n_estimators': 500, 'max_depth': 5, 'learnin...           0.918803   \n24  {'n_estimators': 100, 'max_depth': 1, 'learnin...           0.952991   \n25  {'n_estimators': 200, 'max_depth': 1, 'learnin...           0.948718   \n26  {'n_estimators': 500, 'max_depth': 1, 'learnin...           0.948718   \n27  {'n_estimators': 100, 'max_depth': 2, 'learnin...           0.978632   \n28  {'n_estimators': 200, 'max_depth': 2, 'learnin...           0.974359   \n29  {'n_estimators': 500, 'max_depth': 2, 'learnin...           0.974359   \n30  {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.965812   \n31  {'n_estimators': 200, 'max_depth': 3, 'learnin...           0.970085   \n32  {'n_estimators': 500, 'max_depth': 3, 'learnin...           0.982906   \n33  {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.918803   \n34  {'n_estimators': 200, 'max_depth': 5, 'learnin...           0.923077   \n35  {'n_estimators': 500, 'max_depth': 5, 'learnin...           0.918803   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.952991           0.940171           0.935622   \n1            0.961538           0.952991           0.957082   \n2            0.961538           0.965812           0.961373   \n3            0.970085           0.935897           0.969957   \n4            0.965812           0.948718           0.961373   \n5            0.965812           0.952991           0.961373   \n6            0.952991           0.940171           0.965665   \n7            0.957265           0.940171           0.965665   \n8            0.961538           0.940171           0.969957   \n9            0.948718           0.944444           0.927039   \n10           0.944444           0.952991           0.939914   \n11           0.944444           0.952991           0.935622   \n12           0.961538           0.952991           0.957082   \n13           0.965812           0.957265           0.948498   \n14           0.957265           0.957265           0.965665   \n15           0.974359           0.948718           0.969957   \n16           0.970085           0.957265           0.965665   \n17           0.965812           0.952991           0.961373   \n18           0.957265           0.944444           0.961373   \n19           0.961538           0.944444           0.965665   \n20           0.957265           0.944444           0.965665   \n21           0.940171           0.944444           0.948498   \n22           0.948718           0.944444           0.948498   \n23           0.944444           0.948718           0.952790   \n24           0.961538           0.952991           0.957082   \n25           0.961538           0.961538           0.961373   \n26           0.957265           0.961538           0.965665   \n27           0.974359           0.952991           0.969957   \n28           0.965812           0.952991           0.969957   \n29           0.965812           0.952991           0.969957   \n30           0.965812           0.952991           0.974249   \n31           0.961538           0.948718           0.969957   \n32           0.961538           0.952991           0.965665   \n33           0.957265           0.944444           0.948498   \n34           0.948718           0.957265           0.952790   \n35           0.948718           0.948718           0.948498   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.922747           0.927350           0.961538   \n1            0.931330           0.957265           0.974359   \n2            0.935622           0.961538           0.952991   \n3            0.939914           0.952991           0.965812   \n4            0.957082           0.965812           0.965812   \n5            0.961373           0.961538           0.970085   \n6            0.957082           0.957265           0.978632   \n7            0.952790           0.952991           0.982906   \n8            0.957082           0.957265           0.982906   \n9            0.952790           0.944444           0.948718   \n10           0.939914           0.940171           0.952991   \n11           0.952790           0.931624           0.944444   \n12           0.931330           0.961538           0.974359   \n13           0.939914           0.965812           0.965812   \n14           0.948498           0.961538           0.961538   \n15           0.957082           0.965812           0.970085   \n16           0.961373           0.957265           0.982906   \n17           0.965665           0.961538           0.974359   \n18           0.952790           0.957265           0.987179   \n19           0.961373           0.957265           0.982906   \n20           0.957082           0.952991           0.982906   \n21           0.965665           0.944444           0.952991   \n22           0.952790           0.944444           0.944444   \n23           0.948498           0.952991           0.948718   \n24           0.935622           0.965812           0.961538   \n25           0.944206           0.957265           0.965812   \n26           0.948498           0.961538           0.965812   \n27           0.957082           0.961538           0.974359   \n28           0.961373           0.965812           0.978632   \n29           0.961373           0.965812           0.978632   \n30           0.952790           0.952991           0.982906   \n31           0.952790           0.952991           0.982906   \n32           0.952790           0.952991           0.982906   \n33           0.952790           0.957265           0.961538   \n34           0.965665           0.944444           0.952991   \n35           0.961373           0.948718           0.957265   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.940171           0.931330           0.927039   \n1            0.948718           0.939914           0.944206   \n2            0.957265           0.948498           0.957082   \n3            0.970085           0.935622           0.957082   \n4            0.974359           0.939914           0.957082   \n5            0.978632           0.961373           0.965665   \n6            0.957265           0.944206           0.952790   \n7            0.974359           0.948498           0.965665   \n8            0.974359           0.952790           0.965665   \n9            0.940171           0.896996           0.948498   \n10           0.952991           0.918455           0.948498   \n11           0.948718           0.901288           0.957082   \n12           0.952991           0.935622           0.944206   \n13           0.961538           0.948498           0.952790   \n14           0.961538           0.952790           0.961373   \n15           0.978632           0.939914           0.969957   \n16           0.978632           0.957082           0.965665   \n17           0.978632           0.957082           0.969957   \n18           0.965812           0.957082           0.961373   \n19           0.974359           0.957082           0.982833   \n20           0.970085           0.961373           0.969957   \n21           0.935897           0.901288           0.952790   \n22           0.944444           0.918455           0.952790   \n23           0.944444           0.922747           0.948498   \n24           0.961538           0.939914           0.948498   \n25           0.961538           0.948498           0.961373   \n26           0.965812           0.944206           0.969957   \n27           0.974359           0.952790           0.965665   \n28           0.974359           0.961373           0.974249   \n29           0.974359           0.961373           0.974249   \n30           0.965812           0.952790           0.961373   \n31           0.965812           0.965665           0.974249   \n32           0.974359           0.952790           0.965665   \n33           0.948718           0.922747           0.948498   \n34           0.961538           0.931330           0.952790   \n35           0.940171           0.931330           0.935622   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.931624            0.897436            0.957265   \n1             0.948718            0.905983            0.974359   \n2             0.961538            0.923077            0.970085   \n3             0.935897            0.952991            0.970085   \n4             0.948718            0.957265            0.974359   \n5             0.957265            0.952991            0.978632   \n6             0.940171            0.952991            0.965812   \n7             0.944444            0.952991            0.970085   \n8             0.948718            0.952991            0.970085   \n9             0.923077            0.944444            0.961538   \n10            0.927350            0.948718            0.970085   \n11            0.927350            0.948718            0.970085   \n12            0.952991            0.910256            0.978632   \n13            0.957265            0.923077            0.970085   \n14            0.957265            0.931624            0.970085   \n15            0.948718            0.952991            0.974359   \n16            0.952991            0.952991            0.978632   \n17            0.957265            0.952991            0.974359   \n18            0.948718            0.952991            0.965812   \n19            0.948718            0.952991            0.970085   \n20            0.957265            0.952991            0.970085   \n21            0.918803            0.940171            0.970085   \n22            0.914530            0.948718            0.970085   \n23            0.923077            0.948718            0.970085   \n24            0.957265            0.914530            0.970085   \n25            0.961538            0.923077            0.970085   \n26            0.952991            0.931624            0.961538   \n27            0.952991            0.961538            0.970085   \n28            0.952991            0.948718            0.974359   \n29            0.952991            0.948718            0.978632   \n30            0.952991            0.957265            0.970085   \n31            0.952991            0.957265            0.970085   \n32            0.952991            0.957265            0.974359   \n33            0.923077            0.952991            0.965812   \n34            0.918803            0.965812            0.961538   \n35            0.931624            0.965812            0.961538   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.948498            0.914163         0.932929        0.017663   \n1             0.969957            0.939914         0.948627        0.018354   \n2             0.969957            0.957082         0.955763        0.012075   \n3             0.978541            0.961373         0.957192        0.013986   \n4             0.978541            0.969957         0.962611        0.010786   \n5             0.974249            0.961373         0.964896        0.007843   \n6             0.961373            0.944206         0.955477        0.010229   \n7             0.961373            0.952790         0.958902        0.011136   \n8             0.969957            0.948498         0.961471        0.011268   \n9             0.952790            0.948498         0.938640        0.018681   \n10            0.952790            0.952790         0.943776        0.014061   \n11            0.952790            0.948498         0.941495        0.017941   \n12            0.974249            0.944206         0.950908        0.018022   \n13            0.969957            0.957082         0.956045        0.012067   \n14            0.974249            0.957082         0.958621        0.009452   \n15            0.969957            0.957082         0.963466        0.011380   \n16            0.969957            0.965665         0.966036        0.009308   \n17            0.969957            0.965665         0.965467        0.007883   \n18            0.974249            0.961373         0.961472        0.010591   \n19            0.978541            0.961373         0.964332        0.011321   \n20            0.978541            0.961373         0.963189        0.009838   \n21            0.944206            0.944206         0.941213        0.017578   \n22            0.952790            0.952790         0.943783        0.014629   \n23            0.952790            0.948498         0.944921        0.013052   \n24            0.974249            0.948498         0.953477        0.014495   \n25            0.969957            0.965665         0.957479        0.011732   \n26            0.974249            0.952790         0.957480        0.010760   \n27            0.969957            0.965665         0.965465        0.008348   \n28            0.969957            0.965665         0.966041        0.008751   \n29            0.969957            0.965665         0.966326        0.009080   \n30            0.969957            0.948498         0.961755        0.009580   \n31            0.978541            0.952790         0.963759        0.010222   \n32            0.969957            0.961373         0.964037        0.010468   \n33            0.948498            0.952790         0.946916        0.013786   \n34            0.948498            0.939914         0.948345        0.014097   \n35            0.957082            0.948498         0.946918        0.012768   \n\n    rank_test_score  \n0                36  \n1                26  \n2                22  \n3                20  \n4                12  \n5                 6  \n6                23  \n7                16  \n8                15  \n9                35  \n10               32  \n11               33  \n12               25  \n13               21  \n14               17  \n15               10  \n16                3  \n17                4  \n18               14  \n19                7  \n20               11  \n21               34  \n22               31  \n23               30  \n24               24  \n25               19  \n26               18  \n27                5  \n28                2  \n29                1  \n30               13  \n31                9  \n32                8  \n33               29  \n34               27  \n35               28  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_n_estimators</th>\n      <th>param_max_depth</th>\n      <th>param_learning_rate</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.642961</td>\n      <td>0.349570</td>\n      <td>0.003692</td>\n      <td>0.004762</td>\n      <td>100</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 1, 'learnin...</td>\n      <td>0.905983</td>\n      <td>0.952991</td>\n      <td>0.940171</td>\n      <td>0.935622</td>\n      <td>0.922747</td>\n      <td>0.927350</td>\n      <td>0.961538</td>\n      <td>0.940171</td>\n      <td>0.931330</td>\n      <td>0.927039</td>\n      <td>0.931624</td>\n      <td>0.897436</td>\n      <td>0.957265</td>\n      <td>0.948498</td>\n      <td>0.914163</td>\n      <td>0.932929</td>\n      <td>0.017663</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.999359</td>\n      <td>0.203709</td>\n      <td>0.004941</td>\n      <td>0.003837</td>\n      <td>200</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 200, 'max_depth': 1, 'learnin...</td>\n      <td>0.923077</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.931330</td>\n      <td>0.957265</td>\n      <td>0.974359</td>\n      <td>0.948718</td>\n      <td>0.939914</td>\n      <td>0.944206</td>\n      <td>0.948718</td>\n      <td>0.905983</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.939914</td>\n      <td>0.948627</td>\n      <td>0.018354</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12.595467</td>\n      <td>0.721582</td>\n      <td>0.011132</td>\n      <td>0.002529</td>\n      <td>500</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 500, 'max_depth': 1, 'learnin...</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.961373</td>\n      <td>0.935622</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.957265</td>\n      <td>0.948498</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.923077</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.955763</td>\n      <td>0.012075</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.894051</td>\n      <td>0.036596</td>\n      <td>0.005349</td>\n      <td>0.004280</td>\n      <td>100</td>\n      <td>2</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 2, 'learnin...</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.935897</td>\n      <td>0.969957</td>\n      <td>0.939914</td>\n      <td>0.952991</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.935622</td>\n      <td>0.957082</td>\n      <td>0.935897</td>\n      <td>0.952991</td>\n      <td>0.970085</td>\n      <td>0.978541</td>\n      <td>0.961373</td>\n      <td>0.957192</td>\n      <td>0.013986</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.891996</td>\n      <td>0.278983</td>\n      <td>0.008004</td>\n      <td>0.003453</td>\n      <td>200</td>\n      <td>2</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 200, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.948718</td>\n      <td>0.961373</td>\n      <td>0.957082</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.939914</td>\n      <td>0.957082</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.962611</td>\n      <td>0.010786</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>21.261279</td>\n      <td>2.110532</td>\n      <td>0.022539</td>\n      <td>0.005929</td>\n      <td>500</td>\n      <td>2</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 500, 'max_depth': 2, 'learnin...</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.961373</td>\n      <td>0.961373</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.978632</td>\n      <td>0.961373</td>\n      <td>0.965665</td>\n      <td>0.957265</td>\n      <td>0.952991</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964896</td>\n      <td>0.007843</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5.787422</td>\n      <td>0.297477</td>\n      <td>0.007823</td>\n      <td>0.003027</td>\n      <td>100</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.940171</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.957265</td>\n      <td>0.978632</td>\n      <td>0.957265</td>\n      <td>0.944206</td>\n      <td>0.952790</td>\n      <td>0.940171</td>\n      <td>0.952991</td>\n      <td>0.965812</td>\n      <td>0.961373</td>\n      <td>0.944206</td>\n      <td>0.955477</td>\n      <td>0.010229</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>11.085659</td>\n      <td>0.292676</td>\n      <td>0.011759</td>\n      <td>0.002605</td>\n      <td>200</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 200, 'max_depth': 3, 'learnin...</td>\n      <td>0.961538</td>\n      <td>0.957265</td>\n      <td>0.940171</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.952991</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.965665</td>\n      <td>0.944444</td>\n      <td>0.952991</td>\n      <td>0.970085</td>\n      <td>0.961373</td>\n      <td>0.952790</td>\n      <td>0.958902</td>\n      <td>0.011136</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>15.043124</td>\n      <td>0.331604</td>\n      <td>0.016522</td>\n      <td>0.003834</td>\n      <td>500</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 500, 'max_depth': 3, 'learnin...</td>\n      <td>0.970085</td>\n      <td>0.961538</td>\n      <td>0.940171</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.952790</td>\n      <td>0.965665</td>\n      <td>0.948718</td>\n      <td>0.952991</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.948498</td>\n      <td>0.961471</td>\n      <td>0.011268</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8.923743</td>\n      <td>0.106728</td>\n      <td>0.010022</td>\n      <td>0.002663</td>\n      <td>100</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.897436</td>\n      <td>0.948718</td>\n      <td>0.944444</td>\n      <td>0.927039</td>\n      <td>0.952790</td>\n      <td>0.944444</td>\n      <td>0.948718</td>\n      <td>0.940171</td>\n      <td>0.896996</td>\n      <td>0.948498</td>\n      <td>0.923077</td>\n      <td>0.944444</td>\n      <td>0.961538</td>\n      <td>0.952790</td>\n      <td>0.948498</td>\n      <td>0.938640</td>\n      <td>0.018681</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10.732419</td>\n      <td>0.190475</td>\n      <td>0.010971</td>\n      <td>0.002015</td>\n      <td>200</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 200, 'max_depth': 5, 'learnin...</td>\n      <td>0.914530</td>\n      <td>0.944444</td>\n      <td>0.952991</td>\n      <td>0.939914</td>\n      <td>0.939914</td>\n      <td>0.940171</td>\n      <td>0.952991</td>\n      <td>0.952991</td>\n      <td>0.918455</td>\n      <td>0.948498</td>\n      <td>0.927350</td>\n      <td>0.948718</td>\n      <td>0.970085</td>\n      <td>0.952790</td>\n      <td>0.952790</td>\n      <td>0.943776</td>\n      <td>0.014061</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12.636336</td>\n      <td>0.264456</td>\n      <td>0.011763</td>\n      <td>0.002748</td>\n      <td>500</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 500, 'max_depth': 5, 'learnin...</td>\n      <td>0.905983</td>\n      <td>0.944444</td>\n      <td>0.952991</td>\n      <td>0.935622</td>\n      <td>0.952790</td>\n      <td>0.931624</td>\n      <td>0.944444</td>\n      <td>0.948718</td>\n      <td>0.901288</td>\n      <td>0.957082</td>\n      <td>0.927350</td>\n      <td>0.948718</td>\n      <td>0.970085</td>\n      <td>0.952790</td>\n      <td>0.948498</td>\n      <td>0.941495</td>\n      <td>0.017941</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2.367153</td>\n      <td>0.038601</td>\n      <td>0.002167</td>\n      <td>0.002575</td>\n      <td>100</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 100, 'max_depth': 1, 'learnin...</td>\n      <td>0.931624</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.931330</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.935622</td>\n      <td>0.944206</td>\n      <td>0.952991</td>\n      <td>0.910256</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.944206</td>\n      <td>0.950908</td>\n      <td>0.018022</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>4.701224</td>\n      <td>0.035751</td>\n      <td>0.005713</td>\n      <td>0.004291</td>\n      <td>200</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 200, 'max_depth': 1, 'learnin...</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957265</td>\n      <td>0.948498</td>\n      <td>0.939914</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.948498</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.923077</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.956045</td>\n      <td>0.012067</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>11.757734</td>\n      <td>0.088428</td>\n      <td>0.011210</td>\n      <td>0.002635</td>\n      <td>500</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 500, 'max_depth': 1, 'learnin...</td>\n      <td>0.961538</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.952790</td>\n      <td>0.961373</td>\n      <td>0.957265</td>\n      <td>0.931624</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.958621</td>\n      <td>0.009452</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>3.851834</td>\n      <td>0.020800</td>\n      <td>0.005251</td>\n      <td>0.003990</td>\n      <td>100</td>\n      <td>2</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 100, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.948718</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.978632</td>\n      <td>0.939914</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.952991</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.963466</td>\n      <td>0.011380</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>8.029278</td>\n      <td>0.260042</td>\n      <td>0.009908</td>\n      <td>0.003124</td>\n      <td>200</td>\n      <td>2</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 200, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.961373</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.952991</td>\n      <td>0.952991</td>\n      <td>0.978632</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.966036</td>\n      <td>0.009308</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>11.975694</td>\n      <td>0.463063</td>\n      <td>0.011637</td>\n      <td>0.001323</td>\n      <td>500</td>\n      <td>2</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 500, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.961373</td>\n      <td>0.965665</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.969957</td>\n      <td>0.957265</td>\n      <td>0.952991</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.965467</td>\n      <td>0.007883</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>5.709525</td>\n      <td>0.210762</td>\n      <td>0.006322</td>\n      <td>0.003480</td>\n      <td>100</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.957265</td>\n      <td>0.944444</td>\n      <td>0.961373</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.987179</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.961373</td>\n      <td>0.948718</td>\n      <td>0.952991</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.961472</td>\n      <td>0.010591</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>7.733753</td>\n      <td>0.593058</td>\n      <td>0.008706</td>\n      <td>0.002143</td>\n      <td>200</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 200, 'max_depth': 3, 'learnin...</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.944444</td>\n      <td>0.965665</td>\n      <td>0.961373</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.957082</td>\n      <td>0.982833</td>\n      <td>0.948718</td>\n      <td>0.952991</td>\n      <td>0.970085</td>\n      <td>0.978541</td>\n      <td>0.961373</td>\n      <td>0.964332</td>\n      <td>0.011321</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>10.412810</td>\n      <td>1.213208</td>\n      <td>0.009183</td>\n      <td>0.003110</td>\n      <td>500</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 500, 'max_depth': 3, 'learnin...</td>\n      <td>0.965812</td>\n      <td>0.957265</td>\n      <td>0.944444</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.952991</td>\n      <td>0.982906</td>\n      <td>0.970085</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.957265</td>\n      <td>0.952991</td>\n      <td>0.970085</td>\n      <td>0.978541</td>\n      <td>0.961373</td>\n      <td>0.963189</td>\n      <td>0.009838</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>6.083747</td>\n      <td>0.500584</td>\n      <td>0.009866</td>\n      <td>0.010180</td>\n      <td>100</td>\n      <td>5</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.914530</td>\n      <td>0.940171</td>\n      <td>0.944444</td>\n      <td>0.948498</td>\n      <td>0.965665</td>\n      <td>0.944444</td>\n      <td>0.952991</td>\n      <td>0.935897</td>\n      <td>0.901288</td>\n      <td>0.952790</td>\n      <td>0.918803</td>\n      <td>0.940171</td>\n      <td>0.970085</td>\n      <td>0.944206</td>\n      <td>0.944206</td>\n      <td>0.941213</td>\n      <td>0.017578</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>6.401049</td>\n      <td>0.198407</td>\n      <td>0.008371</td>\n      <td>0.002446</td>\n      <td>200</td>\n      <td>5</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 200, 'max_depth': 5, 'learnin...</td>\n      <td>0.918803</td>\n      <td>0.948718</td>\n      <td>0.944444</td>\n      <td>0.948498</td>\n      <td>0.952790</td>\n      <td>0.944444</td>\n      <td>0.944444</td>\n      <td>0.944444</td>\n      <td>0.918455</td>\n      <td>0.952790</td>\n      <td>0.914530</td>\n      <td>0.948718</td>\n      <td>0.970085</td>\n      <td>0.952790</td>\n      <td>0.952790</td>\n      <td>0.943783</td>\n      <td>0.014629</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>8.505841</td>\n      <td>0.260087</td>\n      <td>0.007739</td>\n      <td>0.003045</td>\n      <td>500</td>\n      <td>5</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 500, 'max_depth': 5, 'learnin...</td>\n      <td>0.918803</td>\n      <td>0.944444</td>\n      <td>0.948718</td>\n      <td>0.952790</td>\n      <td>0.948498</td>\n      <td>0.952991</td>\n      <td>0.948718</td>\n      <td>0.944444</td>\n      <td>0.922747</td>\n      <td>0.948498</td>\n      <td>0.923077</td>\n      <td>0.948718</td>\n      <td>0.970085</td>\n      <td>0.952790</td>\n      <td>0.948498</td>\n      <td>0.944921</td>\n      <td>0.013052</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2.415644</td>\n      <td>0.044271</td>\n      <td>0.002429</td>\n      <td>0.003037</td>\n      <td>100</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 100, 'max_depth': 1, 'learnin...</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.939914</td>\n      <td>0.948498</td>\n      <td>0.957265</td>\n      <td>0.914530</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.948498</td>\n      <td>0.953477</td>\n      <td>0.014495</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>4.847136</td>\n      <td>0.052056</td>\n      <td>0.005031</td>\n      <td>0.003322</td>\n      <td>200</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 200, 'max_depth': 1, 'learnin...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.944206</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.948498</td>\n      <td>0.961373</td>\n      <td>0.961538</td>\n      <td>0.923077</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.957479</td>\n      <td>0.011732</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>12.296079</td>\n      <td>0.364221</td>\n      <td>0.011093</td>\n      <td>0.002024</td>\n      <td>500</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 500, 'max_depth': 1, 'learnin...</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.961538</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.969957</td>\n      <td>0.952991</td>\n      <td>0.931624</td>\n      <td>0.961538</td>\n      <td>0.974249</td>\n      <td>0.952790</td>\n      <td>0.957480</td>\n      <td>0.010760</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>4.013767</td>\n      <td>0.035984</td>\n      <td>0.006011</td>\n      <td>0.004047</td>\n      <td>100</td>\n      <td>2</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 100, 'max_depth': 2, 'learnin...</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.952790</td>\n      <td>0.965665</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.965465</td>\n      <td>0.008348</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>7.195184</td>\n      <td>0.266932</td>\n      <td>0.008840</td>\n      <td>0.002748</td>\n      <td>200</td>\n      <td>2</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 200, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.969957</td>\n      <td>0.961373</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.952991</td>\n      <td>0.948718</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.966041</td>\n      <td>0.008751</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>9.806637</td>\n      <td>0.370849</td>\n      <td>0.010576</td>\n      <td>0.001803</td>\n      <td>500</td>\n      <td>2</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 500, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.969957</td>\n      <td>0.961373</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.952991</td>\n      <td>0.948718</td>\n      <td>0.978632</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.966326</td>\n      <td>0.009080</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>5.035684</td>\n      <td>0.179999</td>\n      <td>0.006923</td>\n      <td>0.004330</td>\n      <td>100</td>\n      <td>3</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.974249</td>\n      <td>0.952790</td>\n      <td>0.952991</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.952790</td>\n      <td>0.961373</td>\n      <td>0.952991</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.948498</td>\n      <td>0.961755</td>\n      <td>0.009580</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>5.630193</td>\n      <td>0.120447</td>\n      <td>0.005871</td>\n      <td>0.004670</td>\n      <td>200</td>\n      <td>3</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 200, 'max_depth': 3, 'learnin...</td>\n      <td>0.970085</td>\n      <td>0.961538</td>\n      <td>0.948718</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.952991</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.974249</td>\n      <td>0.952991</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.978541</td>\n      <td>0.952790</td>\n      <td>0.963759</td>\n      <td>0.010222</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>8.013972</td>\n      <td>0.100420</td>\n      <td>0.009025</td>\n      <td>0.002454</td>\n      <td>500</td>\n      <td>3</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 500, 'max_depth': 3, 'learnin...</td>\n      <td>0.982906</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.952991</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.952790</td>\n      <td>0.965665</td>\n      <td>0.952991</td>\n      <td>0.957265</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.961373</td>\n      <td>0.964037</td>\n      <td>0.010468</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>4.124968</td>\n      <td>0.099238</td>\n      <td>0.004156</td>\n      <td>0.003066</td>\n      <td>100</td>\n      <td>5</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.918803</td>\n      <td>0.957265</td>\n      <td>0.944444</td>\n      <td>0.948498</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.961538</td>\n      <td>0.948718</td>\n      <td>0.922747</td>\n      <td>0.948498</td>\n      <td>0.923077</td>\n      <td>0.952991</td>\n      <td>0.965812</td>\n      <td>0.948498</td>\n      <td>0.952790</td>\n      <td>0.946916</td>\n      <td>0.013786</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>4.962349</td>\n      <td>0.169187</td>\n      <td>0.005773</td>\n      <td>0.003368</td>\n      <td>200</td>\n      <td>5</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 200, 'max_depth': 5, 'learnin...</td>\n      <td>0.923077</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.952790</td>\n      <td>0.965665</td>\n      <td>0.944444</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.931330</td>\n      <td>0.952790</td>\n      <td>0.918803</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.948498</td>\n      <td>0.939914</td>\n      <td>0.948345</td>\n      <td>0.014097</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>7.033018</td>\n      <td>0.105714</td>\n      <td>0.009071</td>\n      <td>0.002170</td>\n      <td>500</td>\n      <td>5</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 500, 'max_depth': 5, 'learnin...</td>\n      <td>0.918803</td>\n      <td>0.948718</td>\n      <td>0.948718</td>\n      <td>0.948498</td>\n      <td>0.961373</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.940171</td>\n      <td>0.931330</td>\n      <td>0.935622</td>\n      <td>0.931624</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.957082</td>\n      <td>0.948498</td>\n      <td>0.946918</td>\n      <td>0.012768</td>\n      <td>28</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "GB_model = GradientBoostingClassifier()\n",
    "GB_params = dict()\n",
    "GB_params['n_estimators'] = [100, 200, 500]\n",
    "GB_params['max_depth'] = [1, 2, 3, 5]\n",
    "GB_params['learning_rate'] = [0.1, 0.2, 0.3]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(GB_model, GB_params, n_iter=36, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "GB = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(GB.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[3]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les paramètres testés pour ce modèle sont :\n",
    "Le training rate qui correspond à la rapidité d'apprentissage du modèle. Le lien ci-dessous explique plus en détail l'importance de cet hyperparamètre. Dans notre cas, une valeur de 0.3 semble donner les meilleurs résultats.\n",
    "> https://www.machinelearningplus.com/machine-learning/an-introduction-to-gradient-boosting-decision-trees/#:~:text=Using%20a%20low%20learning%20rate,0.3%20gives%20the%20best%20results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification par voisin le plus proche"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        0.003329      0.006906         0.023800        0.006793   \n1        0.013395      0.007127         0.023204        0.004185   \n2        0.001198      0.000651         0.017535        0.007605   \n3        0.002200      0.003165         0.009199        0.005104   \n4        0.001002      0.000365         0.016532        0.003182   \n5        0.001003      0.000364         0.009131        0.001256   \n6        0.019730      0.001689         0.049535        0.004287   \n7        0.007263      0.002646         0.016402        0.000711   \n8        0.028934      0.001290         0.225867        0.005600   \n9        0.001068      0.000251         0.020332        0.000866   \n10       0.001138      0.000501         0.022261        0.003171   \n11       0.019595      0.002679         0.043137        0.001925   \n12       0.001136      0.001360         0.014797        0.000539   \n13       0.000870      0.000341         0.017729        0.001062   \n14       0.027932      0.001288         0.093735        0.005120   \n15       0.009331      0.000470         0.018335        0.001192   \n16       0.000934      0.000443         0.020597        0.000880   \n17       0.013263      0.000572         0.025004        0.001035   \n18       0.001001      0.000004         0.008798        0.000746   \n19       0.000866      0.000340         0.009201        0.001515   \n20       0.001135      0.000498         0.008731        0.000572   \n21       0.001070      0.000444         0.022461        0.003440   \n22       0.027396      0.000948         0.053468        0.001964   \n23       0.001468      0.001746         0.008267        0.000445   \n24       0.013398      0.000488         0.033735        0.000998   \n25       0.028733      0.000677         0.209001        0.009219   \n26       0.000935      0.000250         0.020064        0.000680   \n27       0.006801      0.001470         0.020999        0.000633   \n28       0.013200      0.000401         0.025466        0.001544   \n29       0.012597      0.000712         0.024535        0.000719   \n30       0.009266      0.000575         0.022667        0.001012   \n31       0.018130      0.000718         0.038802        0.001047   \n32       0.013466      0.001019         0.037733        0.001342   \n33       0.006400      0.000490         0.022000        0.000516   \n34       0.000935      0.000250         0.014132        0.001086   \n35       0.000934      0.000442         0.015200        0.001513   \n36       0.000936      0.000442         0.020531        0.000617   \n37       0.001402      0.001307         0.005931        0.000853   \n38       0.000868      0.000500         0.021130        0.001856   \n39       0.000933      0.000441         0.013868        0.000338   \n40       0.019396      0.002653         0.027803        0.002040   \n41       0.001066      0.000443         0.014132        0.001025   \n42       0.001070      0.000444         0.008462        0.000885   \n43       0.001200      0.000542         0.014267        0.000572   \n44       0.000868      0.000499         0.013199        0.001758   \n45       0.000866      0.000499         0.014466        0.001145   \n46       0.001001      0.000366         0.018399        0.001450   \n47       0.014126      0.001747         0.035271        0.003891   \n48       0.001197      0.000400         0.016065        0.001843   \n49       0.019797      0.000977         0.032333        0.001349   \n\n   param_weights param_p param_n_neighbors param_leaf_size param_algorithm  \\\n0        uniform       1                16              30            auto   \n1       distance       1                 6              10       ball_tree   \n2       distance       1                 1              10           brute   \n3       distance       2                 1              30            auto   \n4        uniform       2                 6              30           brute   \n5       distance       2                11               1            auto   \n6        uniform       2                11              10         kd_tree   \n7       distance       1                 1              30       ball_tree   \n8        uniform       2                16               1         kd_tree   \n9        uniform       1                 6              30           brute   \n10       uniform       1                16              10           brute   \n11      distance       2                16              10         kd_tree   \n12      distance       1                16              10            auto   \n13       uniform       1                 1              10            auto   \n14       uniform       1                16               1         kd_tree   \n15      distance       2                 1              10       ball_tree   \n16       uniform       1                16              10            auto   \n17      distance       2                 1               1       ball_tree   \n18      distance       2                16              10            auto   \n19      distance       2                11               1           brute   \n20      distance       2                16              30            auto   \n21       uniform       1                 6               1            auto   \n22      distance       1                 1               1         kd_tree   \n23      distance       2                11              30           brute   \n24       uniform       1                11               1       ball_tree   \n25       uniform       2                11               1         kd_tree   \n26       uniform       1                11               1           brute   \n27       uniform       2                 1              30       ball_tree   \n28      distance       1                 6               1       ball_tree   \n29       uniform       2                 1              30         kd_tree   \n30       uniform       2                 1              10       ball_tree   \n31      distance       2                 6              10         kd_tree   \n32       uniform       2                11               1       ball_tree   \n33       uniform       1                 6              30       ball_tree   \n34       uniform       2                11              30           brute   \n35       uniform       2                16               1            auto   \n36       uniform       1                11              30            auto   \n37      distance       2                 1              10            auto   \n38       uniform       1                 6              10           brute   \n39       uniform       2                16              30           brute   \n40      distance       1                 1              10         kd_tree   \n41       uniform       2                16              10           brute   \n42      distance       2                 6              10           brute   \n43       uniform       2                 6               1           brute   \n44      distance       1                 1              30            auto   \n45       uniform       2                11              10           brute   \n46       uniform       1                 1              10           brute   \n47      distance       2                11               1       ball_tree   \n48      distance       1                 6              10           brute   \n49       uniform       1                 1              10         kd_tree   \n\n                                               params  split0_test_score  \\\n0   {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.948718   \n1   {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.987179   \n2   {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n3   {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n4   {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.982906   \n5   {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n6   {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n7   {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n8   {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.944444   \n9   {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n10  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.948718   \n11  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.957265   \n12  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.961538   \n13  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n14  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.944444   \n15  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n16  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.948718   \n17  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n18  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.957265   \n19  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n20  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.957265   \n21  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n22  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n23  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n24  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.965812   \n25  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n26  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.965812   \n27  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.987179   \n28  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.987179   \n29  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.987179   \n30  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.987179   \n31  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.982906   \n32  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n33  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.982906   \n34  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n35  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.940171   \n36  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.965812   \n37  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n38  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n39  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.940171   \n40  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n41  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.940171   \n42  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.982906   \n43  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.982906   \n44  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n45  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n46  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n47  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n48  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.982906   \n49  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.961538           0.970085           0.969957   \n1            0.970085           0.974359           0.982833   \n2            0.978632           0.970085           0.987124   \n3            0.987179           0.978632           0.978541   \n4            0.991453           0.970085           0.965665   \n5            0.982906           0.965812           0.978541   \n6            0.978632           0.965812           0.974249   \n7            0.978632           0.970085           0.987124   \n8            0.970085           0.965812           0.969957   \n9            0.974359           0.965812           0.969957   \n10           0.961538           0.970085           0.969957   \n11           0.970085           0.970085           0.974249   \n12           0.961538           0.970085           0.974249   \n13           0.978632           0.970085           0.987124   \n14           0.961538           0.970085           0.974249   \n15           0.987179           0.978632           0.978541   \n16           0.961538           0.970085           0.969957   \n17           0.987179           0.978632           0.978541   \n18           0.970085           0.970085           0.974249   \n19           0.982906           0.965812           0.978541   \n20           0.970085           0.970085           0.974249   \n21           0.974359           0.965812           0.969957   \n22           0.978632           0.970085           0.987124   \n23           0.982906           0.965812           0.978541   \n24           0.974359           0.957265           0.965665   \n25           0.978632           0.965812           0.974249   \n26           0.970085           0.957265           0.965665   \n27           0.987179           0.978632           0.978541   \n28           0.970085           0.974359           0.982833   \n29           0.987179           0.978632           0.978541   \n30           0.987179           0.978632           0.978541   \n31           0.991453           0.970085           0.974249   \n32           0.978632           0.965812           0.974249   \n33           0.974359           0.965812           0.969957   \n34           0.978632           0.965812           0.974249   \n35           0.970085           0.965812           0.969957   \n36           0.970085           0.957265           0.965665   \n37           0.987179           0.978632           0.978541   \n38           0.974359           0.965812           0.969957   \n39           0.970085           0.965812           0.969957   \n40           0.978632           0.970085           0.987124   \n41           0.970085           0.965812           0.969957   \n42           0.991453           0.970085           0.974249   \n43           0.991453           0.970085           0.965665   \n44           0.978632           0.970085           0.987124   \n45           0.978632           0.965812           0.974249   \n46           0.978632           0.970085           0.987124   \n47           0.982906           0.965812           0.978541   \n48           0.970085           0.974359           0.982833   \n49           0.978632           0.970085           0.987124   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.957082           0.961538           0.970085   \n1            0.982833           0.982906           0.995726   \n2            0.982833           0.974359           0.982906   \n3            0.991416           0.978632           1.000000   \n4            0.995708           0.978632           0.995726   \n5            0.969957           0.974359           0.982906   \n6            0.965665           0.965812           0.978632   \n7            0.982833           0.974359           0.982906   \n8            0.957082           0.970085           0.974359   \n9            0.978541           0.965812           0.982906   \n10           0.957082           0.961538           0.970085   \n11           0.957082           0.978632           0.978632   \n12           0.965665           0.974359           0.978632   \n13           0.982833           0.974359           0.982906   \n14           0.957082           0.961538           0.970085   \n15           0.991416           0.978632           1.000000   \n16           0.957082           0.961538           0.970085   \n17           0.991416           0.978632           1.000000   \n18           0.957082           0.978632           0.978632   \n19           0.969957           0.974359           0.982906   \n20           0.957082           0.978632           0.978632   \n21           0.978541           0.965812           0.982906   \n22           0.982833           0.974359           0.982906   \n23           0.969957           0.974359           0.982906   \n24           0.957082           0.957265           0.974359   \n25           0.965665           0.965812           0.978632   \n26           0.957082           0.961538           0.974359   \n27           0.991416           0.978632           1.000000   \n28           0.982833           0.982906           0.991453   \n29           0.991416           0.978632           1.000000   \n30           0.991416           0.978632           1.000000   \n31           0.995708           0.982906           1.000000   \n32           0.965665           0.965812           0.978632   \n33           0.982833           0.965812           0.982906   \n34           0.965665           0.965812           0.978632   \n35           0.957082           0.970085           0.974359   \n36           0.957082           0.961538           0.974359   \n37           0.991416           0.978632           1.000000   \n38           0.978541           0.965812           0.982906   \n39           0.957082           0.970085           0.974359   \n40           0.982833           0.974359           0.982906   \n41           0.957082           0.970085           0.974359   \n42           0.995708           0.982906           1.000000   \n43           0.995708           0.978632           0.995726   \n44           0.982833           0.974359           0.982906   \n45           0.965665           0.965812           0.978632   \n46           0.982833           0.974359           0.982906   \n47           0.969957           0.974359           0.982906   \n48           0.982833           0.982906           0.991453   \n49           0.982833           0.974359           0.982906   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.944444           0.952790           0.969957   \n1            0.991453           0.957082           0.991416   \n2            0.995726           0.965665           0.982833   \n3            1.000000           0.978541           0.987124   \n4            0.974359           0.969957           0.978541   \n5            0.974359           0.969957           0.982833   \n6            0.974359           0.961373           0.974249   \n7            0.991453           0.965665           0.982833   \n8            0.952991           0.957082           0.965665   \n9            0.974359           0.948498           0.982833   \n10           0.944444           0.952790           0.969957   \n11           0.974359           0.961373           0.969957   \n12           0.961538           0.961373           0.978541   \n13           0.995726           0.965665           0.982833   \n14           0.948718           0.952790           0.969957   \n15           1.000000           0.978541           0.987124   \n16           0.944444           0.952790           0.969957   \n17           1.000000           0.978541           0.987124   \n18           0.974359           0.961373           0.969957   \n19           0.974359           0.969957           0.982833   \n20           0.974359           0.961373           0.969957   \n21           0.974359           0.948498           0.982833   \n22           0.991453           0.965665           0.982833   \n23           0.974359           0.969957           0.982833   \n24           0.961538           0.957082           0.978541   \n25           0.974359           0.961373           0.974249   \n26           0.961538           0.961373           0.978541   \n27           1.000000           0.978541           0.987124   \n28           0.991453           0.957082           0.991416   \n29           1.000000           0.978541           0.987124   \n30           1.000000           0.978541           0.987124   \n31           0.995726           0.969957           0.978541   \n32           0.974359           0.965665           0.974249   \n33           0.974359           0.948498           0.982833   \n34           0.974359           0.965665           0.974249   \n35           0.952991           0.957082           0.965665   \n36           0.961538           0.961373           0.978541   \n37           1.000000           0.978541           0.987124   \n38           0.974359           0.948498           0.982833   \n39           0.952991           0.957082           0.965665   \n40           0.991453           0.965665           0.982833   \n41           0.952991           0.957082           0.965665   \n42           0.995726           0.969957           0.978541   \n43           0.974359           0.969957           0.978541   \n44           0.995726           0.965665           0.982833   \n45           0.974359           0.965665           0.974249   \n46           0.995726           0.965665           0.982833   \n47           0.974359           0.969957           0.982833   \n48           0.991453           0.957082           0.991416   \n49           0.991453           0.965665           0.982833   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.948718            0.965812            0.961538   \n1             0.965812            0.974359            0.978632   \n2             0.974359            0.974359            0.974359   \n3             0.987179            0.987179            1.000000   \n4             0.978632            0.978632            0.978632   \n5             0.982906            0.974359            0.987179   \n6             0.965812            0.974359            0.978632   \n7             0.974359            0.974359            0.974359   \n8             0.965812            0.970085            0.965812   \n9             0.965812            0.961538            0.974359   \n10            0.948718            0.965812            0.961538   \n11            0.974359            0.974359            0.974359   \n12            0.961538            0.965812            0.974359   \n13            0.974359            0.974359            0.974359   \n14            0.948718            0.965812            0.961538   \n15            0.987179            0.987179            1.000000   \n16            0.948718            0.965812            0.961538   \n17            0.987179            0.987179            1.000000   \n18            0.974359            0.974359            0.974359   \n19            0.982906            0.974359            0.987179   \n20            0.974359            0.974359            0.974359   \n21            0.965812            0.961538            0.974359   \n22            0.974359            0.974359            0.974359   \n23            0.982906            0.974359            0.987179   \n24            0.961538            0.961538            0.965812   \n25            0.965812            0.974359            0.978632   \n26            0.957265            0.961538            0.965812   \n27            0.987179            0.987179            1.000000   \n28            0.965812            0.974359            0.978632   \n29            0.987179            0.987179            1.000000   \n30            0.987179            0.987179            1.000000   \n31            0.991453            0.982906            0.991453   \n32            0.965812            0.974359            0.978632   \n33            0.965812            0.961538            0.974359   \n34            0.965812            0.974359            0.982906   \n35            0.965812            0.970085            0.965812   \n36            0.957265            0.961538            0.965812   \n37            0.987179            0.987179            1.000000   \n38            0.965812            0.961538            0.974359   \n39            0.965812            0.970085            0.965812   \n40            0.974359            0.974359            0.974359   \n41            0.965812            0.970085            0.965812   \n42            0.991453            0.982906            0.991453   \n43            0.978632            0.978632            0.978632   \n44            0.974359            0.974359            0.974359   \n45            0.965812            0.974359            0.982906   \n46            0.974359            0.974359            0.974359   \n47            0.982906            0.974359            0.982906   \n48            0.965812            0.974359            0.978632   \n49            0.974359            0.974359            0.974359   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.969957            0.957082         0.960620        0.008569   \n1             0.982833            0.987124         0.980309        0.010108   \n2             0.978541            0.987124         0.979169        0.007303   \n3             0.991416            0.991416         0.988296        0.007410   \n4             0.974249            0.978541         0.979448        0.008604   \n5             0.978541            0.978541         0.976883        0.006013   \n6             0.974249            0.969957         0.970889        0.005895   \n7             0.978541            0.987124         0.978884        0.006710   \n8             0.974249            0.961373         0.964326        0.008067   \n9             0.982833            0.974249         0.972033        0.009103   \n10            0.969957            0.957082         0.960620        0.008569   \n11            0.978541            0.969957         0.970886        0.006869   \n12            0.974249            0.961373         0.968323        0.006588   \n13            0.978541            0.987124         0.979169        0.007303   \n14            0.969957            0.957082         0.960906        0.008940   \n15            0.991416            0.991416         0.988296        0.007410   \n16            0.969957            0.957082         0.960620        0.008569   \n17            0.991416            0.991416         0.988296        0.007410   \n18            0.978541            0.969957         0.970886        0.006869   \n19            0.978541            0.978541         0.976883        0.006013   \n20            0.978541            0.969957         0.970886        0.006869   \n21            0.982833            0.974249         0.972033        0.009103   \n22            0.978541            0.987124         0.978884        0.006710   \n23            0.978541            0.978541         0.976883        0.006013   \n24            0.974249            0.965665         0.965185        0.006961   \n25            0.974249            0.969957         0.970889        0.005895   \n26            0.974249            0.969957         0.965472        0.006516   \n27            0.991416            0.991416         0.988296        0.007410   \n28            0.982833            0.987124         0.980024        0.009722   \n29            0.991416            0.991416         0.988296        0.007410   \n30            0.991416            0.991416         0.988296        0.007410   \n31            0.982833            0.991416         0.985440        0.009099   \n32            0.974249            0.969957         0.971175        0.005519   \n33            0.974249            0.974249         0.972032        0.009238   \n34            0.974249            0.969957         0.971460        0.005987   \n35            0.974249            0.961373         0.964041        0.008806   \n36            0.974249            0.969957         0.965472        0.006516   \n37            0.991416            0.991416         0.988296        0.007410   \n38            0.982833            0.974249         0.972033        0.009103   \n39            0.974249            0.961373         0.964041        0.008806   \n40            0.978541            0.987124         0.978884        0.006710   \n41            0.974249            0.961373         0.964041        0.008806   \n42            0.982833            0.991416         0.985440        0.009099   \n43            0.974249            0.978541         0.979448        0.008604   \n44            0.978541            0.987124         0.979169        0.007303   \n45            0.974249            0.969957         0.971460        0.005987   \n46            0.978541            0.987124         0.979169        0.007303   \n47            0.978541            0.978541         0.976598        0.005606   \n48            0.982833            0.987124         0.979739        0.009570   \n49            0.978541            0.987124         0.978884        0.006710   \n\n    rank_test_score  \n0                48  \n1                10  \n2                15  \n3                 1  \n4                13  \n5                23  \n6                34  \n7                19  \n8                43  \n9                27  \n10               48  \n11               36  \n12               39  \n13               15  \n14               47  \n15                1  \n16               48  \n17                1  \n18               36  \n19               23  \n20               36  \n21               27  \n22               19  \n23               23  \n24               42  \n25               34  \n26               40  \n27                1  \n28               11  \n29                1  \n30                1  \n31                8  \n32               33  \n33               30  \n34               31  \n35               44  \n36               40  \n37                1  \n38               27  \n39               44  \n40               19  \n41               44  \n42                8  \n43               13  \n44               15  \n45               31  \n46               15  \n47               26  \n48               12  \n49               19  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_weights</th>\n      <th>param_p</th>\n      <th>param_n_neighbors</th>\n      <th>param_leaf_size</th>\n      <th>param_algorithm</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.003329</td>\n      <td>0.006906</td>\n      <td>0.023800</td>\n      <td>0.006793</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.944444</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960620</td>\n      <td>0.008569</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.013395</td>\n      <td>0.007127</td>\n      <td>0.023204</td>\n      <td>0.004185</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>6</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.991453</td>\n      <td>0.957082</td>\n      <td>0.991416</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.987124</td>\n      <td>0.980309</td>\n      <td>0.010108</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.001198</td>\n      <td>0.000651</td>\n      <td>0.017535</td>\n      <td>0.007605</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.979169</td>\n      <td>0.007303</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.002200</td>\n      <td>0.003165</td>\n      <td>0.009199</td>\n      <td>0.005104</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.001002</td>\n      <td>0.000365</td>\n      <td>0.016532</td>\n      <td>0.003182</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>6</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.965665</td>\n      <td>0.995708</td>\n      <td>0.978632</td>\n      <td>0.995726</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.978541</td>\n      <td>0.979448</td>\n      <td>0.008604</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.001003</td>\n      <td>0.000364</td>\n      <td>0.009131</td>\n      <td>0.001256</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976883</td>\n      <td>0.006013</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.019730</td>\n      <td>0.001689</td>\n      <td>0.049535</td>\n      <td>0.004287</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.970889</td>\n      <td>0.005895</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.007263</td>\n      <td>0.002646</td>\n      <td>0.016402</td>\n      <td>0.000711</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.028934</td>\n      <td>0.001290</td>\n      <td>0.225867</td>\n      <td>0.005600</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>16</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.944444</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964326</td>\n      <td>0.008067</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.001068</td>\n      <td>0.000251</td>\n      <td>0.020332</td>\n      <td>0.000866</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.974249</td>\n      <td>0.972033</td>\n      <td>0.009103</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.001138</td>\n      <td>0.000501</td>\n      <td>0.022261</td>\n      <td>0.003171</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.944444</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960620</td>\n      <td>0.008569</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.019595</td>\n      <td>0.002679</td>\n      <td>0.043137</td>\n      <td>0.001925</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>16</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.970886</td>\n      <td>0.006869</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.001136</td>\n      <td>0.001360</td>\n      <td>0.014797</td>\n      <td>0.000539</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>16</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.978541</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.968323</td>\n      <td>0.006588</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.000870</td>\n      <td>0.000341</td>\n      <td>0.017729</td>\n      <td>0.001062</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.979169</td>\n      <td>0.007303</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.027932</td>\n      <td>0.001288</td>\n      <td>0.093735</td>\n      <td>0.005120</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.944444</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.948718</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960906</td>\n      <td>0.008940</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.009331</td>\n      <td>0.000470</td>\n      <td>0.018335</td>\n      <td>0.001192</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.000934</td>\n      <td>0.000443</td>\n      <td>0.020597</td>\n      <td>0.000880</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.944444</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960620</td>\n      <td>0.008569</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.013263</td>\n      <td>0.000572</td>\n      <td>0.025004</td>\n      <td>0.001035</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.001001</td>\n      <td>0.000004</td>\n      <td>0.008798</td>\n      <td>0.000746</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>16</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.970886</td>\n      <td>0.006869</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.000866</td>\n      <td>0.000340</td>\n      <td>0.009201</td>\n      <td>0.001515</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976883</td>\n      <td>0.006013</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.001135</td>\n      <td>0.000498</td>\n      <td>0.008731</td>\n      <td>0.000572</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>16</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.970886</td>\n      <td>0.006869</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.001070</td>\n      <td>0.000444</td>\n      <td>0.022461</td>\n      <td>0.003440</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.974249</td>\n      <td>0.972033</td>\n      <td>0.009103</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.027396</td>\n      <td>0.000948</td>\n      <td>0.053468</td>\n      <td>0.001964</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.001468</td>\n      <td>0.001746</td>\n      <td>0.008267</td>\n      <td>0.000445</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976883</td>\n      <td>0.006013</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.013398</td>\n      <td>0.000488</td>\n      <td>0.033735</td>\n      <td>0.000998</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>11</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.957265</td>\n      <td>0.974359</td>\n      <td>0.961538</td>\n      <td>0.957082</td>\n      <td>0.978541</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965185</td>\n      <td>0.006961</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.028733</td>\n      <td>0.000677</td>\n      <td>0.209001</td>\n      <td>0.009219</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.970889</td>\n      <td>0.005895</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.000935</td>\n      <td>0.000250</td>\n      <td>0.020064</td>\n      <td>0.000680</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>11</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.978541</td>\n      <td>0.957265</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.965472</td>\n      <td>0.006516</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.006801</td>\n      <td>0.001470</td>\n      <td>0.020999</td>\n      <td>0.000633</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>1</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.013200</td>\n      <td>0.000401</td>\n      <td>0.025466</td>\n      <td>0.001544</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.991453</td>\n      <td>0.957082</td>\n      <td>0.991416</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.987124</td>\n      <td>0.980024</td>\n      <td>0.009722</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.012597</td>\n      <td>0.000712</td>\n      <td>0.024535</td>\n      <td>0.000719</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>1</td>\n      <td>30</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.009266</td>\n      <td>0.000575</td>\n      <td>0.022667</td>\n      <td>0.001012</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>1</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.018130</td>\n      <td>0.000718</td>\n      <td>0.038802</td>\n      <td>0.001047</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>6</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.995708</td>\n      <td>0.982906</td>\n      <td>1.000000</td>\n      <td>0.995726</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.991453</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.982833</td>\n      <td>0.991416</td>\n      <td>0.985440</td>\n      <td>0.009099</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.013466</td>\n      <td>0.001019</td>\n      <td>0.037733</td>\n      <td>0.001342</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.971175</td>\n      <td>0.005519</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.006400</td>\n      <td>0.000490</td>\n      <td>0.022000</td>\n      <td>0.000516</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.974249</td>\n      <td>0.972032</td>\n      <td>0.009238</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.000935</td>\n      <td>0.000250</td>\n      <td>0.014132</td>\n      <td>0.001086</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.971460</td>\n      <td>0.005987</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.000934</td>\n      <td>0.000442</td>\n      <td>0.015200</td>\n      <td>0.001513</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>16</td>\n      <td>1</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.940171</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964041</td>\n      <td>0.008806</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.000936</td>\n      <td>0.000442</td>\n      <td>0.020531</td>\n      <td>0.000617</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>11</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.978541</td>\n      <td>0.957265</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.965472</td>\n      <td>0.006516</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.001402</td>\n      <td>0.001307</td>\n      <td>0.005931</td>\n      <td>0.000853</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.000868</td>\n      <td>0.000500</td>\n      <td>0.021130</td>\n      <td>0.001856</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.974249</td>\n      <td>0.972033</td>\n      <td>0.009103</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.000933</td>\n      <td>0.000441</td>\n      <td>0.013868</td>\n      <td>0.000338</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>16</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.940171</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964041</td>\n      <td>0.008806</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.019396</td>\n      <td>0.002653</td>\n      <td>0.027803</td>\n      <td>0.002040</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>0.001066</td>\n      <td>0.000443</td>\n      <td>0.014132</td>\n      <td>0.001025</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>16</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.940171</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964041</td>\n      <td>0.008806</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>0.001070</td>\n      <td>0.000444</td>\n      <td>0.008462</td>\n      <td>0.000885</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>6</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.995708</td>\n      <td>0.982906</td>\n      <td>1.000000</td>\n      <td>0.995726</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.991453</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.982833</td>\n      <td>0.991416</td>\n      <td>0.985440</td>\n      <td>0.009099</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>0.001200</td>\n      <td>0.000542</td>\n      <td>0.014267</td>\n      <td>0.000572</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>6</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.965665</td>\n      <td>0.995708</td>\n      <td>0.978632</td>\n      <td>0.995726</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.978541</td>\n      <td>0.979448</td>\n      <td>0.008604</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>0.000868</td>\n      <td>0.000499</td>\n      <td>0.013199</td>\n      <td>0.001758</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.979169</td>\n      <td>0.007303</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>0.000866</td>\n      <td>0.000499</td>\n      <td>0.014466</td>\n      <td>0.001145</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.971460</td>\n      <td>0.005987</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>0.001001</td>\n      <td>0.000366</td>\n      <td>0.018399</td>\n      <td>0.001450</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.979169</td>\n      <td>0.007303</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>0.014126</td>\n      <td>0.001747</td>\n      <td>0.035271</td>\n      <td>0.003891</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976598</td>\n      <td>0.005606</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>0.001197</td>\n      <td>0.000400</td>\n      <td>0.016065</td>\n      <td>0.001843</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>6</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.982906</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.991453</td>\n      <td>0.957082</td>\n      <td>0.991416</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.987124</td>\n      <td>0.979739</td>\n      <td>0.009570</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>0.019797</td>\n      <td>0.000977</td>\n      <td>0.032333</td>\n      <td>0.001349</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "KNN_model = KNeighborsClassifier()\n",
    "\n",
    "KNN_params = dict()\n",
    "KNN_params['n_neighbors'] = [i for i in range(1, 20, 5)]\n",
    "KNN_params['weights'] = [\"uniform\", \"distance\"]\n",
    "KNN_params['algorithm'] = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "KNN_params['leaf_size'] = [1, 10, 30]\n",
    "KNN_params['p'] = [1, 2]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(KNN_model, KNN_params, n_iter=50, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "KNN = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(KNN.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[4]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAF8CAYAAADfMEA+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAzD0lEQVR4nO3debyd47n/8c9XhEhCzD0lgqoiFQmCKGqsQ2kMbZWDKir0dNCjNR0OStvT0qNoVWlraKtVWlTNrZnWkJjHSg0V1PQjghgS1++P+1nJys6exM66137u7/v12q+9n2Hvda21977W89zDdSsiMDOz/m+B3AGYmVnfcEI3M6sJJ3Qzs5pwQjczqwkndDOzmnBCNzOriQVzPfDSSy8dK620Uq6HNzPrlyZNmvRiRCzT2bFsCX2llVZi4sSJuR7ezKxfkvRkV8fc5GJmVhNO6GZmNdFjQpd0pqTnJd3fxXFJOkXSZEn3Slqn78M0M7Oe9KYN/Wzgx8Avuzi+LbBq9bEBcFr1+T175513mDJlCm+++ea8fLsVaNCgQQwfPpyBAwfmDsUsux4TekTcKGmlbk7ZAfhlpCpft0paXNIHI+LZ9xrMlClTWHTRRVlppZWQ9F6/3QoTEbz00ktMmTKFlVdeOXc4Ztn1RRv68sBTTdtTqn3v2ZtvvslSSy3lZG69IomlllrKd3RmlZZ2ikqaIGmipIkvvPBCV+e0MiTr5/z3YjZbXyT0p4EVmraHV/vmEhFnRMTYiBi7zDKdjovPbsCAAYwZM4bRo0ezzjrr8Ne//nWefs5JJ53EG2+80emxzTbbjBEjRtBci37HHXdk6NCh8/RYvXHWWWcxZswYxowZw0ILLcSoUaMYM2YMhx12WK9/RnfP6dJLL2Xttddm9OjRjBw5ktNPP72vQjezXuqLiUWXAF+RdB6pM3TqvLSfd2alwy7rix8zyxPf267HcxZZZBHuvvtuAK666ioOP/xwbrjhhvf8WCeddBJ77LEHgwcP7vT44osvzi233MLGG2/MK6+8wrPP9slL1qW9996bvffeG0iTuq677jqWXnrp9/QzunpO77zzDhMmTOD2229n+PDhvPXWWzzxxBPvK96IICJYYAGPrLW+0df5ZF70Jge9H70Ztvhb4G/AapKmSNpX0gGSDqhOuRx4DJgM/Az4z/kWbYu9+uqrLLHEErO2TzjhBNZbbz3WWmstjj76aABef/11tttuO0aPHs2aa67J7373O0455RSeeeYZNt98czbffPNOf/auu+7KeeedB8CFF17IzjvvPMfxzh4L0pX8uuuuy0c/+lHOOOOMWfuHDh3KEUccwejRoxk3bhzPPfdcr55jXzynadOmMWPGDJZaaikAFl54YVZbbTUAnnvuOXbaaSdGjx7N6NGjZ93xnHjiiay55pqsueaanHTSSQA88cQTrLbaanz+859nzTXX5KmnnurydTCzufVmlMtuPRwP4Mt9FlFm06dPZ8yYMbz55ps8++yzXHvttQBcffXVPProo9x+++1EBOPHj+fGG2/khRdeYLnlluOyy9K7/9SpUxk2bBgnnnhit1fBW265Jfvttx8zZ87kvPPO44wzzuC4447r9rE+/vGPc+aZZ7Lkkksyffp01ltvPT796U+z1FJL8frrrzNu3Di+853vcMghh/Czn/2MI488stvn2lfPackll2T8+PGsuOKKbLnllmy//fbstttuLLDAAnzta19j00035aKLLmLmzJm89tprTJo0ibPOOovbbruNiGCDDTZg0003ZYklluDRRx/lnHPOYdy4cd2+DmY2N9/PdtBocnn44Ye58sor+fznP09EcPXVV3P11Vez9tprs8466/Dwww/z6KOPMmrUKP785z9z6KGHctNNNzFs2LBePc6AAQPYeOONOe+885g+fTrNhcq6eiyAU045ZdZV+FNPPTVr/0ILLcT2228PwLrrrturJo++fE4///nPueaaa1h//fX5wQ9+wD777APAtddey5e+9KVZz3nYsGHcfPPN7LTTTgwZMoShQ4ey8847c9NNNwGw4oorMm7cuB5fBzObW7biXDncO+WVHs95N2afN2SFNfjX8y9w/T2Tef7VN9nzgAP57B57z3H+R4Yvzp133snll1/OkUceyZZbbslRRx3Vq3h23XVXdtppJ4455pg59kcEhx9+OPvvv/8c+6+//nr+8pe/8Le//Y3Bgwez2WabzRqyN3DgwFkjPgYMGMCMGTN6fPyuHgeYp+c0atQoRo0axZ577snKK6/M2Wef3eP3dDRkyJBexWdmc/MVejcen/x33p05k8WXWJKPbboFF//uXN54/TUAnnv2GV568QWeeeYZBg8ezB577MHBBx/MnXfeCcCiiy7KtGnTuv35m2yyCYcffji77TZnq9a///u/c+aZZ/Laa+mxnn76aZ5//nmmTp3KEkssweDBg3n44Ye59dZb39fz6+px3utzeu2117j++utnbd99992suOKKQGpaOu200wCYOXMmU6dOZZNNNuHiiy/mjTfe4PXXX+eiiy5ik0026XV8Zta5oq7Qe+OtN6ezy7+n5BIRHPfDnzBgwAA+tukWPD757+y5w9YADB4ylO+efDr3PfcYBx98MAsssAADBw6clbwmTJjANttsw3LLLcd1113X6WNJ4pvf/OZc+7feemseeughNtxwQyB1eP76179mm2224ac//SlrrLEGq6222qymiXnV1eNMnjz5PT2niOD4449n//33Z5FFFmHIkCGzrs5PPvlkJkyYwC9+8QsGDBjAaaedxoYbbsgXvvAF1l9/fQC++MUvsvbaa8/VTNRVfMsuu+z7et5mdaXmsdCtNHbs2OhYD/2hhx5ijTXW6PXP6E0Tyvy21vDFc4dQvPf6d1OSEobq9VZdXgtJkyJibGfH3ORiZlYTTuhmZjXhhG5mVhNtl9Bztelb/+S/F7PZ2iqhDxo0iJdeesn/pNYrjXrogwYNyh2KWVtoq2GLw4cPZ8qUKXRVWrej516ePp8j6tlD0xbJHULRGisWmVmbJfSBAwe+p5Vntq3JMCQzs77QVk0uZmY275zQzcxqwgndzKwmnNDNzGrCCd3MrCac0M3MasIJ3cysJpzQzcxqwgndzKwmnNDNzGrCCd3MrCbaqpaLzbu6LK9lZvPOV+hmZjXhhG5mVhNO6GZmNeGEbmZWE07oZmY14YRuZlYTTuhmZjXhcehWOx6Tb6XyFbqZWU30KqFL2kbSI5ImSzqsk+MjJF0n6S5J90r6ZN+HamZm3ekxoUsaAJwKbAuMBHaTNLLDaUcC50fE2sCuwE/6OlAzM+teb67Q1wcmR8RjEfE2cB6wQ4dzAlis+noY8EzfhWhmZr3Rm07R5YGnmranABt0OOcY4GpJXwWGAFv1SXRmZtZrfdUpuhtwdkQMBz4J/ErSXD9b0gRJEyVNfOGFF/rooc3MDHqX0J8GVmjaHl7ta7YvcD5ARPwNGAQs3fEHRcQZETE2IsYus8wy8xaxmZl1qjcJ/Q5gVUkrS1qI1Ol5SYdz/glsCSBpDVJC9yW4mVkL9ZjQI2IG8BXgKuAh0miWByQdK2l8ddo3gP0k3QP8FvhCRMT8CtrMzObWq5miEXE5cHmHfUc1ff0gsFHfhmZmZu+FZ4qamdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1USvErqkbSQ9ImmypMO6OGcXSQ9KekDSb/o2TDMz68mCPZ0gaQBwKvAJYApwh6RLIuLBpnNWBQ4HNoqIlyUtO78CNjOzzvXmCn19YHJEPBYRbwPnATt0OGc/4NSIeBkgIp7v2zDNzKwnvUnoywNPNW1PqfY1+wjwEUm3SLpV0jZ9FaCZmfVOj00u7+HnrApsBgwHbpQ0KiJeaT5J0gRgAsCIESP66KHNzAx6d4X+NLBC0/bwal+zKcAlEfFORDwO/J2U4OcQEWdExNiIGLvMMsvMa8xmZtaJ3iT0O4BVJa0saSFgV+CSDudcTLo6R9LSpCaYx/ouTDMz60mPCT0iZgBfAa4CHgLOj4gHJB0raXx12lXAS5IeBK4DDo6Il+ZX0GZmNrdetaFHxOXA5R32HdX0dQAHVR9mZpaBZ4qamdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1USvErqkbSQ9ImmypMO6Oe/TkkLS2L4L0czMeqPHhC5pAHAqsC0wEthN0shOzlsUOBC4ra+DNDOznvXmCn19YHJEPBYRbwPnATt0ct5xwPeBN/swPjMz66XeJPTlgaeatqdU+2aRtA6wQkRc1oexmZnZe/C+O0UlLQCcCHyjF+dOkDRR0sQXXnjh/T60mZk16U1CfxpYoWl7eLWvYVFgTeB6SU8A44BLOusYjYgzImJsRIxdZpll5j1qMzObS28S+h3AqpJWlrQQsCtwSeNgREyNiKUjYqWIWAm4FRgfERPnS8RmZtapHhN6RMwAvgJcBTwEnB8RD0g6VtL4+R2gmZn1zoK9OSkiLgcu77DvqC7O3ez9h2VmZu+VZ4qamdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1YQTuplZTTihm5nVhBO6mVlNOKGbmdWEE7qZWU04oZuZ1USvErqkbSQ9ImmypMM6OX6QpAcl3SvpGkkr9n2oZmbWnR4TuqQBwKnAtsBIYDdJIzucdhcwNiLWAn4PHN/XgZqZWfd6c4W+PjA5Ih6LiLeB84Admk+IiOsi4o1q81ZgeN+GaWZmPelNQl8eeKppe0q1ryv7Ale8n6DMzOy9W7Avf5ikPYCxwKZdHJ8ATAAYMWJEXz60mVnxenOF/jSwQtP28GrfHCRtBRwBjI+Itzr7QRFxRkSMjYixyyyzzLzEa2ZmXehNQr8DWFXSypIWAnYFLmk+QdLawOmkZP5834dpZmY96TGhR8QM4CvAVcBDwPkR8YCkYyWNr047ARgKXCDpbkmXdPHjzMxsPulVG3pEXA5c3mHfUU1fb9XHcZmZ2XvkmaJmZjXhhG5mVhNO6GZmNeGEbmZWE07oZmY14YRuZlYTTuhmZjXhhG5mVhNO6GZmNeGEbmZWE07oZmY14YRuZlYTTuhmZjXhhG5mVhNO6GZmNeGEbmZWE07oZmY14YRuZlYTTuhmZjXhhG5mVhNO6GZmNeGEbmZWE07oZmY14YRuZlYTTuhmZjXhhG5mVhNO6GZmNeGEbmZWE07oZmY14YRuZlYTTuhmZjXhhG5mVhNO6GZmNdGrhC5pG0mPSJos6bBOji8s6XfV8dskrdTnkZqZWbd6TOiSBgCnAtsCI4HdJI3scNq+wMsR8WHgh8D3+zpQMzPrXm+u0NcHJkfEYxHxNnAesEOHc3YAzqm+/j2wpST1XZhmZtaT3iT05YGnmranVPs6PSciZgBTgaX6IkAzM+udBVv5YJImABOqzdckPdLKx+/C0sCL8/rNqlfjkl+L5H29DuDXoplfi9n66LVYsasDvUnoTwMrNG0Pr/Z1ds4USQsCw4CXOv6giDgDOKMXj9kykiZGxNjccbQDvxaJX4fZ/FrM1h9ei940udwBrCppZUkLAbsCl3Q45xJgr+rrzwDXRkT0XZhmZtaTHq/QI2KGpK8AVwEDgDMj4gFJxwITI+IS4BfAryRNBv4fKembmVkL9aoNPSIuBy7vsO+opq/fBD7bt6G1TFs1AWXm1yLx6zCbX4vZ2v61kFtGzMzqwVP/zcxqwgndzKwmWjoOvR1IWgAYDSwHTAfuj4jn80aVh6RlgY1oei1IHd3vZg0sA78Wc5I0BHgzImbmjiWX/vg3UUwbuqRVgEOBrYBHgReAQcBHgDeA04Fz2vmX1VckbQ4cBiwJ3AU8z+zXYhVS+Yb/i4hXswXZIn4tkupCZ1dgd2A94C1gYdJEmsuA0yNicr4IW6c//02UlNB/C5wG3NRxjLykDwC7kQqMndPZ99eJpBOAH0XEPzs5tiCwPTAgIv7Q8uBazK9FIukG4C/AH0l3re9W+5cENgf+A7goIn6dL8rW6M9/E8UkdLPOSNo5Ii7MHUdukgZGxDvv9xzLq/iELmlr4OCI+ETuWFpJ0qakO5J7Je0CfBz4B/CTiHgrb3StI+nOiFgndxztRtJgUrnsJyPihdzxtJKkz3d3PCJ+2apY3qtiOkUlbQH8lNTBcTGpZvtZgIDv5Ius9SSdCqwFLCzp78BQ4EpSB9CZpHZUK4ik8cAppJneR5LWQHgOWEnSoSU0RTZZr4v940mVZds2oRdzhS7pLuC/gL+RFuv4NXBYRPw4a2AZSHowIkZKGkQqrLZsRMysatjfGxGjMofYMpLeADrr7BMQEbFWi0PKQtI9pNnew4DrgLUi4rFqpMc1Jf1NNKv+J3YnDah4EPhORNybN6quFXOFTvrnvL76+mJJT5eYzCtvQirZIOnJxtC0iAhJpbWRPg58KncQbeDdiPg7gKTHI+IxgIh4XtKMvKG1XtX5+QXgm8CtwGcioh3KfXerpIS+uKSdm7YXbN4urGNsWUkHka5CG19TbS+TL6ws3o6IJ3MH0QYWkLQEabLhu9XXjVXHipqAKOnLwIHANcA2EfFE3oh6r6Qml7O6ORwRsU/LgslM0tHdHY+Ib7Uqltwk/TgivpI7jtwkPQG8y+wk3iwi4kOtjSgfSe+Sxp6/ADQnyLZvhismoXdH0qfbcUyptYak1Ugraa1e7XoI+Fl/uMW2viepyxWBANr5js4JHZD0z4gYkTuOVpF0VDeHIyKOa1kwmUnaELiQNFP4LtJV2NrAfsDOEXFrxvBaRtIAYJGIeK3aHgcsVB2+KyKmZQvOes0JHZD0VESs0POZ9SDpG53sHgLsCywVEUNbHFI2kq4Avt/UYd7YvylpFNS2WQJrMUk/AJ6PiOOr7cdJtUsGAXdGxKE542ul6rnP1dRSfR0RsUrro+odJ3TKu0JvJmlRUgfQvsD5pBoVxRQrk/T3iPhIF8ceiYjVWh1TDtWw3vUiYkZjOyLWrobt3RQRG+eNsHUkLdVh1wLALqQRL3dGxKdbH1XvFDPKRdJ9zPmuO+sQ8IEWh5NdVaPjINIY23OAdSLi5bxRZdFdU8LrLYsivwUaybxyKMwaylrMHRtARLwEswqW7QkcDNwNbBcRD2YMrUfFJHRSQR1jVvGhnUlLao1qtJsWagVJp3SyX6RZgaVYSNKijbbyiLgaQNIwUrNLMSQNBPYhTUS8Gdixv1SaLKbJRZI6Vlmcl3PqoBqW9RYwg86HZS2WJbAMJO3V3fFSprxXcxG2Ag5oVBmsRnucBlwbET/IGV8rSZpC+t84CZir4mI7z1kpKaFfD/wB+GNzWUxJCwEbA3sB10XE2VkCNMtM0gHAf5M6yAFeA74XEafli6r1JJ1N582z0OZzVkpK6INIt1G7AysDr5BuJQcAV5OqDN6VLcAWkjS0p2aW3pxTB5J+BpwcEfd3cmwI8DngrYg4t+XBZVJ1lFPqUEVJwyJiahfHxkbExFbH1FvFJPRmVRvZ0sD0iHglczgtJ+kaUifPH4FJEfF6tf9DpMUMdiFNrPl9tiBbRNIY0lXpKNIwvcZKVqsCi5GqT/607iWFJe0B/Ca6WLGrWvHrgxFxc2sjaz1JdwBbdxwkIOkTwJntPMS5pE7RWaoi/c/mjiOXiNhS0ieB/YGNqrodM4BHSMuN7RUR/8oZY6tExN3ALtVIjrHAB0nrRz5U2EzRpYC7JE0CJjH7je3DwKakpegOyxdeS50BXCfpE41a8JL+g1Rme7uskfWgyCt0M5tbNVt0C1Jd/FlvbMAV0clybHUmaU/gEGBrUrPbAfSDQl1O6GZmnZD0WeBHpJEun4yIFzOH1CMndDOzJk2TEAWsSGp+eh1XW2w/VQ307wPLkn5BxY29tjlVTQ3fj4hv5o7F8nO1xX5E0mTgUxHxUO5YcqqS2AMRsXqPJxdA0q0RMS53HJZff56EWOIol+dKT+YAkdYQfUTSiNI6vLpwl6RLgAtoquHSzrMC54em1auaTSUNb727xeHkcp2kHichAmfnCa9rJV6hnwz8G3Axafo7UN4/LoCkG0m1v29nziQ2PltQmXSxolVbzwqcHyT9hjR880/Vru2Be4GVgAsa5XXrrD9PQiwxofsft1LV/J5LRNzQ6lisPVRv8p9sWuhiKGluwjakq/SROeNrtf42CbG4JpeI2Dt3DO0iIm6oOoBWjYi/SBpMugopjqSPkApRfSAi1pS0FjA+Ir6dObRWW5amO1fgHdJrMl1SrWfLdqa/TUIsajVvAEnDJV0k6fnq4w+ShueOKwdJ+wG/Jy2/Bqlc7MXZAsrrZ8DhpARGRNwL7Jo1ojzOBW6TdHS1mPgtwG+qujZtXQvcCkzowFnAJcBy1cefqn0l+jJpVuCrABHxKOkKrUSDI+L2DvtmdHpmjUVaT3Z/UrvxK6RyusdGxOsRsXvO2KxnxTW5AMtERHMCP1vS13MFk9lbEfF2WmUMJC1I12VD6+7FqgBVAEj6DP3oVruP3Qk8TZUfSh0JVd2VTI+Id6smudVJZRDeyRxal0pM6C9VleV+W23vBryUMZ6cbpD038AiVSW5/2T26IbSfJlUlGl1SU8DjwN75A2p9SR9FTgaeA6YyewFktt2duR8dCOwSVW87mrgDlJdl7a9UylxlMuKpPoMG5L+UP8KfK3QK5AFSItDb036x70K+Hk7TpholeqqbIGCa4FPBjZorKtZMkl3RsQ61ZvcIhFxvKS7I2JM7ti6UtwVejVtt7hx1p2pal//rPookqQ9IuLXHSfUNJqhIuLELIHl8xRpIpGlCaEbkq7I9632tfUosGISuqRDqnfYH9FJO3FEfC1DWFlIOj8idmkqQjSHdi4+NB8Mrj4vmjWK9vEYcL2ky5hz4l1pb2wAB5JGPl0UEQ9UC8BclzmmbhWT0El1nQHadvmoFvp69Xn7nEG0iVWqzw9GxAVZI2kP/6w+Fqo+ilTVOhrfPGs6Ih4D2vrCr7g29GZVG/LQiHg1dyyt1NQ2+KuI2DN3PDlVdylrkWZBrpM7Hmsf/bFgW0lX6MCsWhUHkHrw7wAWk3RyRJyQN7KWWqhaUutjVTnhORRW1+ZK4GVgqKTmN/aiyipLOikivi7pT3TeDFdiv1O/K9hW3BV6o5da0u7AOqR1EieV1G4saWNSR88upElWzUqta/PHiNghdxy5SFo3Iia5vs9s/bHuU3FX6MDAquDOjsCPI+IdSUW9q1Urt98saWJE/CJ3PO2g5GQOEBGTqi/HRMTJzcckHQgUl9D7Y92nEhP66cATwD3AjdW49NLa0LeIiGuBl0tvcpF0c0RsLGkas5cdayimyaXJXsDJHfZ9oZN9tVddoXfW/NS2V+jFNbl0RtKCEVFM3Q5J34qIo/vjLaXNH5J2A/6DtIDDTU2HFgNmRsSWWQLLSNKnmzYHATsBz7TzEOfiEnp1+3gWMA34OWmBh8Mi4uqsgVlWVR2XKRHxlqTNSCNfftkfamD3hepOdWXgf0n9Sg3TgHtLuuDpSjUq7uaI+FjuWLpSYrXFfaphilsDSwB7At/LG1Iekg6UtJiSn0u6U9LWuePK5A/ATEkfJtV0WQH4Td6QWicinoyI64GtgJuqTtBngeHM2QxVslVp82qkJSb0xh/nJ4FfRcQDlPsH2/zmthQFv7kB71ZXoTsBP4qIg4EPZo4phxuBQZKWJxWk2pM2XDuzFSRNk/Rq4zOpcN2huePqTomdopMkXU26vTxc0qLAu5ljyqX5ze2X1fTmUt/c3qnakfcCPlXtG5gxnlwUEW9I2pe0dubxku7OHVQOEdHvykGUmND3BcYAj1V/uEsB/W54Uh/xm9tse5MmnH0nIh6XtDLwq8wx5dDvClLNT5LGAx+vNq+PiEtzxtOTEjtFRfpj/VBEHCtpBPBvnaxWU3tVJ88Y0pvbK5KWBIZXy68Vq6p/vUKJr0M1segbwC0R8f2qINXX23lkx/wi6XvAeqRl+SCtnXBHRPx3vqi6V2JCP410FbpFRKzRKF4fEetlDq3lJG0E3B0Rr1eLfqwDnFyVGC6KpOtJZZUXBCYBz5OS2kHdfV9dSRoKEBGv5Y4lF0n3kiZavVttDwDuaudZ5SV2im4QEV8G3gSIiJcpt6rcacAbkkaTrsr+Afwyb0jZDKs6iHcm9SdsQBrxURRJoyTdBTwAPChpkqSP5o4ro8Wbvh6WK4jeKjGhv1O90zbWjlyGctuNZ1SrE+1AKoNwKuXWBV9Q0gdJ9W3aup10PjsdOCgiVoyIEaQ3+lIXQPlfUoGusyWdQ7pz+07mmLpVYqfoKcBFwLKSvgN8Bjgyb0jZTJN0OGntzI9XbeoljuwAOJa0BN/NEXFH1Xb8aOaYchgSEbMWcYiI66tl+YoTEb+tmuLWI10AHhoR/8obVfeKakOvEtY44P8BW5KG7V0TEQ91+401JenfSNO974iIm6oO4s0iotRml+JJugi4k9kjfPYA1o2InfJFlU9V62hjUkK/OSIuyhxSt4pK6ACS7oqItXPHYe1F0iDSML2Pkup2AO1diGl+qAYJfIuUxCDVdTmm6msqiqSfAB8Gflvt+hzwj6oPri2VmNB/APwNuDBKe/IdSBoH/AhYg9QxPAB4LSLavvOnr0m6AHiYdMdyLGlo60MRcWDWwDKp5iRE4aNcHgbWaOSJ6g7/gYhYI29kXSuxU3R/0gokb3WY1luiH5PG1j4KLAJ8EfhJ1ojy+XBE/A/wekScA2wHbJA5ppZrGuVyP/BANcplzdxxZTIZGNG0vUK1r20V1ynaH6fzzk8RMVnSgIiYCZxV/TMfnjuuDN6pPr9SJbB/0eaFmOaTxiiX6wCqypNnAG1bYbCvNS3DtyjwkKTbq+0NgLaegFhcQpfU2ULAU4EnCywR+oakhYC7JR1Pqq5X4l0bwBlV+/H/kJblGwoclTekLDzKBX6QO4B5VWIb+q2kGZH3VbtGkW4vhwFfKqkuelUD+3nSUMX/Ir0GP4mItr6ttPnHo1z6txIT+oXA/1Rlc5E0ktQJdgipo3RMxvCsxSR1O7U/Ik5sVSztwKNcZquGLH6f1PSm6qOtlyUsrskF+EgjmQNExIOSVo+Ix0qpHCvpPjpZK7GhnWtVzAfuU2lSJe7iCnF14XjgU/1pnkqJCf2BqkDXedX250g1KxZmdsdY3W2fO4B2ERHfyh1DO5C0NPBl4GXgTOAEYBNSfZ9vFNoM91x/SuZQZpPLIsB/MvuW8hbSUL03gcEljLutlln7QETc0mH/RsC/IuIfeSJrPUknAJMj4vQO+/cHVo6Iwzr/znqp6uJPJN2xbElapegSUlLfPSI2yxZci1VNLQCbAv8GXAy81TgeERdmCKtXikvoMCupj4iIR3LHkoOkS4HDI+K+DvtHAd+NiE91/p31I2kSMLbjJLNqEsm9EVHEGGxJ90TE6Gq9gCerwlyNY3eX1Lck6axuDkc7zx4ursmlWoHkBNLMyJUljQGOjYjxWQNrrQ90TOYAEXGfpJUyxJPTwp3NGI6Idwtbjm8mpGwl6cUOx4qqRhoR/XYFs+ISOnA0sD5wPUBE3F0tN1aSxbs5tkirgmgT0yWtGhFzVFaUtCowPVNMOXxI0iWkkRyNr6m2S/v/AEDSKZ3sngpMjIg/tjqe3igxob8TEVM7XHyV1u40UdJ+ETFHnWtJXyTVfC7JUcAVkr7N7Oc+ljRb9uu5gspgh6avO06s6bcTbd6nQcDqpFIhAJ8GHgdGS9o8Ir6eK7CuFNeGLukXwDXAYaRf0NeAgRFxQNbAWkjSB0g14d9mziS2ELBTu9d87mvVVP+DgUZ7+f3ADzprlrJyVJMQN6rKYiBpQdK4/I2B+yJiZM74OlNiQh8MHAFsTbqdvBI4LiLe6vYba0jS5sxOYg9ExLU54zFrJ5IeAdaPiKnV9jDg9ohYrV3LcBeX0DuStBrwzYjYL3csZtY+JO1LWs3setLF38eB75Lqox8TEQfni65zxSR0SWuR2gKXI40rPZVUPnYD4P8i4of5ojNrD5I+GxEX9LSvFNU6s+tXm3dExDM54+lJSZX1fgb8htRu/iJwN2kW3IedzM1m6ax0clHllCWtXn1eB/gg8FT18W9dVGttGyVdoc8xOULSYxHxoYwhZVeVRZ1ejbn+CKlH/4qIKKUEwizV8z+NNEZ/zeqObnxEfDtzaC0haVvgk8AuwO+aDi0GjIyI9Tv9xhqSdEZETJB0XSeHIyK2aHlQvVRSQn+YtDpPY7ziuaTlxgQQEXdmCi2bapbkJsASpBIIdwBvR8TuWQPLQNINpJEupzc6uyTdX9BM0dHAGFLl0eY68NOA60qsttgflZTQO3u3bWjrd935RdKdEbGOpK8Ci0TE8aVN826QdEdErNc8eqHE10LSwBLv0DpTjYg7iFQmZEI12Wy1iLg0c2hdKmZiUURsnjuGNiRJG5IWRN632jcgYzw5vShpFapJZpI+Q1rBqTTrSzoGWJGUHxo1wEtsnjyLNE+jsfze06RJRk7o1pa+TurwuigiHpD0IaC7O5k6+zJp7czVJT1NmhG4R96QsvgFafWqSVT1XQq2SkR8TtJuABHxRrvX93FCL1hE3ADcUN1aEhGPUejiBtVz36rqKF4gIqbljimTqRFxRe4g2sTbVWXWxl3bKjSV0W1HJQ1btA4kbSjpQeDhanu0pJ9kDisLSR+oykL8PiKmSRpZTSwpzXWSTqj+NtZpfOQOKpOjSTPJV5B0LqlkyCF5Q+peMZ2iDZJ2Aq5tms67OLBZRFycM64cJN0GfAa4pMSRHc0kXUFqMz2iqgu+IHBXRIzKHFpL9cehevOTpKWAcaS+hFsjomNp4bZSYkKfa+RCu9ZlmN8k3RYRG3QY2XFPRIzOHVureZSLNUga0d3xiPhnq2J5r0psQ++smanE1wHgKUkfA0LSQOBAoF+todiHXq+uxhrtpeNIta+LUlXi/C6wXERsK2kksGFE/CJzaK10GenvoLkDNIBlgGVp45FgJbahT5R0oqRVqo8TKa8GeMMBpNEdy5OGZI2ptkt0EGkNzVUk3QL8Evhq3pCyOBu4ilTzCODvlFUXnogYFRFrVZ9HAZ8iTbx7jTZ/LUq8Mv0q8D/Mnt78Z8pNYlHirNCOJA0gLQi8KbAa6crskUIn2CwdEedLOhwgImZIKnL4YjWR6AiqAn7A19r9b6K4hB4Rr5MWtzC4VdLdwJnAlZ2trVmCiJgpabeqSNsDuePJrPimp2rBkyOAjwLHA/s2Frlod8V0iko6KSK+LulPdLLkXGGLRANpmiiwFbAPsB5wPnB2RPw9a2AZSPohMJB05/Z6Y39pNX6qIYo/Ii18cj+p3fgzEXFv1sBaqLojeYrUlj5XIo+Itp2rUVJCXzciJknatLPj1SSbYlWrF/0aGALcAxwWEX/LG9X8J+nqiNjaw/Vmq4ZsFtv0JGmv7o5HxDmtiuW9KiahN0g6MCJO7mlfCapb6z2APYHnSNO+LyF1jl4QEbVf7b3UIasdSdoiIq6VtHNnxyPiwlbHZO9dcW3owF5Ax+T9hU72leBvwK+AHSNiStP+iZJ+mimmVhvWVRKDohLZpsC1pBEdHQVQyuvQrxVzhV4V2PkP0ordNzUdWgyYGRFbZgksI0kqtSO0QdJLwB+Zc8xxQ0TEPi0OyWyelXSF/ldSOdSlSUOQGqYBxXT4dLC0pENIvfmDGjsLazd+0kkbJB3U3fGIOLFVsbQLSRtFxC097WsnxUwsiognI+J60qiOm6pO0GeB4XR+dVaCc0mFuVYGvgU8QVq1qCSl/u47WrT6GAt8iTTZbHnS5LNSi3P9qJf72kYxTS4NXnZtNkmTImJdSfdGxFrVvjsiYr3csbWKpDUj4v7ccbQLSTcC2zXKB0taFLgsIj6eN7LWqRZ9+RhpVmjzAvKLATu1c62jYq7Qmygi3gB2Bn4SEZ8lNTmUqDEc7VlJ20laG1gyZ0Ct5mQ+lw8Abzdtv13tK8lCwFBSk/SiTR+vkqqTtq2S2tAbvOzabN+WNAz4BulWcjHSajVWrl8Ct0u6qNreEWjbcdfzQ9PCL2dHxJMAkhYAhkbEq3mj616JTS6bkhLYLRHx/WrZta+38+wvs1aStC5pNBjAjRFxV854cpH0G1IfwkxS0+xiwMkRcULWwLpRXEI3kDQI+BzwMvAn0iosmwD/AI5r9yL+84OkjYBj8OLIAEhaljlHPrVtDfD5pVEPX9LupI7hw4BJjf6mdlRMk4truczhl6T28yGku5X7gR+TrsrOBrbPFlk+XhwZkDSeNKx3OeB5YARpJFSJ/UwDq3UCdgR+HBHvSGrrK+BiEjppRiTAD7JG0R5GRsSaVc2OKRHRqG9zpaR7cgaWkRdHTo4jLbn2l4hYu6rxs0fmmHI5nTSU9x7gRkkrkjpG25abXAok6c6IWKfj151tl0LS90id4xfStLJ7gdUWJ0bE2OqNfe2IeLfUZQk7I2nBiJiRO46ulHSFDoCk+5i7yWUqMBH4dkS81PqoWm64pFNI7cSNr6m2l88XVlYbVJ/HNu0LoKRZswCvSBoK3AicK+l5msoJl6Sr5fhIzXNtqbgrdEnHk9pIf1Pt2hUYDPwL2DgiOitOVCv9uTyozV+ShgDTSXNUdgeGAecWcqEzB0lXAGcBR0TE6KqJ8q5qWbq2VGJCn6tJobFP0n3t/Muy+acaj3800JgReQNwbEQUs1pPtRTfXyJi89yxtIPGrOnmEsuNkS+ZQ+tSiTNFB0hav7EhaT1mTyxq27Yxm+/OJBVq26X6eJV0dVaMapm1d6s3N+uHy/EV14YOfBE4s2onFOkfd9/qVvN/s0ZmOa0SEZ9u2v5Wtd5qaV4D7pP0Z+Zciq/EiXcHkRZ8WUXSLVTL8eUNqXvFJfSIuAMY1bgK6XBLfX6eqKwNTJe0cUTcDLMmGk3PHFMOF+LFLBrNT5tWH/1mOb4S29CLbyttkPQR4DTgA9W49LWA8RHx7cyhtZykMaSaJcNI/7z/D/hCRJQ6Lr94km6PiPV7PrN9lJjQ/0CaGdkYybEnMDoiulyGrK4k3QAcDJze1Olzf0SsmTeyfCQtBtDuRZj6mqQdgOERcWq1fRupiQHgkIj4fbbgMpH0Q2Ag8DvmbH5q27kJxTW54LbSZoMj4nZpjjUeiuoYlrRHRPy644o9jdekoJV6DiEN4W1YGFiPVB7iLKC4hE5aLB3g2KZ9bT03ocSE7rbS2V6UtAqze/E/Q1rFqSRDqs+LZo0iv4Ui4qmm7ZursecvVQMGitMfh2+W2OQymlScqjE062Vgr4gobl3RqnTwGaTVWV4GHgd2b9SAtnJImhwRH+7i2D8iYpVWx9QOJG3H3GvuHtv1d+RV3Dj0iGjUpVgLWKtqO27bW6j57MmI2IrUVrp6RGxcajKXdLykxSQNlHSNpBcklVSU6jZJ+3XcKWl/4PYM8WQn6aekMtNfJXWUf5ZUXrltFXeF3hlJ/4yIEbnjaDVJ/wSuJHX6XBsF/zE01b7eiVQ++CDS4g5FFKWq6p9fTCpM1uj0W5fUlr5jRDyXKbRsGmvtNn0eClwREZvkjq0rJbahd6bUld9XJyWvLwO/kHQpcF6jf6Ewjf+F7YALImJqh87iWouI54GPSdqC2bXPL4uIazOGlVujb+0NScsBLwEfzBhPj5zQkyKvTKvFss8Hzpe0BHAyaVx+iWusXirpYdI/8ZckLQO8mTmmlqsSeMlJvNmlkhYHTiDdtQTw86wR9aCYJhdJ0+g8cQtYJCKKfHOr1lj9HLANqYTw7yLiD3mjykPSkqSFLmZKGgwsFhH/yh2X5SdpYWBQu09ALCah29wkPQHcRbpKvyQiiqt7LWmLiLhWUqcTyyKi+Gnwpare1L8BjIiI/SStCqwWEZdmDq1LRV6Vlk7ShIg4gzTKp6gZkZ3YlNTE0Fkd/MB1TUp2FmmN2Q2r7aeBC4C2Tei+Qi+QpM9GxAVNKxXNodDKemZzaFqOr7keelsvx+cr9DKtXH2elDWKNiLpu8DxEfFKtb0E8I2IODJrYJbT25IWYfZM6lVoWm+2HfkKvUCSPhURf8odRztpvgpr2lfkgtmWSPoEcCQwErga2IhUgfP6nHF1xwm9YNXQvENJf7DNU5uLmzkr6V5gvYh4q9peBJgYER/t/jutzqoVi8aRRsPdGhEvZg6pW25yKdu5pFmi2wEHAHsBL2SNKJ9zgWskNZad25vZJZatIJI6zhq/r/o8WNKIiPhnq2PqLV+hF0zSpIhYtzG1udp3R0Sslzu2HCRtA2xVbf45Iq7KGY/lIek+Urt581ThINU8WjYi2nbina/Qy9ZYTuvZqqrcM8CSGePJ7SFgRkT8RdJgSYtGxLTcQVlrRcSo5m1JK5GaJrcCvpsjpt4qrtqizeHb1ZJ83wC+SZrW/F95Q8qjqjT4e+D0atfypGJVVihJq0o6G7iCNCJsZET8KG9U3XOTixmp2iKwPnBb05jj+zperVn9SVoTOIJUpOx44LcRMTNvVL3jJpcCSTqqm8MREce1LJj28VZEvN2osChpQQot2mbcAzwFXEZ6k1+/ufJmO0+8c0IvU2c1W4YA+wJLASUm9Bsk/TewSDX++D8Bj9Uv0z65A5hXbnIpnKRFgQNJyfx84P+q2thFkbQA6TXYmjS64Srg5yUv+mH9jxN6oapSsQcBu5PGW58cES/njSqvaqIVEVHqWHzr5zzKpUCSTgDuAKYBoyLimFKTuZJjJL0IPAI8Uq0n2l0/g1lb8hV6gSS9SyoyNIM5O/5E6hRdLEtgGUg6CNgWmBARj1f7PgScBlwZET/MGZ+1nqQVIuKpLo5t38710J3QrWiS7gI+0bFGR9X8cnXHgl1Wf9VShNtExBMd9u8DHBERq2QJrBfc5GKlG9hZwaWqHX1ghngsv4OAq6sVigCQdDhp0t2m2aLqBQ9btNK9PY/HrKYi4nJJbwFXSNoR+CJpPPrH272vyU0uVjRJM+l8XL5IiwL7Kr1QkjYBLgL+CuwSEW9mDqlHTuhmZk0kTWN2tcWFSUXsZtIPBg04oZuZ1YQ7Rc3MasIJ3cysJpzQzcxqwgndzKwmnNDNzGrCCd3MrCb+P8/ZbYQmZiNzAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                               Best Mean Test Score\nLogistic Regression (LR)                   0.972313\nNaïve Bayes Classifier (NB)                0.864722\nDecision Tree Classifier (DT)              0.830470\nGradient Boosting (GB)                     0.954907\nK Nearest Neighbours (KNN)                 0.988296",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Best Mean Test Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression (LR)</th>\n      <td>0.972313</td>\n    </tr>\n    <tr>\n      <th>Naïve Bayes Classifier (NB)</th>\n      <td>0.864722</td>\n    </tr>\n    <tr>\n      <th>Decision Tree Classifier (DT)</th>\n      <td>0.830470</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting (GB)</th>\n      <td>0.954907</td>\n    </tr>\n    <tr>\n      <th>K Nearest Neighbours (KNN)</th>\n      <td>0.988296</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(Evaluation_Results)\n",
    "df.plot(kind = 'bar')\n",
    "plt.show()\n",
    "\n",
    "Evaluation_Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Citation :\n",
    "Stephanie Glen. \"Regularization: Simple Definition, L1 & L2 Penalties\" From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/regularization/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}