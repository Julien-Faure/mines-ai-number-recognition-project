{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7529c77",
   "metadata": {},
   "source": [
    "# Rapport du projet de résolution de problème\n",
    "\n",
    "- Paul Achard\n",
    "- Julien Faure\n",
    "\n",
    "    \n",
    "- *Date : 20/01/2022*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b10f2c0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680a1e74",
   "metadata": {},
   "source": [
    "## Problématique\n",
    "\n",
    "Notre problématique est d'identifier un chiffre à partir d'une image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d9f6f",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "- Trouver un modèle permettant d'identifier un chiffre à partir d'une image\n",
    "- Comparer différentes stratégies de solveur pour le modèle trouvé\n",
    "\n",
    "## Analyse du dataset\n",
    "\n",
    "Identifier un chiffre à partir d'une image est une tache qui peut s'avérer très complexe. Afin d'avoir une difficulté raisonnable et adapté au contexte de ce projet, nous avons fixé certains paramètres dans notre dataset.\n",
    "\n",
    "- La résolution de nos images est identiques pour tout le dataset. Cette résolution est **8 pixels par 8 pixels**.\n",
    "- Chaque pixel est codé sur **4 bits**.\n",
    "- Les images contiennent uniquement un chiffre sans élément parasite, sans effet et sans traitement.\n",
    "\n",
    "Nous utilisons le dataset `digits` de `seaborn`.\n",
    "\n",
    "### Forme du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c0d0854",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data Shape (1797, 64)\n",
      "Label Data Shape (1797,)\n"
     ]
    }
   ],
   "source": [
    "# Import du dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Affiche le nombre d'images et leur format\n",
    "print(\"Image Data Shape\" , digits.data.shape)\n",
    "\n",
    "# Affiche le nombre de labels\n",
    "print(\"Label Data Shape\", digits.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e9f33",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir ci-dessus, le dataset est composé de **1797** images labellisées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42e085",
   "metadata": {},
   "source": [
    "### Répartition des images du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76ddfc05",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Text(0.5, 0, 'n° labellisé'), Text(0, 0.5, 'Occurrence')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphique = sns.countplot(x=digits.target)\n",
    "graphique.set(xlabel=\"n° labellisé\", ylabel = \"Occurrence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93457c8b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Les données labellisées sont équitablement distribuées (environ 175 par label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8b6d6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Représentation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5322bb7f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x216 with 10 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Création de 10 figures de 10 par 3 pixels\n",
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 3))\n",
    "\n",
    "# Pour chaque figure, on affiche l'image du chiffre et son label en titre\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "    ax.set_title(\"Label: %i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac3dd0f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Représentation des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f0586c1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "\n",
      " Type de chaque valeur : <class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "# Affiche le tableau représentant la première image\n",
    "print(digits.images[0])\n",
    "print(\"\\n Type de chaque valeur :\", type(digits.images[0][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a25c9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Comme nous l'avons vu précédemment, chaque image possède une résolution de 8x8 pixels en niveaux de gris codés sur 4bits par pixel.\n",
    "\n",
    "Dans notre programme, une image est représentée par une matrice de dimension 8x8. Chaque élément représente un pixel avec un niveau de gris codé sur 4bits (de 0 à 15). Plus la valeur est élevée, plus la couleur est foncée.\n",
    "\n",
    "> Exemple :\n",
    "* 0 : Blanc\n",
    "* 15 : Noir"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "60 fits failed out of a total of 360.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.96139044 0.96139044 0.94692324 0.95212291 0.94542934        nan\n",
      " 0.96139044 0.96176288 0.94543762 0.95212291 0.94542934        nan\n",
      " 0.96139044 0.96139044 0.94432444 0.95212291 0.94542934        nan\n",
      " 0.96139044 0.96176288 0.94432444 0.95212291 0.94542934        nan]\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [ 1.  1.  1.  1.  1. nan  1.  1.  1.  1.  1. nan  1.  1.  1.  1.  1. nan\n",
      "  1.  1.  1.  1.  1. nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n0        0.584403      0.104062         0.000734        0.000772       3   \n1        2.008419      1.209724         0.000399        0.000489       3   \n2        0.047065      0.004738         0.000335        0.000474       3   \n3        0.122170      0.010602         0.000267        0.000442       3   \n4        0.045334      0.009023         0.000400        0.000490       3   \n5        0.000400      0.000490         0.000000        0.000000       3   \n6        0.681299      0.207431         0.000468        0.000500       4   \n7        1.676612      0.593168         0.000171        0.000355       4   \n8        0.043160      0.002165         0.000400        0.000490       4   \n9        0.116088      0.004604         0.000558        0.000497       4   \n10       0.035548      0.002042         0.000468        0.000500       4   \n11       0.000267      0.000443         0.000000        0.000000       4   \n12       0.824094      0.275461         0.000267        0.000442       5   \n13       1.771293      0.600389         0.000200        0.000401       5   \n14       0.045731      0.001694         0.000200        0.000401       5   \n15       0.131458      0.013329         0.000267        0.000442       5   \n16       0.037143      0.001933         0.000401        0.000491       5   \n17       0.000133      0.000340         0.000000        0.000000       5   \n18       0.622099      0.157090         0.000568        0.000479       6   \n19       1.937098      0.619552         0.000334        0.000472       6   \n20       0.047062      0.003143         0.000534        0.000499       6   \n21       0.143530      0.035502         0.000401        0.000491       6   \n22       0.039483      0.003417         0.000602        0.000491       6   \n23       0.000133      0.000339         0.000000        0.000000       6   \n\n   param_penalty param_solver  \\\n0             l2    newton-cg   \n1             l2        lbfgs   \n2             l2    liblinear   \n3           none    newton-cg   \n4           none        lbfgs   \n5           none    liblinear   \n6             l2    newton-cg   \n7             l2        lbfgs   \n8             l2    liblinear   \n9           none    newton-cg   \n10          none        lbfgs   \n11          none    liblinear   \n12            l2    newton-cg   \n13            l2        lbfgs   \n14            l2    liblinear   \n15          none    newton-cg   \n16          none        lbfgs   \n17          none    liblinear   \n18            l2    newton-cg   \n19            l2        lbfgs   \n20            l2    liblinear   \n21          none    newton-cg   \n22          none        lbfgs   \n23          none    liblinear   \n\n                                               params  split0_test_score  \\\n0    {'C': 3, 'penalty': 'l2', 'solver': 'newton-cg'}           0.966667   \n1        {'C': 3, 'penalty': 'l2', 'solver': 'lbfgs'}           0.966667   \n2    {'C': 3, 'penalty': 'l2', 'solver': 'liblinear'}           0.933333   \n3   {'C': 3, 'penalty': 'none', 'solver': 'newton-...           0.944444   \n4      {'C': 3, 'penalty': 'none', 'solver': 'lbfgs'}           0.955556   \n5   {'C': 3, 'penalty': 'none', 'solver': 'libline...                NaN   \n6    {'C': 4, 'penalty': 'l2', 'solver': 'newton-cg'}           0.966667   \n7        {'C': 4, 'penalty': 'l2', 'solver': 'lbfgs'}           0.966667   \n8    {'C': 4, 'penalty': 'l2', 'solver': 'liblinear'}           0.927778   \n9   {'C': 4, 'penalty': 'none', 'solver': 'newton-...           0.944444   \n10     {'C': 4, 'penalty': 'none', 'solver': 'lbfgs'}           0.955556   \n11  {'C': 4, 'penalty': 'none', 'solver': 'libline...                NaN   \n12   {'C': 5, 'penalty': 'l2', 'solver': 'newton-cg'}           0.966667   \n13       {'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}           0.966667   \n14   {'C': 5, 'penalty': 'l2', 'solver': 'liblinear'}           0.927778   \n15  {'C': 5, 'penalty': 'none', 'solver': 'newton-...           0.944444   \n16     {'C': 5, 'penalty': 'none', 'solver': 'lbfgs'}           0.955556   \n17  {'C': 5, 'penalty': 'none', 'solver': 'libline...                NaN   \n18   {'C': 6, 'penalty': 'l2', 'solver': 'newton-cg'}           0.966667   \n19       {'C': 6, 'penalty': 'l2', 'solver': 'lbfgs'}           0.966667   \n20   {'C': 6, 'penalty': 'l2', 'solver': 'liblinear'}           0.927778   \n21  {'C': 6, 'penalty': 'none', 'solver': 'newton-...           0.944444   \n22     {'C': 6, 'penalty': 'none', 'solver': 'lbfgs'}           0.955556   \n23  {'C': 6, 'penalty': 'none', 'solver': 'libline...                NaN   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.961111           0.966667           0.955307   \n1            0.961111           0.966667           0.955307   \n2            0.944444           0.972222           0.944134   \n3            0.950000           0.955556           0.938547   \n4            0.944444           0.955556           0.944134   \n5                 NaN                NaN                NaN   \n6            0.961111           0.966667           0.955307   \n7            0.961111           0.966667           0.955307   \n8            0.944444           0.972222           0.944134   \n9            0.950000           0.955556           0.938547   \n10           0.944444           0.955556           0.944134   \n11                NaN                NaN                NaN   \n12           0.961111           0.966667           0.955307   \n13           0.961111           0.966667           0.955307   \n14           0.944444           0.972222           0.944134   \n15           0.950000           0.955556           0.938547   \n16           0.944444           0.955556           0.944134   \n17                NaN                NaN                NaN   \n18           0.961111           0.966667           0.955307   \n19           0.961111           0.966667           0.960894   \n20           0.944444           0.972222           0.944134   \n21           0.950000           0.955556           0.938547   \n22           0.944444           0.955556           0.944134   \n23                NaN                NaN                NaN   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.977654           0.966667           0.972222   \n1            0.977654           0.966667           0.972222   \n2            0.960894           0.955556           0.950000   \n3            0.977654           0.966667           0.955556   \n4            0.955307           0.961111           0.955556   \n5                 NaN                NaN                NaN   \n6            0.977654           0.966667           0.972222   \n7            0.977654           0.966667           0.972222   \n8            0.955307           0.955556           0.950000   \n9            0.977654           0.966667           0.955556   \n10           0.955307           0.961111           0.955556   \n11                NaN                NaN                NaN   \n12           0.977654           0.966667           0.972222   \n13           0.977654           0.966667           0.972222   \n14           0.949721           0.955556           0.950000   \n15           0.977654           0.966667           0.955556   \n16           0.955307           0.961111           0.955556   \n17                NaN                NaN                NaN   \n18           0.977654           0.966667           0.972222   \n19           0.977654           0.966667           0.972222   \n20           0.949721           0.955556           0.950000   \n21           0.977654           0.966667           0.955556   \n22           0.955307           0.961111           0.955556   \n23                NaN                NaN                NaN   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.933333           0.960894           0.960894   \n1            0.933333           0.960894           0.960894   \n2            0.927778           0.944134           0.966480   \n3            0.922222           0.955307           0.960894   \n4            0.916667           0.960894           0.932961   \n5                 NaN                NaN                NaN   \n6            0.933333           0.960894           0.960894   \n7            0.933333           0.960894           0.960894   \n8            0.922222           0.944134           0.966480   \n9            0.922222           0.955307           0.960894   \n10           0.916667           0.960894           0.932961   \n11                NaN                NaN                NaN   \n12           0.933333           0.960894           0.960894   \n13           0.933333           0.960894           0.960894   \n14           0.922222           0.944134           0.966480   \n15           0.922222           0.955307           0.960894   \n16           0.916667           0.960894           0.932961   \n17                NaN                NaN                NaN   \n18           0.933333           0.960894           0.960894   \n19           0.933333           0.960894           0.960894   \n20           0.922222           0.944134           0.966480   \n21           0.922222           0.955307           0.960894   \n22           0.916667           0.960894           0.932961   \n23                NaN                NaN                NaN   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.972222            0.972222            0.955556   \n1             0.972222            0.972222            0.955556   \n2             0.938889            0.955556            0.933333   \n3             0.961111            0.961111            0.933333   \n4             0.950000            0.955556            0.927778   \n5                  NaN                 NaN                 NaN   \n6             0.972222            0.972222            0.955556   \n7             0.972222            0.972222            0.955556   \n8             0.938889            0.955556            0.933333   \n9             0.961111            0.961111            0.933333   \n10            0.950000            0.955556            0.927778   \n11                 NaN                 NaN                 NaN   \n12            0.972222            0.972222            0.955556   \n13            0.972222            0.972222            0.955556   \n14            0.933333            0.950000            0.933333   \n15            0.961111            0.961111            0.933333   \n16            0.950000            0.955556            0.927778   \n17                 NaN                 NaN                 NaN   \n18            0.972222            0.972222            0.955556   \n19            0.972222            0.972222            0.955556   \n20            0.933333            0.950000            0.933333   \n21            0.961111            0.961111            0.933333   \n22            0.950000            0.955556            0.927778   \n23                 NaN                 NaN                 NaN   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.949721            0.949721         0.961390        0.011049   \n1             0.949721            0.949721         0.961390        0.011049   \n2             0.938547            0.938547         0.946923        0.012486   \n3             0.944134            0.955307         0.952123        0.013427   \n4             0.932961            0.932961         0.945429        0.013220   \n5                  NaN                 NaN              NaN             NaN   \n6             0.949721            0.949721         0.961390        0.011049   \n7             0.949721            0.955307         0.961763        0.010739   \n8             0.932961            0.938547         0.945438        0.013493   \n9             0.944134            0.955307         0.952123        0.013427   \n10            0.932961            0.932961         0.945429        0.013220   \n11                 NaN                 NaN              NaN             NaN   \n12            0.949721            0.949721         0.961390        0.011049   \n13            0.949721            0.949721         0.961390        0.011049   \n14            0.932961            0.938547         0.944324        0.013305   \n15            0.944134            0.955307         0.952123        0.013427   \n16            0.932961            0.932961         0.945429        0.013220   \n17                 NaN                 NaN              NaN             NaN   \n18            0.949721            0.949721         0.961390        0.011049   \n19            0.949721            0.949721         0.961763        0.010931   \n20            0.932961            0.938547         0.944324        0.013305   \n21            0.944134            0.955307         0.952123        0.013427   \n22            0.932961            0.932961         0.945429        0.013220   \n23                 NaN                 NaN              NaN             NaN   \n\n    rank_test_score  split0_train_score  split1_train_score  \\\n0                 3                 1.0                 1.0   \n1                 3                 1.0                 1.0   \n2                13                 1.0                 1.0   \n3                 9                 1.0                 1.0   \n4                15                 1.0                 1.0   \n5                22                 NaN                 NaN   \n6                 3                 1.0                 1.0   \n7                 1                 1.0                 1.0   \n8                14                 1.0                 1.0   \n9                 9                 1.0                 1.0   \n10               15                 1.0                 1.0   \n11               21                 NaN                 NaN   \n12                3                 1.0                 1.0   \n13                3                 1.0                 1.0   \n14               19                 1.0                 1.0   \n15                9                 1.0                 1.0   \n16               15                 1.0                 1.0   \n17               23                 NaN                 NaN   \n18                3                 1.0                 1.0   \n19                1                 1.0                 1.0   \n20               19                 1.0                 1.0   \n21                9                 1.0                 1.0   \n22               15                 1.0                 1.0   \n23               24                 NaN                 NaN   \n\n    split2_train_score  split3_train_score  split4_train_score  \\\n0                  1.0                 1.0                 1.0   \n1                  1.0                 1.0                 1.0   \n2                  1.0                 1.0                 1.0   \n3                  1.0                 1.0                 1.0   \n4                  1.0                 1.0                 1.0   \n5                  NaN                 NaN                 NaN   \n6                  1.0                 1.0                 1.0   \n7                  1.0                 1.0                 1.0   \n8                  1.0                 1.0                 1.0   \n9                  1.0                 1.0                 1.0   \n10                 1.0                 1.0                 1.0   \n11                 NaN                 NaN                 NaN   \n12                 1.0                 1.0                 1.0   \n13                 1.0                 1.0                 1.0   \n14                 1.0                 1.0                 1.0   \n15                 1.0                 1.0                 1.0   \n16                 1.0                 1.0                 1.0   \n17                 NaN                 NaN                 NaN   \n18                 1.0                 1.0                 1.0   \n19                 1.0                 1.0                 1.0   \n20                 1.0                 1.0                 1.0   \n21                 1.0                 1.0                 1.0   \n22                 1.0                 1.0                 1.0   \n23                 NaN                 NaN                 NaN   \n\n    split5_train_score  split6_train_score  split7_train_score  \\\n0                  1.0                 1.0                 1.0   \n1                  1.0                 1.0                 1.0   \n2                  1.0                 1.0                 1.0   \n3                  1.0                 1.0                 1.0   \n4                  1.0                 1.0                 1.0   \n5                  NaN                 NaN                 NaN   \n6                  1.0                 1.0                 1.0   \n7                  1.0                 1.0                 1.0   \n8                  1.0                 1.0                 1.0   \n9                  1.0                 1.0                 1.0   \n10                 1.0                 1.0                 1.0   \n11                 NaN                 NaN                 NaN   \n12                 1.0                 1.0                 1.0   \n13                 1.0                 1.0                 1.0   \n14                 1.0                 1.0                 1.0   \n15                 1.0                 1.0                 1.0   \n16                 1.0                 1.0                 1.0   \n17                 NaN                 NaN                 NaN   \n18                 1.0                 1.0                 1.0   \n19                 1.0                 1.0                 1.0   \n20                 1.0                 1.0                 1.0   \n21                 1.0                 1.0                 1.0   \n22                 1.0                 1.0                 1.0   \n23                 NaN                 NaN                 NaN   \n\n    split8_train_score  split9_train_score  split10_train_score  \\\n0                  1.0                 1.0                  1.0   \n1                  1.0                 1.0                  1.0   \n2                  1.0                 1.0                  1.0   \n3                  1.0                 1.0                  1.0   \n4                  1.0                 1.0                  1.0   \n5                  NaN                 NaN                  NaN   \n6                  1.0                 1.0                  1.0   \n7                  1.0                 1.0                  1.0   \n8                  1.0                 1.0                  1.0   \n9                  1.0                 1.0                  1.0   \n10                 1.0                 1.0                  1.0   \n11                 NaN                 NaN                  NaN   \n12                 1.0                 1.0                  1.0   \n13                 1.0                 1.0                  1.0   \n14                 1.0                 1.0                  1.0   \n15                 1.0                 1.0                  1.0   \n16                 1.0                 1.0                  1.0   \n17                 NaN                 NaN                  NaN   \n18                 1.0                 1.0                  1.0   \n19                 1.0                 1.0                  1.0   \n20                 1.0                 1.0                  1.0   \n21                 1.0                 1.0                  1.0   \n22                 1.0                 1.0                  1.0   \n23                 NaN                 NaN                  NaN   \n\n    split11_train_score  split12_train_score  split13_train_score  \\\n0                   1.0                  1.0                  1.0   \n1                   1.0                  1.0                  1.0   \n2                   1.0                  1.0                  1.0   \n3                   1.0                  1.0                  1.0   \n4                   1.0                  1.0                  1.0   \n5                   NaN                  NaN                  NaN   \n6                   1.0                  1.0                  1.0   \n7                   1.0                  1.0                  1.0   \n8                   1.0                  1.0                  1.0   \n9                   1.0                  1.0                  1.0   \n10                  1.0                  1.0                  1.0   \n11                  NaN                  NaN                  NaN   \n12                  1.0                  1.0                  1.0   \n13                  1.0                  1.0                  1.0   \n14                  1.0                  1.0                  1.0   \n15                  1.0                  1.0                  1.0   \n16                  1.0                  1.0                  1.0   \n17                  NaN                  NaN                  NaN   \n18                  1.0                  1.0                  1.0   \n19                  1.0                  1.0                  1.0   \n20                  1.0                  1.0                  1.0   \n21                  1.0                  1.0                  1.0   \n22                  1.0                  1.0                  1.0   \n23                  NaN                  NaN                  NaN   \n\n    split14_train_score  mean_train_score  std_train_score  \n0                   1.0               1.0              0.0  \n1                   1.0               1.0              0.0  \n2                   1.0               1.0              0.0  \n3                   1.0               1.0              0.0  \n4                   1.0               1.0              0.0  \n5                   NaN               NaN              NaN  \n6                   1.0               1.0              0.0  \n7                   1.0               1.0              0.0  \n8                   1.0               1.0              0.0  \n9                   1.0               1.0              0.0  \n10                  1.0               1.0              0.0  \n11                  NaN               NaN              NaN  \n12                  1.0               1.0              0.0  \n13                  1.0               1.0              0.0  \n14                  1.0               1.0              0.0  \n15                  1.0               1.0              0.0  \n16                  1.0               1.0              0.0  \n17                  NaN               NaN              NaN  \n18                  1.0               1.0              0.0  \n19                  1.0               1.0              0.0  \n20                  1.0               1.0              0.0  \n21                  1.0               1.0              0.0  \n22                  1.0               1.0              0.0  \n23                  NaN               NaN              NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_C</th>\n      <th>param_penalty</th>\n      <th>param_solver</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n      <th>split0_train_score</th>\n      <th>split1_train_score</th>\n      <th>split2_train_score</th>\n      <th>split3_train_score</th>\n      <th>split4_train_score</th>\n      <th>split5_train_score</th>\n      <th>split6_train_score</th>\n      <th>split7_train_score</th>\n      <th>split8_train_score</th>\n      <th>split9_train_score</th>\n      <th>split10_train_score</th>\n      <th>split11_train_score</th>\n      <th>split12_train_score</th>\n      <th>split13_train_score</th>\n      <th>split14_train_score</th>\n      <th>mean_train_score</th>\n      <th>std_train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.584403</td>\n      <td>0.104062</td>\n      <td>0.000734</td>\n      <td>0.000772</td>\n      <td>3</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 3, 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n      <td>0.966667</td>\n      <td>0.961111</td>\n      <td>0.966667</td>\n      <td>0.955307</td>\n      <td>0.977654</td>\n      <td>0.966667</td>\n      <td>0.972222</td>\n      <td>0.933333</td>\n      <td>0.960894</td>\n      <td>0.960894</td>\n      <td>0.972222</td>\n      <td>0.972222</td>\n      <td>0.955556</td>\n      <td>0.949721</td>\n      <td>0.949721</td>\n      <td>0.961390</td>\n      <td>0.011049</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.008419</td>\n      <td>1.209724</td>\n      <td>0.000399</td>\n      <td>0.000489</td>\n      <td>3</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 3, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.966667</td>\n      <td>0.961111</td>\n      <td>0.966667</td>\n      <td>0.955307</td>\n      <td>0.977654</td>\n      <td>0.966667</td>\n      <td>0.972222</td>\n      <td>0.933333</td>\n      <td>0.960894</td>\n      <td>0.960894</td>\n      <td>0.972222</td>\n      <td>0.972222</td>\n      <td>0.955556</td>\n      <td>0.949721</td>\n      <td>0.949721</td>\n      <td>0.961390</td>\n      <td>0.011049</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.047065</td>\n      <td>0.004738</td>\n      <td>0.000335</td>\n      <td>0.000474</td>\n      <td>3</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 3, 'penalty': 'l2', 'solver': 'liblinear'}</td>\n      <td>0.933333</td>\n      <td>0.944444</td>\n      <td>0.972222</td>\n      <td>0.944134</td>\n      <td>0.960894</td>\n      <td>0.955556</td>\n      <td>0.950000</td>\n      <td>0.927778</td>\n      <td>0.944134</td>\n      <td>0.966480</td>\n      <td>0.938889</td>\n      <td>0.955556</td>\n      <td>0.933333</td>\n      <td>0.938547</td>\n      <td>0.938547</td>\n      <td>0.946923</td>\n      <td>0.012486</td>\n      <td>13</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.122170</td>\n      <td>0.010602</td>\n      <td>0.000267</td>\n      <td>0.000442</td>\n      <td>3</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 3, 'penalty': 'none', 'solver': 'newton-...</td>\n      <td>0.944444</td>\n      <td>0.950000</td>\n      <td>0.955556</td>\n      <td>0.938547</td>\n      <td>0.977654</td>\n      <td>0.966667</td>\n      <td>0.955556</td>\n      <td>0.922222</td>\n      <td>0.955307</td>\n      <td>0.960894</td>\n      <td>0.961111</td>\n      <td>0.961111</td>\n      <td>0.933333</td>\n      <td>0.944134</td>\n      <td>0.955307</td>\n      <td>0.952123</td>\n      <td>0.013427</td>\n      <td>9</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.045334</td>\n      <td>0.009023</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>3</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 3, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.955556</td>\n      <td>0.944444</td>\n      <td>0.955556</td>\n      <td>0.944134</td>\n      <td>0.955307</td>\n      <td>0.961111</td>\n      <td>0.955556</td>\n      <td>0.916667</td>\n      <td>0.960894</td>\n      <td>0.932961</td>\n      <td>0.950000</td>\n      <td>0.955556</td>\n      <td>0.927778</td>\n      <td>0.932961</td>\n      <td>0.932961</td>\n      <td>0.945429</td>\n      <td>0.013220</td>\n      <td>15</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 3, 'penalty': 'none', 'solver': 'libline...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>22</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.681299</td>\n      <td>0.207431</td>\n      <td>0.000468</td>\n      <td>0.000500</td>\n      <td>4</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 4, 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n      <td>0.966667</td>\n      <td>0.961111</td>\n      <td>0.966667</td>\n      <td>0.955307</td>\n      <td>0.977654</td>\n      <td>0.966667</td>\n      <td>0.972222</td>\n      <td>0.933333</td>\n      <td>0.960894</td>\n      <td>0.960894</td>\n      <td>0.972222</td>\n      <td>0.972222</td>\n      <td>0.955556</td>\n      <td>0.949721</td>\n      <td>0.949721</td>\n      <td>0.961390</td>\n      <td>0.011049</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.676612</td>\n      <td>0.593168</td>\n      <td>0.000171</td>\n      <td>0.000355</td>\n      <td>4</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 4, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.966667</td>\n      <td>0.961111</td>\n      <td>0.966667</td>\n      <td>0.955307</td>\n      <td>0.977654</td>\n      <td>0.966667</td>\n      <td>0.972222</td>\n      <td>0.933333</td>\n      <td>0.960894</td>\n      <td>0.960894</td>\n      <td>0.972222</td>\n      <td>0.972222</td>\n      <td>0.955556</td>\n      <td>0.949721</td>\n      <td>0.955307</td>\n      <td>0.961763</td>\n      <td>0.010739</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.043160</td>\n      <td>0.002165</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>4</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 4, 'penalty': 'l2', 'solver': 'liblinear'}</td>\n      <td>0.927778</td>\n      <td>0.944444</td>\n      <td>0.972222</td>\n      <td>0.944134</td>\n      <td>0.955307</td>\n      <td>0.955556</td>\n      <td>0.950000</td>\n      <td>0.922222</td>\n      <td>0.944134</td>\n      <td>0.966480</td>\n      <td>0.938889</td>\n      <td>0.955556</td>\n      <td>0.933333</td>\n      <td>0.932961</td>\n      <td>0.938547</td>\n      <td>0.945438</td>\n      <td>0.013493</td>\n      <td>14</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.116088</td>\n      <td>0.004604</td>\n      <td>0.000558</td>\n      <td>0.000497</td>\n      <td>4</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 4, 'penalty': 'none', 'solver': 'newton-...</td>\n      <td>0.944444</td>\n      <td>0.950000</td>\n      <td>0.955556</td>\n      <td>0.938547</td>\n      <td>0.977654</td>\n      <td>0.966667</td>\n      <td>0.955556</td>\n      <td>0.922222</td>\n      <td>0.955307</td>\n      <td>0.960894</td>\n      <td>0.961111</td>\n      <td>0.961111</td>\n      <td>0.933333</td>\n      <td>0.944134</td>\n      <td>0.955307</td>\n      <td>0.952123</td>\n      <td>0.013427</td>\n      <td>9</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.035548</td>\n      <td>0.002042</td>\n      <td>0.000468</td>\n      <td>0.000500</td>\n      <td>4</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 4, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.955556</td>\n      <td>0.944444</td>\n      <td>0.955556</td>\n      <td>0.944134</td>\n      <td>0.955307</td>\n      <td>0.961111</td>\n      <td>0.955556</td>\n      <td>0.916667</td>\n      <td>0.960894</td>\n      <td>0.932961</td>\n      <td>0.950000</td>\n      <td>0.955556</td>\n      <td>0.927778</td>\n      <td>0.932961</td>\n      <td>0.932961</td>\n      <td>0.945429</td>\n      <td>0.013220</td>\n      <td>15</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>4</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 4, 'penalty': 'none', 'solver': 'libline...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>21</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.824094</td>\n      <td>0.275461</td>\n      <td>0.000267</td>\n      <td>0.000442</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n      <td>0.966667</td>\n      <td>0.961111</td>\n      <td>0.966667</td>\n      <td>0.955307</td>\n      <td>0.977654</td>\n      <td>0.966667</td>\n      <td>0.972222</td>\n      <td>0.933333</td>\n      <td>0.960894</td>\n      <td>0.960894</td>\n      <td>0.972222</td>\n      <td>0.972222</td>\n      <td>0.955556</td>\n      <td>0.949721</td>\n      <td>0.949721</td>\n      <td>0.961390</td>\n      <td>0.011049</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.771293</td>\n      <td>0.600389</td>\n      <td>0.000200</td>\n      <td>0.000401</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.966667</td>\n      <td>0.961111</td>\n      <td>0.966667</td>\n      <td>0.955307</td>\n      <td>0.977654</td>\n      <td>0.966667</td>\n      <td>0.972222</td>\n      <td>0.933333</td>\n      <td>0.960894</td>\n      <td>0.960894</td>\n      <td>0.972222</td>\n      <td>0.972222</td>\n      <td>0.955556</td>\n      <td>0.949721</td>\n      <td>0.949721</td>\n      <td>0.961390</td>\n      <td>0.011049</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.045731</td>\n      <td>0.001694</td>\n      <td>0.000200</td>\n      <td>0.000401</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'liblinear'}</td>\n      <td>0.927778</td>\n      <td>0.944444</td>\n      <td>0.972222</td>\n      <td>0.944134</td>\n      <td>0.949721</td>\n      <td>0.955556</td>\n      <td>0.950000</td>\n      <td>0.922222</td>\n      <td>0.944134</td>\n      <td>0.966480</td>\n      <td>0.933333</td>\n      <td>0.950000</td>\n      <td>0.933333</td>\n      <td>0.932961</td>\n      <td>0.938547</td>\n      <td>0.944324</td>\n      <td>0.013305</td>\n      <td>19</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.131458</td>\n      <td>0.013329</td>\n      <td>0.000267</td>\n      <td>0.000442</td>\n      <td>5</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'newton-...</td>\n      <td>0.944444</td>\n      <td>0.950000</td>\n      <td>0.955556</td>\n      <td>0.938547</td>\n      <td>0.977654</td>\n      <td>0.966667</td>\n      <td>0.955556</td>\n      <td>0.922222</td>\n      <td>0.955307</td>\n      <td>0.960894</td>\n      <td>0.961111</td>\n      <td>0.961111</td>\n      <td>0.933333</td>\n      <td>0.944134</td>\n      <td>0.955307</td>\n      <td>0.952123</td>\n      <td>0.013427</td>\n      <td>9</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.037143</td>\n      <td>0.001933</td>\n      <td>0.000401</td>\n      <td>0.000491</td>\n      <td>5</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.955556</td>\n      <td>0.944444</td>\n      <td>0.955556</td>\n      <td>0.944134</td>\n      <td>0.955307</td>\n      <td>0.961111</td>\n      <td>0.955556</td>\n      <td>0.916667</td>\n      <td>0.960894</td>\n      <td>0.932961</td>\n      <td>0.950000</td>\n      <td>0.955556</td>\n      <td>0.927778</td>\n      <td>0.932961</td>\n      <td>0.932961</td>\n      <td>0.945429</td>\n      <td>0.013220</td>\n      <td>15</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.000133</td>\n      <td>0.000340</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'libline...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>23</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.622099</td>\n      <td>0.157090</td>\n      <td>0.000568</td>\n      <td>0.000479</td>\n      <td>6</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 6, 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n      <td>0.966667</td>\n      <td>0.961111</td>\n      <td>0.966667</td>\n      <td>0.955307</td>\n      <td>0.977654</td>\n      <td>0.966667</td>\n      <td>0.972222</td>\n      <td>0.933333</td>\n      <td>0.960894</td>\n      <td>0.960894</td>\n      <td>0.972222</td>\n      <td>0.972222</td>\n      <td>0.955556</td>\n      <td>0.949721</td>\n      <td>0.949721</td>\n      <td>0.961390</td>\n      <td>0.011049</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1.937098</td>\n      <td>0.619552</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>6</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 6, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.966667</td>\n      <td>0.961111</td>\n      <td>0.966667</td>\n      <td>0.960894</td>\n      <td>0.977654</td>\n      <td>0.966667</td>\n      <td>0.972222</td>\n      <td>0.933333</td>\n      <td>0.960894</td>\n      <td>0.960894</td>\n      <td>0.972222</td>\n      <td>0.972222</td>\n      <td>0.955556</td>\n      <td>0.949721</td>\n      <td>0.949721</td>\n      <td>0.961763</td>\n      <td>0.010931</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.047062</td>\n      <td>0.003143</td>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>6</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 6, 'penalty': 'l2', 'solver': 'liblinear'}</td>\n      <td>0.927778</td>\n      <td>0.944444</td>\n      <td>0.972222</td>\n      <td>0.944134</td>\n      <td>0.949721</td>\n      <td>0.955556</td>\n      <td>0.950000</td>\n      <td>0.922222</td>\n      <td>0.944134</td>\n      <td>0.966480</td>\n      <td>0.933333</td>\n      <td>0.950000</td>\n      <td>0.933333</td>\n      <td>0.932961</td>\n      <td>0.938547</td>\n      <td>0.944324</td>\n      <td>0.013305</td>\n      <td>19</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.143530</td>\n      <td>0.035502</td>\n      <td>0.000401</td>\n      <td>0.000491</td>\n      <td>6</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 6, 'penalty': 'none', 'solver': 'newton-...</td>\n      <td>0.944444</td>\n      <td>0.950000</td>\n      <td>0.955556</td>\n      <td>0.938547</td>\n      <td>0.977654</td>\n      <td>0.966667</td>\n      <td>0.955556</td>\n      <td>0.922222</td>\n      <td>0.955307</td>\n      <td>0.960894</td>\n      <td>0.961111</td>\n      <td>0.961111</td>\n      <td>0.933333</td>\n      <td>0.944134</td>\n      <td>0.955307</td>\n      <td>0.952123</td>\n      <td>0.013427</td>\n      <td>9</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.039483</td>\n      <td>0.003417</td>\n      <td>0.000602</td>\n      <td>0.000491</td>\n      <td>6</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 6, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.955556</td>\n      <td>0.944444</td>\n      <td>0.955556</td>\n      <td>0.944134</td>\n      <td>0.955307</td>\n      <td>0.961111</td>\n      <td>0.955556</td>\n      <td>0.916667</td>\n      <td>0.960894</td>\n      <td>0.932961</td>\n      <td>0.950000</td>\n      <td>0.955556</td>\n      <td>0.927778</td>\n      <td>0.932961</td>\n      <td>0.932961</td>\n      <td>0.945429</td>\n      <td>0.013220</td>\n      <td>15</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.000133</td>\n      <td>0.000339</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 6, 'penalty': 'none', 'solver': 'libline...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>24</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "a6be3c4a",
   "metadata": {},
   "source": [
    "\n",
    "## Modèle\n",
    "\n",
    "\n",
    "\n",
    "Pour résoudre notre problème, il existe plusieurs modèles possibles :\n",
    "* Modèle de régression logistique.\n",
    "* \n",
    "\n",
    "### Régression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79254ea7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Redimensionne le tableau en vecteur\n",
    "print(digits.images[0].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faed23a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 18 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.96590528 0.96590528 0.96265083 0.95500065 0.95569428        nan\n",
      " 0.96172813 0.96172813 0.95406665 0.95500065 0.95569428        nan\n",
      " 0.96149584 0.96149584 0.95244306 0.95500065 0.95569428        nan]\n",
      "  warnings.warn(\n",
      "e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.99988401 0.99988401 0.99338878 1.         1.                nan\n",
      " 1.         1.         0.99768035 1.         1.                nan\n",
      " 1.         1.         0.99785427 1.         1.                nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n0        0.681888      0.071425         0.000400        0.000490     0.1   \n1        1.798371      0.689899         0.000534        0.000499     0.1   \n2        0.081474      0.007299         0.000467        0.000499     0.1   \n3        0.162815      0.010653         0.000267        0.000443     0.1   \n4        0.060589      0.013825         0.000801        0.001722     0.1   \n5        0.000534      0.000499         0.000000        0.000000     0.1   \n6        0.891478      0.133195         0.000534        0.000499       5   \n7        2.350740      0.449218         0.000400        0.000490       5   \n8        0.146600      0.006981         0.000400        0.000490       5   \n9        0.162815      0.008316         0.000334        0.000472       5   \n10       0.051247      0.004265         0.000267        0.000443       5   \n11       0.000467      0.000499         0.000000        0.000000       5   \n12       0.887741      0.115713         0.000467        0.000499      10   \n13       2.288684      0.853630         0.000267        0.000443      10   \n14       0.173892      0.011727         0.000534        0.000499      10   \n15       0.163482      0.007802         0.000467        0.000499      10   \n16       0.053182      0.003484         0.000400        0.000490      10   \n17       0.000534      0.000499         0.000000        0.000000      10   \n\n   param_penalty param_solver  \\\n0             l2    newton-cg   \n1             l2        lbfgs   \n2             l2    liblinear   \n3           none    newton-cg   \n4           none        lbfgs   \n5           none    liblinear   \n6             l2    newton-cg   \n7             l2        lbfgs   \n8             l2    liblinear   \n9           none    newton-cg   \n10          none        lbfgs   \n11          none    liblinear   \n12            l2    newton-cg   \n13            l2        lbfgs   \n14            l2    liblinear   \n15          none    newton-cg   \n16          none        lbfgs   \n17          none    liblinear   \n\n                                               params  split0_test_score  \\\n0   {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-...           0.954861   \n1      {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}           0.954861   \n2   {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...           0.958333   \n3   {'C': 0.1, 'penalty': 'none', 'solver': 'newto...           0.944444   \n4    {'C': 0.1, 'penalty': 'none', 'solver': 'lbfgs'}           0.944444   \n5   {'C': 0.1, 'penalty': 'none', 'solver': 'libli...                NaN   \n6    {'C': 5, 'penalty': 'l2', 'solver': 'newton-cg'}           0.944444   \n7        {'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}           0.944444   \n8    {'C': 5, 'penalty': 'l2', 'solver': 'liblinear'}           0.947917   \n9   {'C': 5, 'penalty': 'none', 'solver': 'newton-...           0.944444   \n10     {'C': 5, 'penalty': 'none', 'solver': 'lbfgs'}           0.944444   \n11  {'C': 5, 'penalty': 'none', 'solver': 'libline...                NaN   \n12  {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}           0.944444   \n13      {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}           0.944444   \n14  {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}           0.951389   \n15  {'C': 10, 'penalty': 'none', 'solver': 'newton...           0.944444   \n16    {'C': 10, 'penalty': 'none', 'solver': 'lbfgs'}           0.944444   \n17  {'C': 10, 'penalty': 'none', 'solver': 'liblin...                NaN   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.958333           0.972125           0.968641   \n1            0.958333           0.972125           0.968641   \n2            0.958333           0.961672           0.961672   \n3            0.954861           0.965157           0.951220   \n4            0.951389           0.961672           0.958188   \n5                 NaN                NaN                NaN   \n6            0.958333           0.965157           0.965157   \n7            0.958333           0.965157           0.965157   \n8            0.947917           0.951220           0.947735   \n9            0.954861           0.965157           0.951220   \n10           0.951389           0.961672           0.958188   \n11                NaN                NaN                NaN   \n12           0.958333           0.965157           0.965157   \n13           0.958333           0.965157           0.965157   \n14           0.947917           0.947735           0.947735   \n15           0.954861           0.965157           0.951220   \n16           0.951389           0.961672           0.958188   \n17                NaN                NaN                NaN   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.979094           0.961806           0.968750   \n1            0.979094           0.961806           0.968750   \n2            0.958188           0.965278           0.968750   \n3            0.961672           0.947917           0.958333   \n4            0.958188           0.958333           0.951389   \n5                 NaN                NaN                NaN   \n6            0.968641           0.965278           0.958333   \n7            0.968641           0.965278           0.958333   \n8            0.947735           0.961806           0.965278   \n9            0.961672           0.947917           0.958333   \n10           0.958188           0.958333           0.951389   \n11                NaN                NaN                NaN   \n12           0.968641           0.965278           0.958333   \n13           0.968641           0.965278           0.958333   \n14           0.944251           0.961806           0.958333   \n15           0.961672           0.947917           0.958333   \n16           0.958188           0.958333           0.951389   \n17                NaN                NaN                NaN   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.982578           0.965157           0.965157   \n1            0.982578           0.965157           0.965157   \n2            0.982578           0.958188           0.951220   \n3            0.965157           0.958188           0.951220   \n4            0.958188           0.947735           0.961672   \n5                 NaN                NaN                NaN   \n6            0.979094           0.965157           0.958188   \n7            0.979094           0.965157           0.958188   \n8            0.975610           0.972125           0.930314   \n9            0.965157           0.958188           0.951220   \n10           0.958188           0.947735           0.961672   \n11                NaN                NaN                NaN   \n12           0.979094           0.965157           0.958188   \n13           0.979094           0.965157           0.958188   \n14           0.968641           0.975610           0.926829   \n15           0.965157           0.958188           0.951220   \n16           0.958188           0.947735           0.961672   \n17                NaN                NaN                NaN   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.965278            0.968750            0.961672   \n1             0.965278            0.968750            0.961672   \n2             0.975694            0.961806            0.958188   \n3             0.965278            0.951389            0.937282   \n4             0.965278            0.965278            0.944251   \n5                  NaN                 NaN                 NaN   \n6             0.965278            0.968750            0.951220   \n7             0.965278            0.968750            0.951220   \n8             0.961806            0.958333            0.944251   \n9             0.965278            0.951389            0.937282   \n10            0.965278            0.965278            0.944251   \n11                 NaN                 NaN                 NaN   \n12            0.965278            0.968750            0.947735   \n13            0.965278            0.968750            0.947735   \n14            0.958333            0.954861            0.944251   \n15            0.965278            0.951389            0.937282   \n16            0.965278            0.965278            0.944251   \n17                 NaN                 NaN                 NaN   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.958188            0.958188         0.965905        0.007523   \n1             0.958188            0.958188         0.965905        0.007523   \n2             0.947735            0.972125         0.962651        0.008771   \n3             0.961672            0.951220         0.955001        0.007974   \n4             0.951220            0.958188         0.955694        0.006658   \n5                  NaN                 NaN              NaN             NaN   \n6             0.961672            0.951220         0.961728        0.008223   \n7             0.961672            0.951220         0.961728        0.008223   \n8             0.944251            0.954704         0.954067        0.011475   \n9             0.961672            0.951220         0.955001        0.007974   \n10            0.951220            0.958188         0.955694        0.006658   \n11                 NaN                 NaN              NaN             NaN   \n12            0.961672            0.951220         0.961496        0.008559   \n13            0.961672            0.951220         0.961496        0.008559   \n14            0.944251            0.954704         0.952443        0.011226   \n15            0.961672            0.951220         0.955001        0.007974   \n16            0.951220            0.958188         0.955694        0.006658   \n17                 NaN                 NaN              NaN             NaN   \n\n    rank_test_score  split0_train_score  split1_train_score  \\\n0                 1            1.000000            1.000000   \n1                 1            1.000000            1.000000   \n2                 3            0.992167            0.993908   \n3                11            1.000000            1.000000   \n4                 8            1.000000            1.000000   \n5                16                 NaN                 NaN   \n6                 4            1.000000            1.000000   \n7                 4            1.000000            1.000000   \n8                14            0.997389            0.999130   \n9                11            1.000000            1.000000   \n10                8            1.000000            1.000000   \n11               17                 NaN                 NaN   \n12                6            1.000000            1.000000   \n13                6            1.000000            1.000000   \n14               15            0.997389            0.999130   \n15               11            1.000000            1.000000   \n16                8            1.000000            1.000000   \n17               18                 NaN                 NaN   \n\n    split2_train_score  split3_train_score  split4_train_score  \\\n0             1.000000            0.999130            1.000000   \n1             1.000000            0.999130            1.000000   \n2             0.995652            0.993043            0.993043   \n3             1.000000            1.000000            1.000000   \n4             1.000000            1.000000            1.000000   \n5                  NaN                 NaN                 NaN   \n6             1.000000            1.000000            1.000000   \n7             1.000000            1.000000            1.000000   \n8             0.998261            0.996522            0.997391   \n9             1.000000            1.000000            1.000000   \n10            1.000000            1.000000            1.000000   \n11                 NaN                 NaN                 NaN   \n12            1.000000            1.000000            1.000000   \n13            1.000000            1.000000            1.000000   \n14            0.998261            0.997391            0.998261   \n15            1.000000            1.000000            1.000000   \n16            1.000000            1.000000            1.000000   \n17                 NaN                 NaN                 NaN   \n\n    split5_train_score  split6_train_score  split7_train_score  \\\n0             0.999130            1.000000            1.000000   \n1             0.999130            1.000000            1.000000   \n2             0.991297            0.996519            0.992174   \n3             1.000000            1.000000            1.000000   \n4             1.000000            1.000000            1.000000   \n5                  NaN                 NaN                 NaN   \n6             1.000000            1.000000            1.000000   \n7             1.000000            1.000000            1.000000   \n8             0.995648            0.998259            0.998261   \n9             1.000000            1.000000            1.000000   \n10            1.000000            1.000000            1.000000   \n11                 NaN                 NaN                 NaN   \n12            1.000000            1.000000            1.000000   \n13            1.000000            1.000000            1.000000   \n14            0.995648            0.998259            0.998261   \n15            1.000000            1.000000            1.000000   \n16            1.000000            1.000000            1.000000   \n17                 NaN                 NaN                 NaN   \n\n    split8_train_score  split9_train_score  split10_train_score  \\\n0             1.000000            1.000000             1.000000   \n1             1.000000            1.000000             1.000000   \n2             0.993043            0.993913             0.990426   \n3             1.000000            1.000000             1.000000   \n4             1.000000            1.000000             1.000000   \n5                  NaN                 NaN                  NaN   \n6             1.000000            1.000000             1.000000   \n7             1.000000            1.000000             1.000000   \n8             1.000000            0.997391             0.996519   \n9             1.000000            1.000000             1.000000   \n10            1.000000            1.000000             1.000000   \n11                 NaN                 NaN                  NaN   \n12            1.000000            1.000000             1.000000   \n13            1.000000            1.000000             1.000000   \n14            1.000000            0.997391             0.996519   \n15            1.000000            1.000000             1.000000   \n16            1.000000            1.000000             1.000000   \n17                 NaN                 NaN                  NaN   \n\n    split11_train_score  split12_train_score  split13_train_score  \\\n0              1.000000             1.000000             1.000000   \n1              1.000000             1.000000             1.000000   \n2              0.992167             0.993913             0.993913   \n3              1.000000             1.000000             1.000000   \n4              1.000000             1.000000             1.000000   \n5                   NaN                  NaN                  NaN   \n6              1.000000             1.000000             1.000000   \n7              1.000000             1.000000             1.000000   \n8              0.999130             0.996522             0.997391   \n9              1.000000             1.000000             1.000000   \n10             1.000000             1.000000             1.000000   \n11                  NaN                  NaN                  NaN   \n12             1.000000             1.000000             1.000000   \n13             1.000000             1.000000             1.000000   \n14             0.999130             0.996522             0.997391   \n15             1.000000             1.000000             1.000000   \n16             1.000000             1.000000             1.000000   \n17                  NaN                  NaN                  NaN   \n\n    split14_train_score  mean_train_score  std_train_score  \n0              1.000000          0.999884         0.000296  \n1              1.000000          0.999884         0.000296  \n2              0.995652          0.993389         0.001615  \n3              1.000000          1.000000         0.000000  \n4              1.000000          1.000000         0.000000  \n5                   NaN               NaN              NaN  \n6              1.000000          1.000000         0.000000  \n7              1.000000          1.000000         0.000000  \n8              0.997391          0.997680         0.001131  \n9              1.000000          1.000000         0.000000  \n10             1.000000          1.000000         0.000000  \n11                  NaN               NaN              NaN  \n12             1.000000          1.000000         0.000000  \n13             1.000000          1.000000         0.000000  \n14             0.998261          0.997854         0.001094  \n15             1.000000          1.000000         0.000000  \n16             1.000000          1.000000         0.000000  \n17                  NaN               NaN              NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_C</th>\n      <th>param_penalty</th>\n      <th>param_solver</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n      <th>split0_train_score</th>\n      <th>split1_train_score</th>\n      <th>split2_train_score</th>\n      <th>split3_train_score</th>\n      <th>split4_train_score</th>\n      <th>split5_train_score</th>\n      <th>split6_train_score</th>\n      <th>split7_train_score</th>\n      <th>split8_train_score</th>\n      <th>split9_train_score</th>\n      <th>split10_train_score</th>\n      <th>split11_train_score</th>\n      <th>split12_train_score</th>\n      <th>split13_train_score</th>\n      <th>split14_train_score</th>\n      <th>mean_train_score</th>\n      <th>std_train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.681888</td>\n      <td>0.071425</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'newton-...</td>\n      <td>0.954861</td>\n      <td>0.958333</td>\n      <td>0.972125</td>\n      <td>0.968641</td>\n      <td>0.979094</td>\n      <td>0.961806</td>\n      <td>0.968750</td>\n      <td>0.982578</td>\n      <td>0.965157</td>\n      <td>0.965157</td>\n      <td>0.965278</td>\n      <td>0.968750</td>\n      <td>0.961672</td>\n      <td>0.958188</td>\n      <td>0.958188</td>\n      <td>0.965905</td>\n      <td>0.007523</td>\n      <td>1</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.999130</td>\n      <td>1.000000</td>\n      <td>0.999130</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.999884</td>\n      <td>0.000296</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.798371</td>\n      <td>0.689899</td>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.954861</td>\n      <td>0.958333</td>\n      <td>0.972125</td>\n      <td>0.968641</td>\n      <td>0.979094</td>\n      <td>0.961806</td>\n      <td>0.968750</td>\n      <td>0.982578</td>\n      <td>0.965157</td>\n      <td>0.965157</td>\n      <td>0.965278</td>\n      <td>0.968750</td>\n      <td>0.961672</td>\n      <td>0.958188</td>\n      <td>0.958188</td>\n      <td>0.965905</td>\n      <td>0.007523</td>\n      <td>1</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.999130</td>\n      <td>1.000000</td>\n      <td>0.999130</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.999884</td>\n      <td>0.000296</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.081474</td>\n      <td>0.007299</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'libline...</td>\n      <td>0.958333</td>\n      <td>0.958333</td>\n      <td>0.961672</td>\n      <td>0.961672</td>\n      <td>0.958188</td>\n      <td>0.965278</td>\n      <td>0.968750</td>\n      <td>0.982578</td>\n      <td>0.958188</td>\n      <td>0.951220</td>\n      <td>0.975694</td>\n      <td>0.961806</td>\n      <td>0.958188</td>\n      <td>0.947735</td>\n      <td>0.972125</td>\n      <td>0.962651</td>\n      <td>0.008771</td>\n      <td>3</td>\n      <td>0.992167</td>\n      <td>0.993908</td>\n      <td>0.995652</td>\n      <td>0.993043</td>\n      <td>0.993043</td>\n      <td>0.991297</td>\n      <td>0.996519</td>\n      <td>0.992174</td>\n      <td>0.993043</td>\n      <td>0.993913</td>\n      <td>0.990426</td>\n      <td>0.992167</td>\n      <td>0.993913</td>\n      <td>0.993913</td>\n      <td>0.995652</td>\n      <td>0.993389</td>\n      <td>0.001615</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.162815</td>\n      <td>0.010653</td>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'newto...</td>\n      <td>0.944444</td>\n      <td>0.954861</td>\n      <td>0.965157</td>\n      <td>0.951220</td>\n      <td>0.961672</td>\n      <td>0.947917</td>\n      <td>0.958333</td>\n      <td>0.965157</td>\n      <td>0.958188</td>\n      <td>0.951220</td>\n      <td>0.965278</td>\n      <td>0.951389</td>\n      <td>0.937282</td>\n      <td>0.961672</td>\n      <td>0.951220</td>\n      <td>0.955001</td>\n      <td>0.007974</td>\n      <td>11</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.060589</td>\n      <td>0.013825</td>\n      <td>0.000801</td>\n      <td>0.001722</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.944444</td>\n      <td>0.951389</td>\n      <td>0.961672</td>\n      <td>0.958188</td>\n      <td>0.958188</td>\n      <td>0.958333</td>\n      <td>0.951389</td>\n      <td>0.958188</td>\n      <td>0.947735</td>\n      <td>0.961672</td>\n      <td>0.965278</td>\n      <td>0.965278</td>\n      <td>0.944251</td>\n      <td>0.951220</td>\n      <td>0.958188</td>\n      <td>0.955694</td>\n      <td>0.006658</td>\n      <td>8</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'libli...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>16</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.891478</td>\n      <td>0.133195</td>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n      <td>0.944444</td>\n      <td>0.958333</td>\n      <td>0.965157</td>\n      <td>0.965157</td>\n      <td>0.968641</td>\n      <td>0.965278</td>\n      <td>0.958333</td>\n      <td>0.979094</td>\n      <td>0.965157</td>\n      <td>0.958188</td>\n      <td>0.965278</td>\n      <td>0.968750</td>\n      <td>0.951220</td>\n      <td>0.961672</td>\n      <td>0.951220</td>\n      <td>0.961728</td>\n      <td>0.008223</td>\n      <td>4</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2.350740</td>\n      <td>0.449218</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.944444</td>\n      <td>0.958333</td>\n      <td>0.965157</td>\n      <td>0.965157</td>\n      <td>0.968641</td>\n      <td>0.965278</td>\n      <td>0.958333</td>\n      <td>0.979094</td>\n      <td>0.965157</td>\n      <td>0.958188</td>\n      <td>0.965278</td>\n      <td>0.968750</td>\n      <td>0.951220</td>\n      <td>0.961672</td>\n      <td>0.951220</td>\n      <td>0.961728</td>\n      <td>0.008223</td>\n      <td>4</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.146600</td>\n      <td>0.006981</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'liblinear'}</td>\n      <td>0.947917</td>\n      <td>0.947917</td>\n      <td>0.951220</td>\n      <td>0.947735</td>\n      <td>0.947735</td>\n      <td>0.961806</td>\n      <td>0.965278</td>\n      <td>0.975610</td>\n      <td>0.972125</td>\n      <td>0.930314</td>\n      <td>0.961806</td>\n      <td>0.958333</td>\n      <td>0.944251</td>\n      <td>0.944251</td>\n      <td>0.954704</td>\n      <td>0.954067</td>\n      <td>0.011475</td>\n      <td>14</td>\n      <td>0.997389</td>\n      <td>0.999130</td>\n      <td>0.998261</td>\n      <td>0.996522</td>\n      <td>0.997391</td>\n      <td>0.995648</td>\n      <td>0.998259</td>\n      <td>0.998261</td>\n      <td>1.000000</td>\n      <td>0.997391</td>\n      <td>0.996519</td>\n      <td>0.999130</td>\n      <td>0.996522</td>\n      <td>0.997391</td>\n      <td>0.997391</td>\n      <td>0.997680</td>\n      <td>0.001131</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.162815</td>\n      <td>0.008316</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>5</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'newton-...</td>\n      <td>0.944444</td>\n      <td>0.954861</td>\n      <td>0.965157</td>\n      <td>0.951220</td>\n      <td>0.961672</td>\n      <td>0.947917</td>\n      <td>0.958333</td>\n      <td>0.965157</td>\n      <td>0.958188</td>\n      <td>0.951220</td>\n      <td>0.965278</td>\n      <td>0.951389</td>\n      <td>0.937282</td>\n      <td>0.961672</td>\n      <td>0.951220</td>\n      <td>0.955001</td>\n      <td>0.007974</td>\n      <td>11</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.051247</td>\n      <td>0.004265</td>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>5</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.944444</td>\n      <td>0.951389</td>\n      <td>0.961672</td>\n      <td>0.958188</td>\n      <td>0.958188</td>\n      <td>0.958333</td>\n      <td>0.951389</td>\n      <td>0.958188</td>\n      <td>0.947735</td>\n      <td>0.961672</td>\n      <td>0.965278</td>\n      <td>0.965278</td>\n      <td>0.944251</td>\n      <td>0.951220</td>\n      <td>0.958188</td>\n      <td>0.955694</td>\n      <td>0.006658</td>\n      <td>8</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'libline...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>17</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.887741</td>\n      <td>0.115713</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n      <td>0.944444</td>\n      <td>0.958333</td>\n      <td>0.965157</td>\n      <td>0.965157</td>\n      <td>0.968641</td>\n      <td>0.965278</td>\n      <td>0.958333</td>\n      <td>0.979094</td>\n      <td>0.965157</td>\n      <td>0.958188</td>\n      <td>0.965278</td>\n      <td>0.968750</td>\n      <td>0.947735</td>\n      <td>0.961672</td>\n      <td>0.951220</td>\n      <td>0.961496</td>\n      <td>0.008559</td>\n      <td>6</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2.288684</td>\n      <td>0.853630</td>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.944444</td>\n      <td>0.958333</td>\n      <td>0.965157</td>\n      <td>0.965157</td>\n      <td>0.968641</td>\n      <td>0.965278</td>\n      <td>0.958333</td>\n      <td>0.979094</td>\n      <td>0.965157</td>\n      <td>0.958188</td>\n      <td>0.965278</td>\n      <td>0.968750</td>\n      <td>0.947735</td>\n      <td>0.961672</td>\n      <td>0.951220</td>\n      <td>0.961496</td>\n      <td>0.008559</td>\n      <td>6</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.173892</td>\n      <td>0.011727</td>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}</td>\n      <td>0.951389</td>\n      <td>0.947917</td>\n      <td>0.947735</td>\n      <td>0.947735</td>\n      <td>0.944251</td>\n      <td>0.961806</td>\n      <td>0.958333</td>\n      <td>0.968641</td>\n      <td>0.975610</td>\n      <td>0.926829</td>\n      <td>0.958333</td>\n      <td>0.954861</td>\n      <td>0.944251</td>\n      <td>0.944251</td>\n      <td>0.954704</td>\n      <td>0.952443</td>\n      <td>0.011226</td>\n      <td>15</td>\n      <td>0.997389</td>\n      <td>0.999130</td>\n      <td>0.998261</td>\n      <td>0.997391</td>\n      <td>0.998261</td>\n      <td>0.995648</td>\n      <td>0.998259</td>\n      <td>0.998261</td>\n      <td>1.000000</td>\n      <td>0.997391</td>\n      <td>0.996519</td>\n      <td>0.999130</td>\n      <td>0.996522</td>\n      <td>0.997391</td>\n      <td>0.998261</td>\n      <td>0.997854</td>\n      <td>0.001094</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.163482</td>\n      <td>0.007802</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>10</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'newton...</td>\n      <td>0.944444</td>\n      <td>0.954861</td>\n      <td>0.965157</td>\n      <td>0.951220</td>\n      <td>0.961672</td>\n      <td>0.947917</td>\n      <td>0.958333</td>\n      <td>0.965157</td>\n      <td>0.958188</td>\n      <td>0.951220</td>\n      <td>0.965278</td>\n      <td>0.951389</td>\n      <td>0.937282</td>\n      <td>0.961672</td>\n      <td>0.951220</td>\n      <td>0.955001</td>\n      <td>0.007974</td>\n      <td>11</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.053182</td>\n      <td>0.003484</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>10</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.944444</td>\n      <td>0.951389</td>\n      <td>0.961672</td>\n      <td>0.958188</td>\n      <td>0.958188</td>\n      <td>0.958333</td>\n      <td>0.951389</td>\n      <td>0.958188</td>\n      <td>0.947735</td>\n      <td>0.961672</td>\n      <td>0.965278</td>\n      <td>0.965278</td>\n      <td>0.944251</td>\n      <td>0.951220</td>\n      <td>0.958188</td>\n      <td>0.955694</td>\n      <td>0.006658</td>\n      <td>8</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>10</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'liblin...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
    "\n",
    "x = digits.images.reshape((len(digits.images), -1))\n",
    "y = digits.target\n",
    "\n",
    "# séparation du dataset en données \"d'apprentissage\" et de test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n",
    "\n",
    "# Création du modèle de régression\n",
    "lRModel = LogisticRegression(verbose=False, max_iter=10000)\n",
    "\n",
    "# dictionnaire contenant les différents paramètres à essayer\n",
    "paramDict = dict()\n",
    "paramDict['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "paramDict['penalty'] = ['l2', 'none']\n",
    "paramDict['C'] = [0.1,5 , 10]\n",
    "\n",
    "# Création de nos itérations\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "# Création de notre modèle de validation croisée\n",
    "model_cv = GridSearchCV(estimator=lRModel,\n",
    "                        param_grid=paramDict,\n",
    "                        scoring='accuracy',\n",
    "                        cv=kFold,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True)\n",
    "\n",
    "# Entrainement du modèle\n",
    "model_cv.fit(x_train, y_train)\n",
    "# cv results\n",
    "pd.set_option('display.max_columns', None)\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "La validation croisée nous permet de determiner les paramètres optimaux pour notre modèle d'apprentissage.\n",
    "Les résultats de nos tests avec de nombreux paramètres différents (dont la plupart ne sont pas présent dans l'exemple ci-dessus).\n",
    "#TODO mettre dans une liste\n",
    "Le paramètre C correspond à l'inverse de la force de régularisation des données. Plus ce paramètre est grand, plus le risque d'overfitting est grand. Une valeur de 0.1 semble être optimale.\n",
    "Le paramètre de pénalité permet de réduire les coefficients θ. On remarque une perte de précision si l'on n'applique pas de pénalité. La meilleure pénalité semble être la norme L2.\n",
    "Pour finir, le solveur correspond à l'algorithme d'optimisation utilisé pour l'entrainement. Dans notre cas, lbfgs permet d'obtenir la precision la plus élevée malgré un temps d'entrainement significativement plus long que ses concurrents."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modèle de Bayes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n0       0.003003      0.001096         0.000467        0.000499        0.01   \n1       0.002602      0.000712         0.000534        0.000499         0.1   \n2       0.002402      0.000490         0.000601        0.000490         0.5   \n3       0.002202      0.000400         0.000801        0.000400         1.0   \n4       0.002202      0.000654         0.000400        0.000490        10.0   \n\n            params  split0_test_score  split1_test_score  split2_test_score  \\\n0  {'alpha': 0.01}           0.888889           0.836806           0.867596   \n1   {'alpha': 0.1}           0.881944           0.833333           0.864111   \n2   {'alpha': 0.5}           0.878472           0.826389           0.857143   \n3   {'alpha': 1.0}           0.878472           0.822917           0.857143   \n4  {'alpha': 10.0}           0.875000           0.805556           0.860627   \n\n   split3_test_score  split4_test_score  split5_test_score  split6_test_score  \\\n0           0.867596           0.857143           0.878472           0.833333   \n1           0.864111           0.867596           0.871528           0.833333   \n2           0.860627           0.864111           0.868056           0.822917   \n3           0.857143           0.860627           0.868056           0.822917   \n4           0.850174           0.846690           0.850694           0.798611   \n\n   split7_test_score  split8_test_score  split9_test_score  \\\n0           0.871080           0.871080           0.867596   \n1           0.871080           0.874564           0.860627   \n2           0.867596           0.871080           0.857143   \n3           0.867596           0.871080           0.857143   \n4           0.864111           0.860627           0.860627   \n\n   split10_test_score  split11_test_score  split12_test_score  \\\n0            0.885417            0.854167            0.853659   \n1            0.881944            0.843750            0.857143   \n2            0.878472            0.836806            0.853659   \n3            0.878472            0.836806            0.850174   \n4            0.871528            0.815972            0.839721   \n\n   split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0            0.874564            0.871080         0.865232        0.015267   \n1            0.871080            0.867596         0.862916        0.014765   \n2            0.860627            0.867596         0.858046        0.016422   \n3            0.860627            0.867596         0.857118        0.016874   \n4            0.846690            0.860627         0.847150        0.022390   \n\n   rank_test_score  \n0                1  \n1                2  \n2                3  \n3                4  \n4                5  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_alpha</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.003003</td>\n      <td>0.001096</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>0.01</td>\n      <td>{'alpha': 0.01}</td>\n      <td>0.888889</td>\n      <td>0.836806</td>\n      <td>0.867596</td>\n      <td>0.867596</td>\n      <td>0.857143</td>\n      <td>0.878472</td>\n      <td>0.833333</td>\n      <td>0.871080</td>\n      <td>0.871080</td>\n      <td>0.867596</td>\n      <td>0.885417</td>\n      <td>0.854167</td>\n      <td>0.853659</td>\n      <td>0.874564</td>\n      <td>0.871080</td>\n      <td>0.865232</td>\n      <td>0.015267</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.002602</td>\n      <td>0.000712</td>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>0.1</td>\n      <td>{'alpha': 0.1}</td>\n      <td>0.881944</td>\n      <td>0.833333</td>\n      <td>0.864111</td>\n      <td>0.864111</td>\n      <td>0.867596</td>\n      <td>0.871528</td>\n      <td>0.833333</td>\n      <td>0.871080</td>\n      <td>0.874564</td>\n      <td>0.860627</td>\n      <td>0.881944</td>\n      <td>0.843750</td>\n      <td>0.857143</td>\n      <td>0.871080</td>\n      <td>0.867596</td>\n      <td>0.862916</td>\n      <td>0.014765</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.002402</td>\n      <td>0.000490</td>\n      <td>0.000601</td>\n      <td>0.000490</td>\n      <td>0.5</td>\n      <td>{'alpha': 0.5}</td>\n      <td>0.878472</td>\n      <td>0.826389</td>\n      <td>0.857143</td>\n      <td>0.860627</td>\n      <td>0.864111</td>\n      <td>0.868056</td>\n      <td>0.822917</td>\n      <td>0.867596</td>\n      <td>0.871080</td>\n      <td>0.857143</td>\n      <td>0.878472</td>\n      <td>0.836806</td>\n      <td>0.853659</td>\n      <td>0.860627</td>\n      <td>0.867596</td>\n      <td>0.858046</td>\n      <td>0.016422</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.002202</td>\n      <td>0.000400</td>\n      <td>0.000801</td>\n      <td>0.000400</td>\n      <td>1.0</td>\n      <td>{'alpha': 1.0}</td>\n      <td>0.878472</td>\n      <td>0.822917</td>\n      <td>0.857143</td>\n      <td>0.857143</td>\n      <td>0.860627</td>\n      <td>0.868056</td>\n      <td>0.822917</td>\n      <td>0.867596</td>\n      <td>0.871080</td>\n      <td>0.857143</td>\n      <td>0.878472</td>\n      <td>0.836806</td>\n      <td>0.850174</td>\n      <td>0.860627</td>\n      <td>0.867596</td>\n      <td>0.857118</td>\n      <td>0.016874</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.002202</td>\n      <td>0.000654</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>10.0</td>\n      <td>{'alpha': 10.0}</td>\n      <td>0.875000</td>\n      <td>0.805556</td>\n      <td>0.860627</td>\n      <td>0.850174</td>\n      <td>0.846690</td>\n      <td>0.850694</td>\n      <td>0.798611</td>\n      <td>0.864111</td>\n      <td>0.860627</td>\n      <td>0.860627</td>\n      <td>0.871528</td>\n      <td>0.815972</td>\n      <td>0.839721</td>\n      <td>0.846690</td>\n      <td>0.860627</td>\n      <td>0.847150</td>\n      <td>0.022390</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "NB_model = BernoulliNB()\n",
    "\n",
    "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0]}\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(NB_model, params, n_iter=5, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(RCV.cv_results_)\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 32 is smaller than n_iter=50. Running 32 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        0.005472      0.000619         0.000400        0.000490   \n1        0.002202      0.000542         0.000334        0.000472   \n2        0.005205      0.000401         0.000200        0.000401   \n3        0.002402      0.000611         0.000200        0.000400   \n4        0.004804      0.000400         0.000334        0.000472   \n5        0.002069      0.000443         0.000334        0.000472   \n6        0.004537      0.000499         0.000467        0.000499   \n7        0.002202      0.000400         0.000334        0.000472   \n8        0.011277      0.000681         0.000467        0.000499   \n9        0.005071      0.000772         0.000400        0.000490   \n10       0.012545      0.003522         0.000200        0.000400   \n11       0.004537      0.000619         0.000400        0.000490   \n12       0.012144      0.001964         0.000801        0.001975   \n13       0.004137      0.000340         0.000400        0.000490   \n14       0.010543      0.000619         0.000534        0.000499   \n15       0.004671      0.001740         0.000267        0.000443   \n16       0.006339      0.000597         0.000467        0.000499   \n17       0.002469      0.000806         0.000400        0.000490   \n18       0.006406      0.000713         0.000801        0.002008   \n19       0.002335      0.000472         0.000267        0.000443   \n20       0.007207      0.002106         0.000534        0.000499   \n21       0.002536      0.000885         0.000200        0.000400   \n22       0.007874      0.003968         0.001468        0.004181   \n23       0.002469      0.000499         0.000133        0.000340   \n24       0.017349      0.002121         0.000467        0.000499   \n25       0.005938      0.000443         0.000400        0.000490   \n26       0.015614      0.000953         0.000467        0.000499   \n27       0.005538      0.000499         0.000534        0.000499   \n28       0.015080      0.001341         0.000400        0.000490   \n29       0.005272      0.000574         0.000467        0.000499   \n30       0.015347      0.001815         0.000601        0.000490   \n31       0.005005      0.001156         0.000601        0.000490   \n\n   param_splitter param_min_samples_leaf param_max_depth param_criterion  \\\n0            best                      1               3            gini   \n1          random                      1               3            gini   \n2            best                      2               3            gini   \n3          random                      2               3            gini   \n4            best                      3               3            gini   \n5          random                      3               3            gini   \n6            best                      4               3            gini   \n7          random                      4               3            gini   \n8            best                      1            None            gini   \n9          random                      1            None            gini   \n10           best                      2            None            gini   \n11         random                      2            None            gini   \n12           best                      3            None            gini   \n13         random                      3            None            gini   \n14           best                      4            None            gini   \n15         random                      4            None            gini   \n16           best                      1               3         entropy   \n17         random                      1               3         entropy   \n18           best                      2               3         entropy   \n19         random                      2               3         entropy   \n20           best                      3               3         entropy   \n21         random                      3               3         entropy   \n22           best                      4               3         entropy   \n23         random                      4               3         entropy   \n24           best                      1            None         entropy   \n25         random                      1            None         entropy   \n26           best                      2            None         entropy   \n27         random                      2            None         entropy   \n28           best                      3            None         entropy   \n29         random                      3            None         entropy   \n30           best                      4            None         entropy   \n31         random                      4            None         entropy   \n\n                                               params  split0_test_score  \\\n0   {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.489583   \n1   {'splitter': 'random', 'min_samples_leaf': 1, ...           0.555556   \n2   {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.489583   \n3   {'splitter': 'random', 'min_samples_leaf': 2, ...           0.506944   \n4   {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.489583   \n5   {'splitter': 'random', 'min_samples_leaf': 3, ...           0.520833   \n6   {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.489583   \n7   {'splitter': 'random', 'min_samples_leaf': 4, ...           0.517361   \n8   {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.854167   \n9   {'splitter': 'random', 'min_samples_leaf': 1, ...           0.840278   \n10  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.850694   \n11  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.861111   \n12  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.826389   \n13  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.819444   \n14  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.836806   \n15  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.819444   \n16  {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.534722   \n17  {'splitter': 'random', 'min_samples_leaf': 1, ...           0.513889   \n18  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.534722   \n19  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.506944   \n20  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.534722   \n21  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.486111   \n22  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.534722   \n23  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.472222   \n24  {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.854167   \n25  {'splitter': 'random', 'min_samples_leaf': 1, ...           0.868056   \n26  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.854167   \n27  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.864583   \n28  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.847222   \n29  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.864583   \n30  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.857639   \n31  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.791667   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.468750           0.456446           0.473868   \n1            0.489583           0.480836           0.505226   \n2            0.468750           0.456446           0.473868   \n3            0.486111           0.567944           0.480836   \n4            0.468750           0.456446           0.473868   \n5            0.475694           0.487805           0.439024   \n6            0.468750           0.456446           0.473868   \n7            0.430556           0.533101           0.543554   \n8            0.847222           0.850174           0.839721   \n9            0.847222           0.843206           0.860627   \n10           0.833333           0.836237           0.836237   \n11           0.829861           0.853659           0.825784   \n12           0.826389           0.829268           0.815331   \n13           0.822917           0.860627           0.832753   \n14           0.822917           0.818815           0.843206   \n15           0.795139           0.825784           0.811847   \n16           0.513889           0.581882           0.571429   \n17           0.440972           0.498258           0.480836   \n18           0.513889           0.581882           0.571429   \n19           0.586806           0.484321           0.564460   \n20           0.513889           0.581882           0.571429   \n21           0.569444           0.512195           0.567944   \n22           0.513889           0.581882           0.571429   \n23           0.468750           0.515679           0.494774   \n24           0.850694           0.878049           0.846690   \n25           0.833333           0.808362           0.850174   \n26           0.829861           0.881533           0.857143   \n27           0.843750           0.860627           0.853659   \n28           0.819444           0.878049           0.832753   \n29           0.815972           0.839721           0.839721   \n30           0.815972           0.853659           0.829268   \n31           0.829861           0.874564           0.815331   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.463415           0.482639           0.444444   \n1            0.484321           0.493056           0.531250   \n2            0.463415           0.482639           0.444444   \n3            0.515679           0.371528           0.454861   \n4            0.463415           0.482639           0.444444   \n5            0.491289           0.569444           0.513889   \n6            0.463415           0.482639           0.444444   \n7            0.473868           0.534722           0.496528   \n8            0.811847           0.836806           0.812500   \n9            0.836237           0.847222           0.861111   \n10           0.818815           0.836806           0.822917   \n11           0.811847           0.847222           0.812500   \n12           0.818815           0.829861           0.798611   \n13           0.822300           0.850694           0.802083   \n14           0.815331           0.819444           0.784722   \n15           0.787456           0.819444           0.809028   \n16           0.560976           0.569444           0.538194   \n17           0.557491           0.465278           0.447917   \n18           0.560976           0.569444           0.538194   \n19           0.512195           0.576389           0.458333   \n20           0.560976           0.569444           0.538194   \n21           0.581882           0.524306           0.517361   \n22           0.560976           0.569444           0.538194   \n23           0.498258           0.538194           0.510417   \n24           0.829268           0.878472           0.847222   \n25           0.787456           0.840278           0.857639   \n26           0.801394           0.875000           0.836806   \n27           0.801394           0.885417           0.864583   \n28           0.811847           0.875000           0.847222   \n29           0.822300           0.847222           0.829861   \n30           0.818815           0.868056           0.826389   \n31           0.794425           0.826389           0.871528   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.463415           0.477352           0.477352   \n1            0.425087           0.421603           0.543554   \n2            0.463415           0.477352           0.477352   \n3            0.358885           0.456446           0.543554   \n4            0.463415           0.477352           0.477352   \n5            0.498258           0.529617           0.414634   \n6            0.463415           0.477352           0.477352   \n7            0.554007           0.519164           0.557491   \n8            0.860627           0.815331           0.850174   \n9            0.846690           0.853659           0.871080   \n10           0.846690           0.825784           0.822300   \n11           0.839721           0.829268           0.794425   \n12           0.839721           0.829268           0.797909   \n13           0.832753           0.801394           0.832753   \n14           0.815331           0.815331           0.811847   \n15           0.808362           0.801394           0.794425   \n16           0.540070           0.494774           0.581882   \n17           0.519164           0.529617           0.536585   \n18           0.540070           0.494774           0.581882   \n19           0.505226           0.439024           0.459930   \n20           0.540070           0.494774           0.581882   \n21           0.487805           0.529617           0.498258   \n22           0.540070           0.494774           0.581882   \n23           0.519164           0.501742           0.567944   \n24           0.888502           0.850174           0.836237   \n25           0.874564           0.864111           0.857143   \n26           0.874564           0.843206           0.825784   \n27           0.850174           0.839721           0.846690   \n28           0.891986           0.829268           0.825784   \n29           0.804878           0.794425           0.822300   \n30           0.881533           0.825784           0.822300   \n31           0.777003           0.794425           0.829268   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.461806            0.486111            0.452962   \n1             0.531250            0.468750            0.414634   \n2             0.461806            0.486111            0.452962   \n3             0.458333            0.496528            0.554007   \n4             0.461806            0.486111            0.452962   \n5             0.434028            0.434028            0.435540   \n6             0.461806            0.486111            0.452962   \n7             0.534722            0.524306            0.456446   \n8             0.833333            0.812500            0.867596   \n9             0.864583            0.840278            0.832753   \n10            0.802083            0.833333            0.843206   \n11            0.857639            0.788194            0.825784   \n12            0.795139            0.809028            0.829268   \n13            0.798611            0.833333            0.815331   \n14            0.784722            0.826389            0.822300   \n15            0.802083            0.795139            0.818815   \n16            0.559028            0.559028            0.501742   \n17            0.496528            0.524306            0.466899   \n18            0.559028            0.559028            0.501742   \n19            0.524306            0.472222            0.533101   \n20            0.559028            0.559028            0.501742   \n21            0.486111            0.576389            0.459930   \n22            0.559028            0.559028            0.501742   \n23            0.572917            0.486111            0.491289   \n24            0.829861            0.840278            0.864111   \n25            0.836806            0.805556            0.867596   \n26            0.829861            0.840278            0.857143   \n27            0.875000            0.822917            0.825784   \n28            0.819444            0.833333            0.839721   \n29            0.868056            0.829861            0.815331   \n30            0.826389            0.843750            0.832753   \n31            0.819444            0.875000            0.864111   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.494774            0.470383         0.470887        0.013688   \n1             0.533101            0.550523         0.495222        0.045506   \n2             0.494774            0.470383         0.470887        0.013688   \n3             0.435540            0.557491         0.482979        0.060828   \n4             0.494774            0.470383         0.470887        0.013688   \n5             0.456446            0.505226         0.480384        0.042554   \n6             0.494774            0.470383         0.470887        0.013688   \n7             0.533101            0.494774         0.513580        0.035267   \n8             0.864111            0.846690         0.840187        0.018692   \n9             0.850174            0.860627         0.850383        0.010851   \n10            0.857143            0.846690         0.834151        0.013702   \n11            0.871080            0.857143         0.833683        0.023929   \n12            0.860627            0.853659         0.823952        0.018406   \n13            0.825784            0.825784         0.825104        0.016523   \n14            0.836237            0.839721         0.819541        0.016628   \n15            0.822300            0.846690         0.810490        0.014912   \n16            0.557491            0.484321         0.543258        0.030546   \n17            0.567944            0.540070         0.505717        0.037669   \n18            0.557491            0.484321         0.543258        0.030546   \n19            0.487805            0.536585         0.509843        0.042854   \n20            0.557491            0.484321         0.543258        0.030546   \n21            0.487805            0.526132         0.520753        0.036889   \n22            0.557491            0.484321         0.543258        0.030546   \n23            0.484321            0.560976         0.512184        0.032559   \n24            0.815331            0.867596         0.851777        0.019831   \n25            0.832753            0.829268         0.840873        0.024755   \n26            0.801394            0.864111         0.844816        0.023911   \n27            0.836237            0.822300         0.846189        0.021615   \n28            0.801394            0.843206         0.839712        0.024537   \n29            0.825784            0.825784         0.829720        0.019307   \n30            0.822300            0.860627         0.839016        0.019711   \n31            0.797909            0.839721         0.826710        0.031634   \n\n    rank_test_score  \n0                29  \n1                26  \n2                29  \n3                27  \n4                29  \n5                28  \n6                29  \n7                22  \n8                 6  \n9                 2  \n10                9  \n11               10  \n12               14  \n13               13  \n14               15  \n15               16  \n16               17  \n17               25  \n18               17  \n19               24  \n20               17  \n21               21  \n22               17  \n23               23  \n24                1  \n25                5  \n26                4  \n27                3  \n28                7  \n29               11  \n30                8  \n31               12  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_splitter</th>\n      <th>param_min_samples_leaf</th>\n      <th>param_max_depth</th>\n      <th>param_criterion</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.005472</td>\n      <td>0.000619</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>best</td>\n      <td>1</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.489583</td>\n      <td>0.468750</td>\n      <td>0.456446</td>\n      <td>0.473868</td>\n      <td>0.463415</td>\n      <td>0.482639</td>\n      <td>0.444444</td>\n      <td>0.463415</td>\n      <td>0.477352</td>\n      <td>0.477352</td>\n      <td>0.461806</td>\n      <td>0.486111</td>\n      <td>0.452962</td>\n      <td>0.494774</td>\n      <td>0.470383</td>\n      <td>0.470887</td>\n      <td>0.013688</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.002202</td>\n      <td>0.000542</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>random</td>\n      <td>1</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.555556</td>\n      <td>0.489583</td>\n      <td>0.480836</td>\n      <td>0.505226</td>\n      <td>0.484321</td>\n      <td>0.493056</td>\n      <td>0.531250</td>\n      <td>0.425087</td>\n      <td>0.421603</td>\n      <td>0.543554</td>\n      <td>0.531250</td>\n      <td>0.468750</td>\n      <td>0.414634</td>\n      <td>0.533101</td>\n      <td>0.550523</td>\n      <td>0.495222</td>\n      <td>0.045506</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.005205</td>\n      <td>0.000401</td>\n      <td>0.000200</td>\n      <td>0.000401</td>\n      <td>best</td>\n      <td>2</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.489583</td>\n      <td>0.468750</td>\n      <td>0.456446</td>\n      <td>0.473868</td>\n      <td>0.463415</td>\n      <td>0.482639</td>\n      <td>0.444444</td>\n      <td>0.463415</td>\n      <td>0.477352</td>\n      <td>0.477352</td>\n      <td>0.461806</td>\n      <td>0.486111</td>\n      <td>0.452962</td>\n      <td>0.494774</td>\n      <td>0.470383</td>\n      <td>0.470887</td>\n      <td>0.013688</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.002402</td>\n      <td>0.000611</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>random</td>\n      <td>2</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.506944</td>\n      <td>0.486111</td>\n      <td>0.567944</td>\n      <td>0.480836</td>\n      <td>0.515679</td>\n      <td>0.371528</td>\n      <td>0.454861</td>\n      <td>0.358885</td>\n      <td>0.456446</td>\n      <td>0.543554</td>\n      <td>0.458333</td>\n      <td>0.496528</td>\n      <td>0.554007</td>\n      <td>0.435540</td>\n      <td>0.557491</td>\n      <td>0.482979</td>\n      <td>0.060828</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.004804</td>\n      <td>0.000400</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>best</td>\n      <td>3</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.489583</td>\n      <td>0.468750</td>\n      <td>0.456446</td>\n      <td>0.473868</td>\n      <td>0.463415</td>\n      <td>0.482639</td>\n      <td>0.444444</td>\n      <td>0.463415</td>\n      <td>0.477352</td>\n      <td>0.477352</td>\n      <td>0.461806</td>\n      <td>0.486111</td>\n      <td>0.452962</td>\n      <td>0.494774</td>\n      <td>0.470383</td>\n      <td>0.470887</td>\n      <td>0.013688</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.002069</td>\n      <td>0.000443</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>random</td>\n      <td>3</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.520833</td>\n      <td>0.475694</td>\n      <td>0.487805</td>\n      <td>0.439024</td>\n      <td>0.491289</td>\n      <td>0.569444</td>\n      <td>0.513889</td>\n      <td>0.498258</td>\n      <td>0.529617</td>\n      <td>0.414634</td>\n      <td>0.434028</td>\n      <td>0.434028</td>\n      <td>0.435540</td>\n      <td>0.456446</td>\n      <td>0.505226</td>\n      <td>0.480384</td>\n      <td>0.042554</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.004537</td>\n      <td>0.000499</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>4</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.489583</td>\n      <td>0.468750</td>\n      <td>0.456446</td>\n      <td>0.473868</td>\n      <td>0.463415</td>\n      <td>0.482639</td>\n      <td>0.444444</td>\n      <td>0.463415</td>\n      <td>0.477352</td>\n      <td>0.477352</td>\n      <td>0.461806</td>\n      <td>0.486111</td>\n      <td>0.452962</td>\n      <td>0.494774</td>\n      <td>0.470383</td>\n      <td>0.470887</td>\n      <td>0.013688</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.002202</td>\n      <td>0.000400</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>random</td>\n      <td>4</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.517361</td>\n      <td>0.430556</td>\n      <td>0.533101</td>\n      <td>0.543554</td>\n      <td>0.473868</td>\n      <td>0.534722</td>\n      <td>0.496528</td>\n      <td>0.554007</td>\n      <td>0.519164</td>\n      <td>0.557491</td>\n      <td>0.534722</td>\n      <td>0.524306</td>\n      <td>0.456446</td>\n      <td>0.533101</td>\n      <td>0.494774</td>\n      <td>0.513580</td>\n      <td>0.035267</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.011277</td>\n      <td>0.000681</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>1</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.854167</td>\n      <td>0.847222</td>\n      <td>0.850174</td>\n      <td>0.839721</td>\n      <td>0.811847</td>\n      <td>0.836806</td>\n      <td>0.812500</td>\n      <td>0.860627</td>\n      <td>0.815331</td>\n      <td>0.850174</td>\n      <td>0.833333</td>\n      <td>0.812500</td>\n      <td>0.867596</td>\n      <td>0.864111</td>\n      <td>0.846690</td>\n      <td>0.840187</td>\n      <td>0.018692</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.005071</td>\n      <td>0.000772</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>random</td>\n      <td>1</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.840278</td>\n      <td>0.847222</td>\n      <td>0.843206</td>\n      <td>0.860627</td>\n      <td>0.836237</td>\n      <td>0.847222</td>\n      <td>0.861111</td>\n      <td>0.846690</td>\n      <td>0.853659</td>\n      <td>0.871080</td>\n      <td>0.864583</td>\n      <td>0.840278</td>\n      <td>0.832753</td>\n      <td>0.850174</td>\n      <td>0.860627</td>\n      <td>0.850383</td>\n      <td>0.010851</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.012545</td>\n      <td>0.003522</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>best</td>\n      <td>2</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.850694</td>\n      <td>0.833333</td>\n      <td>0.836237</td>\n      <td>0.836237</td>\n      <td>0.818815</td>\n      <td>0.836806</td>\n      <td>0.822917</td>\n      <td>0.846690</td>\n      <td>0.825784</td>\n      <td>0.822300</td>\n      <td>0.802083</td>\n      <td>0.833333</td>\n      <td>0.843206</td>\n      <td>0.857143</td>\n      <td>0.846690</td>\n      <td>0.834151</td>\n      <td>0.013702</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.004537</td>\n      <td>0.000619</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>random</td>\n      <td>2</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.861111</td>\n      <td>0.829861</td>\n      <td>0.853659</td>\n      <td>0.825784</td>\n      <td>0.811847</td>\n      <td>0.847222</td>\n      <td>0.812500</td>\n      <td>0.839721</td>\n      <td>0.829268</td>\n      <td>0.794425</td>\n      <td>0.857639</td>\n      <td>0.788194</td>\n      <td>0.825784</td>\n      <td>0.871080</td>\n      <td>0.857143</td>\n      <td>0.833683</td>\n      <td>0.023929</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.012144</td>\n      <td>0.001964</td>\n      <td>0.000801</td>\n      <td>0.001975</td>\n      <td>best</td>\n      <td>3</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.826389</td>\n      <td>0.826389</td>\n      <td>0.829268</td>\n      <td>0.815331</td>\n      <td>0.818815</td>\n      <td>0.829861</td>\n      <td>0.798611</td>\n      <td>0.839721</td>\n      <td>0.829268</td>\n      <td>0.797909</td>\n      <td>0.795139</td>\n      <td>0.809028</td>\n      <td>0.829268</td>\n      <td>0.860627</td>\n      <td>0.853659</td>\n      <td>0.823952</td>\n      <td>0.018406</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.004137</td>\n      <td>0.000340</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>random</td>\n      <td>3</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.819444</td>\n      <td>0.822917</td>\n      <td>0.860627</td>\n      <td>0.832753</td>\n      <td>0.822300</td>\n      <td>0.850694</td>\n      <td>0.802083</td>\n      <td>0.832753</td>\n      <td>0.801394</td>\n      <td>0.832753</td>\n      <td>0.798611</td>\n      <td>0.833333</td>\n      <td>0.815331</td>\n      <td>0.825784</td>\n      <td>0.825784</td>\n      <td>0.825104</td>\n      <td>0.016523</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.010543</td>\n      <td>0.000619</td>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>4</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.836806</td>\n      <td>0.822917</td>\n      <td>0.818815</td>\n      <td>0.843206</td>\n      <td>0.815331</td>\n      <td>0.819444</td>\n      <td>0.784722</td>\n      <td>0.815331</td>\n      <td>0.815331</td>\n      <td>0.811847</td>\n      <td>0.784722</td>\n      <td>0.826389</td>\n      <td>0.822300</td>\n      <td>0.836237</td>\n      <td>0.839721</td>\n      <td>0.819541</td>\n      <td>0.016628</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.004671</td>\n      <td>0.001740</td>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>random</td>\n      <td>4</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.819444</td>\n      <td>0.795139</td>\n      <td>0.825784</td>\n      <td>0.811847</td>\n      <td>0.787456</td>\n      <td>0.819444</td>\n      <td>0.809028</td>\n      <td>0.808362</td>\n      <td>0.801394</td>\n      <td>0.794425</td>\n      <td>0.802083</td>\n      <td>0.795139</td>\n      <td>0.818815</td>\n      <td>0.822300</td>\n      <td>0.846690</td>\n      <td>0.810490</td>\n      <td>0.014912</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.006339</td>\n      <td>0.000597</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>1</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.534722</td>\n      <td>0.513889</td>\n      <td>0.581882</td>\n      <td>0.571429</td>\n      <td>0.560976</td>\n      <td>0.569444</td>\n      <td>0.538194</td>\n      <td>0.540070</td>\n      <td>0.494774</td>\n      <td>0.581882</td>\n      <td>0.559028</td>\n      <td>0.559028</td>\n      <td>0.501742</td>\n      <td>0.557491</td>\n      <td>0.484321</td>\n      <td>0.543258</td>\n      <td>0.030546</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.002469</td>\n      <td>0.000806</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>random</td>\n      <td>1</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.513889</td>\n      <td>0.440972</td>\n      <td>0.498258</td>\n      <td>0.480836</td>\n      <td>0.557491</td>\n      <td>0.465278</td>\n      <td>0.447917</td>\n      <td>0.519164</td>\n      <td>0.529617</td>\n      <td>0.536585</td>\n      <td>0.496528</td>\n      <td>0.524306</td>\n      <td>0.466899</td>\n      <td>0.567944</td>\n      <td>0.540070</td>\n      <td>0.505717</td>\n      <td>0.037669</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.006406</td>\n      <td>0.000713</td>\n      <td>0.000801</td>\n      <td>0.002008</td>\n      <td>best</td>\n      <td>2</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.534722</td>\n      <td>0.513889</td>\n      <td>0.581882</td>\n      <td>0.571429</td>\n      <td>0.560976</td>\n      <td>0.569444</td>\n      <td>0.538194</td>\n      <td>0.540070</td>\n      <td>0.494774</td>\n      <td>0.581882</td>\n      <td>0.559028</td>\n      <td>0.559028</td>\n      <td>0.501742</td>\n      <td>0.557491</td>\n      <td>0.484321</td>\n      <td>0.543258</td>\n      <td>0.030546</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.002335</td>\n      <td>0.000472</td>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>random</td>\n      <td>2</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.506944</td>\n      <td>0.586806</td>\n      <td>0.484321</td>\n      <td>0.564460</td>\n      <td>0.512195</td>\n      <td>0.576389</td>\n      <td>0.458333</td>\n      <td>0.505226</td>\n      <td>0.439024</td>\n      <td>0.459930</td>\n      <td>0.524306</td>\n      <td>0.472222</td>\n      <td>0.533101</td>\n      <td>0.487805</td>\n      <td>0.536585</td>\n      <td>0.509843</td>\n      <td>0.042854</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.007207</td>\n      <td>0.002106</td>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>3</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.534722</td>\n      <td>0.513889</td>\n      <td>0.581882</td>\n      <td>0.571429</td>\n      <td>0.560976</td>\n      <td>0.569444</td>\n      <td>0.538194</td>\n      <td>0.540070</td>\n      <td>0.494774</td>\n      <td>0.581882</td>\n      <td>0.559028</td>\n      <td>0.559028</td>\n      <td>0.501742</td>\n      <td>0.557491</td>\n      <td>0.484321</td>\n      <td>0.543258</td>\n      <td>0.030546</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.002536</td>\n      <td>0.000885</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>random</td>\n      <td>3</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.486111</td>\n      <td>0.569444</td>\n      <td>0.512195</td>\n      <td>0.567944</td>\n      <td>0.581882</td>\n      <td>0.524306</td>\n      <td>0.517361</td>\n      <td>0.487805</td>\n      <td>0.529617</td>\n      <td>0.498258</td>\n      <td>0.486111</td>\n      <td>0.576389</td>\n      <td>0.459930</td>\n      <td>0.487805</td>\n      <td>0.526132</td>\n      <td>0.520753</td>\n      <td>0.036889</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.007874</td>\n      <td>0.003968</td>\n      <td>0.001468</td>\n      <td>0.004181</td>\n      <td>best</td>\n      <td>4</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.534722</td>\n      <td>0.513889</td>\n      <td>0.581882</td>\n      <td>0.571429</td>\n      <td>0.560976</td>\n      <td>0.569444</td>\n      <td>0.538194</td>\n      <td>0.540070</td>\n      <td>0.494774</td>\n      <td>0.581882</td>\n      <td>0.559028</td>\n      <td>0.559028</td>\n      <td>0.501742</td>\n      <td>0.557491</td>\n      <td>0.484321</td>\n      <td>0.543258</td>\n      <td>0.030546</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.002469</td>\n      <td>0.000499</td>\n      <td>0.000133</td>\n      <td>0.000340</td>\n      <td>random</td>\n      <td>4</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.472222</td>\n      <td>0.468750</td>\n      <td>0.515679</td>\n      <td>0.494774</td>\n      <td>0.498258</td>\n      <td>0.538194</td>\n      <td>0.510417</td>\n      <td>0.519164</td>\n      <td>0.501742</td>\n      <td>0.567944</td>\n      <td>0.572917</td>\n      <td>0.486111</td>\n      <td>0.491289</td>\n      <td>0.484321</td>\n      <td>0.560976</td>\n      <td>0.512184</td>\n      <td>0.032559</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.017349</td>\n      <td>0.002121</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>1</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.854167</td>\n      <td>0.850694</td>\n      <td>0.878049</td>\n      <td>0.846690</td>\n      <td>0.829268</td>\n      <td>0.878472</td>\n      <td>0.847222</td>\n      <td>0.888502</td>\n      <td>0.850174</td>\n      <td>0.836237</td>\n      <td>0.829861</td>\n      <td>0.840278</td>\n      <td>0.864111</td>\n      <td>0.815331</td>\n      <td>0.867596</td>\n      <td>0.851777</td>\n      <td>0.019831</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.005938</td>\n      <td>0.000443</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>random</td>\n      <td>1</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.868056</td>\n      <td>0.833333</td>\n      <td>0.808362</td>\n      <td>0.850174</td>\n      <td>0.787456</td>\n      <td>0.840278</td>\n      <td>0.857639</td>\n      <td>0.874564</td>\n      <td>0.864111</td>\n      <td>0.857143</td>\n      <td>0.836806</td>\n      <td>0.805556</td>\n      <td>0.867596</td>\n      <td>0.832753</td>\n      <td>0.829268</td>\n      <td>0.840873</td>\n      <td>0.024755</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.015614</td>\n      <td>0.000953</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>2</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.854167</td>\n      <td>0.829861</td>\n      <td>0.881533</td>\n      <td>0.857143</td>\n      <td>0.801394</td>\n      <td>0.875000</td>\n      <td>0.836806</td>\n      <td>0.874564</td>\n      <td>0.843206</td>\n      <td>0.825784</td>\n      <td>0.829861</td>\n      <td>0.840278</td>\n      <td>0.857143</td>\n      <td>0.801394</td>\n      <td>0.864111</td>\n      <td>0.844816</td>\n      <td>0.023911</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.005538</td>\n      <td>0.000499</td>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>random</td>\n      <td>2</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.864583</td>\n      <td>0.843750</td>\n      <td>0.860627</td>\n      <td>0.853659</td>\n      <td>0.801394</td>\n      <td>0.885417</td>\n      <td>0.864583</td>\n      <td>0.850174</td>\n      <td>0.839721</td>\n      <td>0.846690</td>\n      <td>0.875000</td>\n      <td>0.822917</td>\n      <td>0.825784</td>\n      <td>0.836237</td>\n      <td>0.822300</td>\n      <td>0.846189</td>\n      <td>0.021615</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.015080</td>\n      <td>0.001341</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>best</td>\n      <td>3</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.847222</td>\n      <td>0.819444</td>\n      <td>0.878049</td>\n      <td>0.832753</td>\n      <td>0.811847</td>\n      <td>0.875000</td>\n      <td>0.847222</td>\n      <td>0.891986</td>\n      <td>0.829268</td>\n      <td>0.825784</td>\n      <td>0.819444</td>\n      <td>0.833333</td>\n      <td>0.839721</td>\n      <td>0.801394</td>\n      <td>0.843206</td>\n      <td>0.839712</td>\n      <td>0.024537</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.005272</td>\n      <td>0.000574</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>random</td>\n      <td>3</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.864583</td>\n      <td>0.815972</td>\n      <td>0.839721</td>\n      <td>0.839721</td>\n      <td>0.822300</td>\n      <td>0.847222</td>\n      <td>0.829861</td>\n      <td>0.804878</td>\n      <td>0.794425</td>\n      <td>0.822300</td>\n      <td>0.868056</td>\n      <td>0.829861</td>\n      <td>0.815331</td>\n      <td>0.825784</td>\n      <td>0.825784</td>\n      <td>0.829720</td>\n      <td>0.019307</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.015347</td>\n      <td>0.001815</td>\n      <td>0.000601</td>\n      <td>0.000490</td>\n      <td>best</td>\n      <td>4</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.857639</td>\n      <td>0.815972</td>\n      <td>0.853659</td>\n      <td>0.829268</td>\n      <td>0.818815</td>\n      <td>0.868056</td>\n      <td>0.826389</td>\n      <td>0.881533</td>\n      <td>0.825784</td>\n      <td>0.822300</td>\n      <td>0.826389</td>\n      <td>0.843750</td>\n      <td>0.832753</td>\n      <td>0.822300</td>\n      <td>0.860627</td>\n      <td>0.839016</td>\n      <td>0.019711</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.005005</td>\n      <td>0.001156</td>\n      <td>0.000601</td>\n      <td>0.000490</td>\n      <td>random</td>\n      <td>4</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.791667</td>\n      <td>0.829861</td>\n      <td>0.874564</td>\n      <td>0.815331</td>\n      <td>0.794425</td>\n      <td>0.826389</td>\n      <td>0.871528</td>\n      <td>0.777003</td>\n      <td>0.794425</td>\n      <td>0.829268</td>\n      <td>0.819444</td>\n      <td>0.875000</td>\n      <td>0.864111</td>\n      <td>0.797909</td>\n      <td>0.839721</td>\n      <td>0.826710</td>\n      <td>0.031634</td>\n      <td>12</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Building Decision Tree Classifier\n",
    "\n",
    "DT_model = DecisionTreeClassifier()\n",
    "\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"min_samples_leaf\": [1,2,3,4],\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"splitter\": [\"best\", \"random\"]}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "RCV = RandomizedSearchCV(DT_model, param_dist, n_iter=50, scoring='accuracy', n_jobs=-1, cv=cv, random_state=1)\n",
    "\n",
    "DT = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(DT.cv_results_)\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 36 is smaller than n_iter=50. Running 36 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        0.177328      0.019438         0.001501    5.004009e-04   \n1        0.681621      0.023324         0.002336    4.718200e-04   \n2        3.303007      0.063671         0.010343    4.715393e-04   \n3        0.262572      0.006215         0.002002    3.576279e-07   \n4        1.116016      0.043078         0.004004    2.753021e-07   \n5        5.776259      0.108986         0.017516    1.385715e-03   \n6        0.374174      0.019065         0.002002    1.638375e-07   \n7        1.582107      0.011378         0.005505    5.003612e-04   \n8        8.565132      0.167357         0.024022    1.000841e-03   \n9        0.175660      0.010494         0.001668    4.718763e-04   \n10       0.674948      0.034676         0.004004    1.415656e-03   \n11       3.297336      0.081394         0.011844    3.731309e-04   \n12       0.278087      0.022526         0.002002    3.371748e-07   \n13       1.096165      0.015118         0.004171    3.730953e-04   \n14       5.576578      0.104421         0.018350    4.720166e-04   \n15       0.375008      0.006878         0.002002    1.638375e-07   \n16       1.650169      0.066753         0.005338    4.720167e-04   \n17       6.835224      0.204455         0.019851    3.728999e-04   \n18       0.162815      0.004273         0.002002    2.753021e-07   \n19       0.655764      0.013334         0.003169    3.729886e-04   \n20       1.307691      0.205025         0.005004    5.779280e-04   \n21       0.264240      0.002238         0.001668    4.717638e-04   \n22       0.687793      0.027175         0.002836    3.729176e-04   \n23       1.078649      0.021511         0.004337    4.719885e-04   \n24       0.377177      0.009865         0.002335    4.719604e-04   \n25       0.631408      0.033678         0.002836    3.728110e-04   \n26       1.015925      0.035637         0.004004    3.000035e-07   \n27       0.172324      0.006315         0.002169    3.731309e-04   \n28       0.726328      0.022550         0.003170    3.729354e-04   \n29       3.496517      0.072956         0.005171    1.345056e-03   \n30       0.153640      0.018634         0.001168    3.727932e-04   \n31       0.321292      0.110434         0.002002    2.139872e-07   \n32       1.147378      0.583076         0.003336    4.717638e-04   \n33       0.082241      0.006848         0.001001    2.665601e-07   \n34       0.164650      0.010285         0.001335    4.717076e-04   \n35       0.550668      0.030074         0.002169    3.730953e-04   \n\n   param_n_estimators param_max_depth param_learning_rate  \\\n0                   5               3                0.01   \n1                  20               3                0.01   \n2                 100               3                0.01   \n3                   5               5                0.01   \n4                  20               5                0.01   \n5                 100               5                0.01   \n6                   5               7                0.01   \n7                  20               7                0.01   \n8                 100               7                0.01   \n9                   5               3                 0.1   \n10                 20               3                 0.1   \n11                100               3                 0.1   \n12                  5               5                 0.1   \n13                 20               5                 0.1   \n14                100               5                 0.1   \n15                  5               7                 0.1   \n16                 20               7                 0.1   \n17                100               7                 0.1   \n18                  5               3                   1   \n19                 20               3                   1   \n20                100               3                   1   \n21                  5               5                   1   \n22                 20               5                   1   \n23                100               5                   1   \n24                  5               7                   1   \n25                 20               7                   1   \n26                100               7                   1   \n27                  5               3                  10   \n28                 20               3                  10   \n29                100               3                  10   \n30                  5               5                  10   \n31                 20               5                  10   \n32                100               5                  10   \n33                  5               7                  10   \n34                 20               7                  10   \n35                100               7                  10   \n\n                                               params  split0_test_score  \\\n0   {'n_estimators': 5, 'max_depth': 3, 'learning_...           0.769124   \n1   {'n_estimators': 20, 'max_depth': 3, 'learning...           0.824757   \n2   {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.865090   \n3   {'n_estimators': 5, 'max_depth': 5, 'learning_...           0.801113   \n4   {'n_estimators': 20, 'max_depth': 5, 'learning...           0.826147   \n5   {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.847010   \n6   {'n_estimators': 5, 'max_depth': 7, 'learning_...           0.835883   \n7   {'n_estimators': 20, 'max_depth': 7, 'learning...           0.833102   \n8   {'n_estimators': 100, 'max_depth': 7, 'learnin...           0.838665   \n9   {'n_estimators': 5, 'max_depth': 3, 'learning_...           0.852573   \n10  {'n_estimators': 20, 'max_depth': 3, 'learning...           0.894298   \n11  {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.934631   \n12  {'n_estimators': 5, 'max_depth': 5, 'learning_...           0.833102   \n13  {'n_estimators': 20, 'max_depth': 5, 'learning...           0.858136   \n14  {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.917942   \n15  {'n_estimators': 5, 'max_depth': 7, 'learning_...           0.821975   \n16  {'n_estimators': 20, 'max_depth': 7, 'learning...           0.838665   \n17  {'n_estimators': 100, 'max_depth': 7, 'learnin...           0.866481   \n18  {'n_estimators': 5, 'max_depth': 3, 'learning_...           0.849791   \n19  {'n_estimators': 20, 'max_depth': 3, 'learning...           0.915160   \n20  {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.940195   \n21  {'n_estimators': 5, 'max_depth': 5, 'learning_...           0.874826   \n22  {'n_estimators': 20, 'max_depth': 5, 'learning...           0.909597   \n23  {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.917942   \n24  {'n_estimators': 5, 'max_depth': 7, 'learning_...           0.897079   \n25  {'n_estimators': 20, 'max_depth': 7, 'learning...           0.902643   \n26  {'n_estimators': 100, 'max_depth': 7, 'learnin...           0.895688   \n27  {'n_estimators': 5, 'max_depth': 3, 'learning_...           0.186370   \n28  {'n_estimators': 20, 'max_depth': 3, 'learning...           0.331015   \n29  {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.015299   \n30  {'n_estimators': 5, 'max_depth': 5, 'learning_...           0.792768   \n31  {'n_estimators': 20, 'max_depth': 5, 'learning...           0.774687   \n32  {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.776078   \n33  {'n_estimators': 5, 'max_depth': 7, 'learning_...           0.780250   \n34  {'n_estimators': 20, 'max_depth': 7, 'learning...           0.766342   \n35  {'n_estimators': 100, 'max_depth': 7, 'learnin...           0.769124   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.753482           0.757997           0.785515   \n1            0.784123           0.821975           0.821727   \n2            0.853760           0.888734           0.866295   \n3            0.786908           0.812239           0.803621   \n4            0.803621           0.841446           0.834262   \n5            0.832869           0.887344           0.867688   \n6            0.779944           0.812239           0.818942   \n7            0.799443           0.830320           0.821727   \n8            0.805014           0.835883           0.818942   \n9            0.827298           0.847010           0.839833   \n10           0.899721           0.916551           0.906685   \n11           0.945682           0.955494           0.940111   \n12           0.795265           0.863700           0.831476   \n13           0.844011           0.901252           0.889972   \n14           0.909471           0.934631           0.905292   \n15           0.807799           0.813630           0.805014   \n16           0.807799           0.835883           0.811978   \n17           0.857939           0.901252           0.869081   \n18           0.825905           0.855355           0.824513   \n19           0.909471           0.926287           0.896936   \n20           0.933148           0.936022           0.908078   \n21           0.864903           0.894298           0.873259   \n22           0.916435           0.924896           0.920613   \n23           0.913649           0.931850           0.910864   \n24           0.889972           0.892907           0.862117   \n25           0.899721           0.906815           0.883008   \n26           0.899721           0.892907           0.896936   \n27           0.313370           0.141864           0.079387   \n28           0.249304           0.141864           0.077994   \n29           0.286908           0.111266           0.077994   \n30           0.732591           0.806676           0.778552   \n31           0.743733           0.798331           0.802228   \n32           0.731198           0.809458           0.777159   \n33           0.729805           0.788595           0.749304   \n34           0.743733           0.801113           0.739554   \n35           0.750696           0.791377           0.745125   \n\n    split4_test_score  split5_test_score  mean_test_score  std_test_score  \\\n0            0.732962           0.771588         0.761778        0.016468   \n1            0.802503           0.828691         0.813963        0.015704   \n2            0.849791           0.874652         0.866387        0.012937   \n3            0.801113           0.814763         0.803293        0.009034   \n4            0.806676           0.841226         0.825563        0.015339   \n5            0.847010           0.870474         0.858732        0.018168   \n6            0.777469           0.824513         0.808165        0.022011   \n7            0.798331           0.825905         0.818138        0.014067   \n8            0.806676           0.828691         0.822312        0.013212   \n9            0.816412           0.855153         0.839713        0.013869   \n10           0.891516           0.899721         0.901415        0.008279   \n11           0.934631           0.954039         0.944098        0.008429   \n12           0.805285           0.846797         0.829271        0.023251   \n13           0.862309           0.884401         0.873347        0.019963   \n14           0.904033           0.931755         0.917187        0.012187   \n15           0.791377           0.807799         0.807932        0.009240   \n16           0.806676           0.831476         0.822080        0.013523   \n17           0.840056           0.881616         0.869404        0.018995   \n18           0.845619           0.876045         0.846205        0.017654   \n19           0.895688           0.938719         0.913710        0.015329   \n20           0.919332           0.947075         0.930642        0.013125   \n21           0.863700           0.891365         0.877058        0.011885   \n22           0.916551           0.937326         0.920903        0.008684   \n23           0.905424           0.938719         0.919741        0.011768   \n24           0.878999           0.894150         0.885871        0.012055   \n25           0.881780           0.906685         0.896775        0.010460   \n26           0.884562           0.915042         0.897476        0.009172   \n27           0.158554           0.103064         0.163768        0.075503   \n28           0.214186           0.101671         0.186006        0.088087   \n29           0.158554           0.101671         0.125282        0.083980   \n30           0.748261           0.809192         0.778007        0.028752   \n31           0.764951           0.798050         0.780330        0.021312   \n32           0.756606           0.781337         0.771973        0.023928   \n33           0.762170           0.781337         0.765243        0.020611   \n34           0.746871           0.774373         0.761998        0.021479   \n35           0.762170           0.775766         0.765710        0.015456   \n\n    rank_test_score  \n0                33  \n1                23  \n2                14  \n3                26  \n4                19  \n5                15  \n6                24  \n7                22  \n8                20  \n9                17  \n10                7  \n11                1  \n12               18  \n13               12  \n14                5  \n15               25  \n16               21  \n17               13  \n18               16  \n19                6  \n20                2  \n21               11  \n22                3  \n23                4  \n24               10  \n25                9  \n26                8  \n27               35  \n28               34  \n29               36  \n30               28  \n31               27  \n32               29  \n33               31  \n34               32  \n35               30  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_n_estimators</th>\n      <th>param_max_depth</th>\n      <th>param_learning_rate</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.177328</td>\n      <td>0.019438</td>\n      <td>0.001501</td>\n      <td>5.004009e-04</td>\n      <td>5</td>\n      <td>3</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 5, 'max_depth': 3, 'learning_...</td>\n      <td>0.769124</td>\n      <td>0.753482</td>\n      <td>0.757997</td>\n      <td>0.785515</td>\n      <td>0.732962</td>\n      <td>0.771588</td>\n      <td>0.761778</td>\n      <td>0.016468</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.681621</td>\n      <td>0.023324</td>\n      <td>0.002336</td>\n      <td>4.718200e-04</td>\n      <td>20</td>\n      <td>3</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 20, 'max_depth': 3, 'learning...</td>\n      <td>0.824757</td>\n      <td>0.784123</td>\n      <td>0.821975</td>\n      <td>0.821727</td>\n      <td>0.802503</td>\n      <td>0.828691</td>\n      <td>0.813963</td>\n      <td>0.015704</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.303007</td>\n      <td>0.063671</td>\n      <td>0.010343</td>\n      <td>4.715393e-04</td>\n      <td>100</td>\n      <td>3</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.865090</td>\n      <td>0.853760</td>\n      <td>0.888734</td>\n      <td>0.866295</td>\n      <td>0.849791</td>\n      <td>0.874652</td>\n      <td>0.866387</td>\n      <td>0.012937</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.262572</td>\n      <td>0.006215</td>\n      <td>0.002002</td>\n      <td>3.576279e-07</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 5, 'max_depth': 5, 'learning_...</td>\n      <td>0.801113</td>\n      <td>0.786908</td>\n      <td>0.812239</td>\n      <td>0.803621</td>\n      <td>0.801113</td>\n      <td>0.814763</td>\n      <td>0.803293</td>\n      <td>0.009034</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.116016</td>\n      <td>0.043078</td>\n      <td>0.004004</td>\n      <td>2.753021e-07</td>\n      <td>20</td>\n      <td>5</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 20, 'max_depth': 5, 'learning...</td>\n      <td>0.826147</td>\n      <td>0.803621</td>\n      <td>0.841446</td>\n      <td>0.834262</td>\n      <td>0.806676</td>\n      <td>0.841226</td>\n      <td>0.825563</td>\n      <td>0.015339</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5.776259</td>\n      <td>0.108986</td>\n      <td>0.017516</td>\n      <td>1.385715e-03</td>\n      <td>100</td>\n      <td>5</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.847010</td>\n      <td>0.832869</td>\n      <td>0.887344</td>\n      <td>0.867688</td>\n      <td>0.847010</td>\n      <td>0.870474</td>\n      <td>0.858732</td>\n      <td>0.018168</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.374174</td>\n      <td>0.019065</td>\n      <td>0.002002</td>\n      <td>1.638375e-07</td>\n      <td>5</td>\n      <td>7</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 5, 'max_depth': 7, 'learning_...</td>\n      <td>0.835883</td>\n      <td>0.779944</td>\n      <td>0.812239</td>\n      <td>0.818942</td>\n      <td>0.777469</td>\n      <td>0.824513</td>\n      <td>0.808165</td>\n      <td>0.022011</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.582107</td>\n      <td>0.011378</td>\n      <td>0.005505</td>\n      <td>5.003612e-04</td>\n      <td>20</td>\n      <td>7</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 20, 'max_depth': 7, 'learning...</td>\n      <td>0.833102</td>\n      <td>0.799443</td>\n      <td>0.830320</td>\n      <td>0.821727</td>\n      <td>0.798331</td>\n      <td>0.825905</td>\n      <td>0.818138</td>\n      <td>0.014067</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8.565132</td>\n      <td>0.167357</td>\n      <td>0.024022</td>\n      <td>1.000841e-03</td>\n      <td>100</td>\n      <td>7</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 100, 'max_depth': 7, 'learnin...</td>\n      <td>0.838665</td>\n      <td>0.805014</td>\n      <td>0.835883</td>\n      <td>0.818942</td>\n      <td>0.806676</td>\n      <td>0.828691</td>\n      <td>0.822312</td>\n      <td>0.013212</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.175660</td>\n      <td>0.010494</td>\n      <td>0.001668</td>\n      <td>4.718763e-04</td>\n      <td>5</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 5, 'max_depth': 3, 'learning_...</td>\n      <td>0.852573</td>\n      <td>0.827298</td>\n      <td>0.847010</td>\n      <td>0.839833</td>\n      <td>0.816412</td>\n      <td>0.855153</td>\n      <td>0.839713</td>\n      <td>0.013869</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.674948</td>\n      <td>0.034676</td>\n      <td>0.004004</td>\n      <td>1.415656e-03</td>\n      <td>20</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 20, 'max_depth': 3, 'learning...</td>\n      <td>0.894298</td>\n      <td>0.899721</td>\n      <td>0.916551</td>\n      <td>0.906685</td>\n      <td>0.891516</td>\n      <td>0.899721</td>\n      <td>0.901415</td>\n      <td>0.008279</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3.297336</td>\n      <td>0.081394</td>\n      <td>0.011844</td>\n      <td>3.731309e-04</td>\n      <td>100</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.934631</td>\n      <td>0.945682</td>\n      <td>0.955494</td>\n      <td>0.940111</td>\n      <td>0.934631</td>\n      <td>0.954039</td>\n      <td>0.944098</td>\n      <td>0.008429</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.278087</td>\n      <td>0.022526</td>\n      <td>0.002002</td>\n      <td>3.371748e-07</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 5, 'max_depth': 5, 'learning_...</td>\n      <td>0.833102</td>\n      <td>0.795265</td>\n      <td>0.863700</td>\n      <td>0.831476</td>\n      <td>0.805285</td>\n      <td>0.846797</td>\n      <td>0.829271</td>\n      <td>0.023251</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.096165</td>\n      <td>0.015118</td>\n      <td>0.004171</td>\n      <td>3.730953e-04</td>\n      <td>20</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 20, 'max_depth': 5, 'learning...</td>\n      <td>0.858136</td>\n      <td>0.844011</td>\n      <td>0.901252</td>\n      <td>0.889972</td>\n      <td>0.862309</td>\n      <td>0.884401</td>\n      <td>0.873347</td>\n      <td>0.019963</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>5.576578</td>\n      <td>0.104421</td>\n      <td>0.018350</td>\n      <td>4.720166e-04</td>\n      <td>100</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.917942</td>\n      <td>0.909471</td>\n      <td>0.934631</td>\n      <td>0.905292</td>\n      <td>0.904033</td>\n      <td>0.931755</td>\n      <td>0.917187</td>\n      <td>0.012187</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.375008</td>\n      <td>0.006878</td>\n      <td>0.002002</td>\n      <td>1.638375e-07</td>\n      <td>5</td>\n      <td>7</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 5, 'max_depth': 7, 'learning_...</td>\n      <td>0.821975</td>\n      <td>0.807799</td>\n      <td>0.813630</td>\n      <td>0.805014</td>\n      <td>0.791377</td>\n      <td>0.807799</td>\n      <td>0.807932</td>\n      <td>0.009240</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1.650169</td>\n      <td>0.066753</td>\n      <td>0.005338</td>\n      <td>4.720167e-04</td>\n      <td>20</td>\n      <td>7</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 20, 'max_depth': 7, 'learning...</td>\n      <td>0.838665</td>\n      <td>0.807799</td>\n      <td>0.835883</td>\n      <td>0.811978</td>\n      <td>0.806676</td>\n      <td>0.831476</td>\n      <td>0.822080</td>\n      <td>0.013523</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>6.835224</td>\n      <td>0.204455</td>\n      <td>0.019851</td>\n      <td>3.728999e-04</td>\n      <td>100</td>\n      <td>7</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 7, 'learnin...</td>\n      <td>0.866481</td>\n      <td>0.857939</td>\n      <td>0.901252</td>\n      <td>0.869081</td>\n      <td>0.840056</td>\n      <td>0.881616</td>\n      <td>0.869404</td>\n      <td>0.018995</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.162815</td>\n      <td>0.004273</td>\n      <td>0.002002</td>\n      <td>2.753021e-07</td>\n      <td>5</td>\n      <td>3</td>\n      <td>1</td>\n      <td>{'n_estimators': 5, 'max_depth': 3, 'learning_...</td>\n      <td>0.849791</td>\n      <td>0.825905</td>\n      <td>0.855355</td>\n      <td>0.824513</td>\n      <td>0.845619</td>\n      <td>0.876045</td>\n      <td>0.846205</td>\n      <td>0.017654</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.655764</td>\n      <td>0.013334</td>\n      <td>0.003169</td>\n      <td>3.729886e-04</td>\n      <td>20</td>\n      <td>3</td>\n      <td>1</td>\n      <td>{'n_estimators': 20, 'max_depth': 3, 'learning...</td>\n      <td>0.915160</td>\n      <td>0.909471</td>\n      <td>0.926287</td>\n      <td>0.896936</td>\n      <td>0.895688</td>\n      <td>0.938719</td>\n      <td>0.913710</td>\n      <td>0.015329</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>1.307691</td>\n      <td>0.205025</td>\n      <td>0.005004</td>\n      <td>5.779280e-04</td>\n      <td>100</td>\n      <td>3</td>\n      <td>1</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.940195</td>\n      <td>0.933148</td>\n      <td>0.936022</td>\n      <td>0.908078</td>\n      <td>0.919332</td>\n      <td>0.947075</td>\n      <td>0.930642</td>\n      <td>0.013125</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.264240</td>\n      <td>0.002238</td>\n      <td>0.001668</td>\n      <td>4.717638e-04</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'n_estimators': 5, 'max_depth': 5, 'learning_...</td>\n      <td>0.874826</td>\n      <td>0.864903</td>\n      <td>0.894298</td>\n      <td>0.873259</td>\n      <td>0.863700</td>\n      <td>0.891365</td>\n      <td>0.877058</td>\n      <td>0.011885</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.687793</td>\n      <td>0.027175</td>\n      <td>0.002836</td>\n      <td>3.729176e-04</td>\n      <td>20</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'n_estimators': 20, 'max_depth': 5, 'learning...</td>\n      <td>0.909597</td>\n      <td>0.916435</td>\n      <td>0.924896</td>\n      <td>0.920613</td>\n      <td>0.916551</td>\n      <td>0.937326</td>\n      <td>0.920903</td>\n      <td>0.008684</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1.078649</td>\n      <td>0.021511</td>\n      <td>0.004337</td>\n      <td>4.719885e-04</td>\n      <td>100</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.917942</td>\n      <td>0.913649</td>\n      <td>0.931850</td>\n      <td>0.910864</td>\n      <td>0.905424</td>\n      <td>0.938719</td>\n      <td>0.919741</td>\n      <td>0.011768</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.377177</td>\n      <td>0.009865</td>\n      <td>0.002335</td>\n      <td>4.719604e-04</td>\n      <td>5</td>\n      <td>7</td>\n      <td>1</td>\n      <td>{'n_estimators': 5, 'max_depth': 7, 'learning_...</td>\n      <td>0.897079</td>\n      <td>0.889972</td>\n      <td>0.892907</td>\n      <td>0.862117</td>\n      <td>0.878999</td>\n      <td>0.894150</td>\n      <td>0.885871</td>\n      <td>0.012055</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.631408</td>\n      <td>0.033678</td>\n      <td>0.002836</td>\n      <td>3.728110e-04</td>\n      <td>20</td>\n      <td>7</td>\n      <td>1</td>\n      <td>{'n_estimators': 20, 'max_depth': 7, 'learning...</td>\n      <td>0.902643</td>\n      <td>0.899721</td>\n      <td>0.906815</td>\n      <td>0.883008</td>\n      <td>0.881780</td>\n      <td>0.906685</td>\n      <td>0.896775</td>\n      <td>0.010460</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1.015925</td>\n      <td>0.035637</td>\n      <td>0.004004</td>\n      <td>3.000035e-07</td>\n      <td>100</td>\n      <td>7</td>\n      <td>1</td>\n      <td>{'n_estimators': 100, 'max_depth': 7, 'learnin...</td>\n      <td>0.895688</td>\n      <td>0.899721</td>\n      <td>0.892907</td>\n      <td>0.896936</td>\n      <td>0.884562</td>\n      <td>0.915042</td>\n      <td>0.897476</td>\n      <td>0.009172</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.172324</td>\n      <td>0.006315</td>\n      <td>0.002169</td>\n      <td>3.731309e-04</td>\n      <td>5</td>\n      <td>3</td>\n      <td>10</td>\n      <td>{'n_estimators': 5, 'max_depth': 3, 'learning_...</td>\n      <td>0.186370</td>\n      <td>0.313370</td>\n      <td>0.141864</td>\n      <td>0.079387</td>\n      <td>0.158554</td>\n      <td>0.103064</td>\n      <td>0.163768</td>\n      <td>0.075503</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.726328</td>\n      <td>0.022550</td>\n      <td>0.003170</td>\n      <td>3.729354e-04</td>\n      <td>20</td>\n      <td>3</td>\n      <td>10</td>\n      <td>{'n_estimators': 20, 'max_depth': 3, 'learning...</td>\n      <td>0.331015</td>\n      <td>0.249304</td>\n      <td>0.141864</td>\n      <td>0.077994</td>\n      <td>0.214186</td>\n      <td>0.101671</td>\n      <td>0.186006</td>\n      <td>0.088087</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>3.496517</td>\n      <td>0.072956</td>\n      <td>0.005171</td>\n      <td>1.345056e-03</td>\n      <td>100</td>\n      <td>3</td>\n      <td>10</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.015299</td>\n      <td>0.286908</td>\n      <td>0.111266</td>\n      <td>0.077994</td>\n      <td>0.158554</td>\n      <td>0.101671</td>\n      <td>0.125282</td>\n      <td>0.083980</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.153640</td>\n      <td>0.018634</td>\n      <td>0.001168</td>\n      <td>3.727932e-04</td>\n      <td>5</td>\n      <td>5</td>\n      <td>10</td>\n      <td>{'n_estimators': 5, 'max_depth': 5, 'learning_...</td>\n      <td>0.792768</td>\n      <td>0.732591</td>\n      <td>0.806676</td>\n      <td>0.778552</td>\n      <td>0.748261</td>\n      <td>0.809192</td>\n      <td>0.778007</td>\n      <td>0.028752</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.321292</td>\n      <td>0.110434</td>\n      <td>0.002002</td>\n      <td>2.139872e-07</td>\n      <td>20</td>\n      <td>5</td>\n      <td>10</td>\n      <td>{'n_estimators': 20, 'max_depth': 5, 'learning...</td>\n      <td>0.774687</td>\n      <td>0.743733</td>\n      <td>0.798331</td>\n      <td>0.802228</td>\n      <td>0.764951</td>\n      <td>0.798050</td>\n      <td>0.780330</td>\n      <td>0.021312</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>1.147378</td>\n      <td>0.583076</td>\n      <td>0.003336</td>\n      <td>4.717638e-04</td>\n      <td>100</td>\n      <td>5</td>\n      <td>10</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.776078</td>\n      <td>0.731198</td>\n      <td>0.809458</td>\n      <td>0.777159</td>\n      <td>0.756606</td>\n      <td>0.781337</td>\n      <td>0.771973</td>\n      <td>0.023928</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.082241</td>\n      <td>0.006848</td>\n      <td>0.001001</td>\n      <td>2.665601e-07</td>\n      <td>5</td>\n      <td>7</td>\n      <td>10</td>\n      <td>{'n_estimators': 5, 'max_depth': 7, 'learning_...</td>\n      <td>0.780250</td>\n      <td>0.729805</td>\n      <td>0.788595</td>\n      <td>0.749304</td>\n      <td>0.762170</td>\n      <td>0.781337</td>\n      <td>0.765243</td>\n      <td>0.020611</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.164650</td>\n      <td>0.010285</td>\n      <td>0.001335</td>\n      <td>4.717076e-04</td>\n      <td>20</td>\n      <td>7</td>\n      <td>10</td>\n      <td>{'n_estimators': 20, 'max_depth': 7, 'learning...</td>\n      <td>0.766342</td>\n      <td>0.743733</td>\n      <td>0.801113</td>\n      <td>0.739554</td>\n      <td>0.746871</td>\n      <td>0.774373</td>\n      <td>0.761998</td>\n      <td>0.021479</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.550668</td>\n      <td>0.030074</td>\n      <td>0.002169</td>\n      <td>3.730953e-04</td>\n      <td>100</td>\n      <td>7</td>\n      <td>10</td>\n      <td>{'n_estimators': 100, 'max_depth': 7, 'learnin...</td>\n      <td>0.769124</td>\n      <td>0.750696</td>\n      <td>0.791377</td>\n      <td>0.745125</td>\n      <td>0.762170</td>\n      <td>0.775766</td>\n      <td>0.765710</td>\n      <td>0.015456</td>\n      <td>30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Building Gradient Boosting Classifier\n",
    "\n",
    "GB_model = GradientBoostingClassifier()\n",
    "param_dist = {\n",
    "    \"n_estimators\":[5,20,100],\n",
    "    \"max_depth\":[3,5,7],\n",
    "    \"learning_rate\":[0.01,0.1,1,10]\n",
    "}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(GB_model, param_dist, n_iter=50, scoring='accuracy', n_jobs=-1, cv=cv, random_state=1)\n",
    "\n",
    "GB = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(GB.cv_results_)\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Citation :\n",
    "Stephanie Glen. \"Regularization: Simple Definition, L1 & L2 Penalties\" From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/regularization/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}