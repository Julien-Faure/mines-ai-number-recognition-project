{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0b8e1d",
   "metadata": {},
   "source": [
    "# Rapport du projet de résolution de problème\n",
    "\n",
    "- Paul Achard\n",
    "- Julien Faure\n",
    "\n",
    "    \n",
    "- *Date : 20/01/2022*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1070770",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42622bf2",
   "metadata": {},
   "source": [
    "## Problématique\n",
    "\n",
    "Notre problématique est d'identifier un chiffre à partir d'une image.\n",
    "Plus spécifiquement, une image de 8x8 pixels en niveau de gris."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3a0c6",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "- Trouver un modèle permettant d'identifier un chiffre à partir d'une image\n",
    "- Comparer les différents modèles ainsi que leur hyper paramètres\n",
    "\n",
    "## Analyse du dataset\n",
    "\n",
    "Identifier un chiffre à partir d'une image est une tache qui peut s'avérer très complexe. Afin d'avoir une difficulté raisonnable et adapté au contexte de ce projet, nous avons fixé certains paramètres dans notre dataset.\n",
    "\n",
    "- La résolution de nos images est identique pour tout le dataset. Cette résolution est **8 pixels par 8 pixels**.\n",
    "- Chaque pixel est codé sur **4 bits**.\n",
    "- Les images contiennent uniquement un chiffre sans élément parasite, sans effet et sans traitement.\n",
    "\n",
    "Nous utilisons le dataset `digits` de `sklearn`.\n",
    "\n",
    "### Forme du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "894968c2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data Shape (1797, 64)\n",
      "Label Data Shape (1797,)\n"
     ]
    }
   ],
   "source": [
    "# Import du dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Affiche le nombre d'images et leur format\n",
    "print(\"Image Data Shape\" , digits.data.shape)\n",
    "\n",
    "# Affiche le nombre de labels\n",
    "print(\"Label Data Shape\", digits.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311b24e7",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir ci-dessus, le dataset est composé de **1797** images labellisées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb1ba9",
   "metadata": {},
   "source": [
    "### Répartition des images du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1826bdc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Text(0.5, 0, 'n° labellisé'), Text(0, 0.5, 'Occurrence')]"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphique = sns.countplot(x=digits.target)\n",
    "graphique.set(xlabel=\"n° labellisé\", ylabel = \"Occurrence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee617b1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Les données labellisées sont équitablement distribuées (environ 175 par label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc387e8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Représentation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17d087b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x216 with 10 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Création de 10 figures de 10 par 3 pixels\n",
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 3))\n",
    "\n",
    "# Pour chaque figure, on affiche l'image du chiffre et son label en titre\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "    ax.set_title(\"Label: %i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e6cc1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Représentation des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb283e09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "\n",
      " Type de chaque valeur : <class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "# Affiche le tableau représentant la première image\n",
    "print(digits.images[0])\n",
    "print(\"\\n Type de chaque valeur :\", type(digits.images[0][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa72f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Comme nous l'avons vu précédemment, chaque image possède une résolution de 8x8 pixels en niveaux de gris codés sur 4bits par pixel.\n",
    "\n",
    "Dans notre programme, une image est représentée par une matrice de dimension 8x8. Chaque élément représente un pixel avec un niveau de gris codé sur 4bits (de 0 à 15). Plus la valeur est élevée, plus la couleur est foncée.\n",
    "\n",
    "> Exemple :\n",
    "* 0 : Blanc\n",
    "* 15 : Noir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3675a",
   "metadata": {},
   "source": [
    "### Mise en forme des données d'entrées\n",
    "\n",
    "Pour commencer, nous devons redimenssionner nos données d'entrées.\n",
    "\n",
    "Actuellement, nous avons des données sous la forme d'un tableau de matrice.\n",
    "\n",
    "Il nous faut les mettre sous forme d'une matrice où chaque vecteur correspond aux pixels de l'image.\n",
    "\n",
    "Donc si nous avons 1797 images, la matrice d'entrée est composée de 1797 vecteurs.\n",
    "\n",
    "Avec une résolution de 8x8, la taille d'un vecteur est de 64 valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74b2574d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Redimensionne la matrice en vecteur\n",
    "print(digits.images[0].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e10f7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redimenssionne le tableau de matrice en matrice\n",
    "x = digits.images.reshape((len(digits.images), -1))\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Préparation des données\n",
    "\n",
    "### Jeu de test et jeu d'entraînement\n",
    "\n",
    "Il nous faut maintenant séparer notre jeu de test et notre jeu d'entraînement du dataset.\n",
    "\n",
    "Pour choisir la taille de notre jeu de test, il est nécessaire de faire attention à plusieurs points :\n",
    "* Le temps d'entrainement de notre modèle\n",
    "* La taille de notre dataset\n",
    "\n",
    "Plus la proportion du jeu d'entraînement est faible, plus notre modèle à un niveau de variance élevé. Ainsi, on augmente les chances d'avoir de l'*over fitting*.\n",
    "\n",
    "A contrario, plus la proportion du jeu de test est faible, plus notre modèle à un niveau de variance faible. Ainsi, on augmente les chances d'avoir de l'*under fitting*.\n",
    "\n",
    "Le but est donc de trouver la valeur qui nous permet d'avoir le taux de variance optimal.\n",
    "\n",
    "\n",
    "Pour trouver cette valeur, nous avons appliqué la procédure suivante, nous avons essayé avec plusieurs valeurs en partant de 20% jusqu'à 80% avec un pas de 5%. Nous avons déterminé que la meilleure valeur est **35%**.\n",
    "\n",
    "\n",
    "> [https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio](https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# séparation du dataset en données \"d'apprentissage\" et de test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.35, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Création du tableau de résultat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "                               Best Mean Test Score\nLogistic Regression (LR)                        0.0\nNaïve Bayes Classifier (NB)                     0.0\nDecision Tree Classifier (DT)                   0.0\nGradient Boosting (GB)                          0.0\nSupport Vector Machines (SVM)                   0.0\nK Nearest Neighbours (KNN)                      0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Best Mean Test Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression (LR)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Naïve Bayes Classifier (NB)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Decision Tree Classifier (DT)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting (GB)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Vector Machines (SVM)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>K Nearest Neighbours (KNN)</th>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluation_Results = pd.DataFrame(np.zeros((6, 1)), columns=['Best Mean Test Score'])\n",
    "Evaluation_Results.index = ['Logistic Regression (LR)', 'Naïve Bayes Classifier (NB)',\n",
    "                            'Decision Tree Classifier (DT)', 'Gradient Boosting (GB)',\n",
    "                            'Support Vector Machines (SVM)', 'K Nearest Neighbours (KNN)']\n",
    "Evaluation_Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test de différents modèles\n",
    "\n",
    "Pour résoudre notre problème, nous allons essayer les modèles suivants :\n",
    "* Modèle de regréssion logistique multiclasse\n",
    "* CLassification naïve bayésienne\n",
    "* Classification par abre de décision\n",
    "* Classification par boosting de gradient\n",
    "* Classification par machine à vecteurs de support\n",
    "* Classification par voisin le plus proche\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "334ad595",
   "metadata": {},
   "source": [
    "### Modèle de regréssion logistique multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc863948",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 18 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.97231332 0.97231332 0.96489613 0.96659819 0.95546507        nan\n",
      " 0.96917575 0.96946187 0.95748138 0.96659819 0.95546507        nan\n",
      " 0.96917697 0.96860595 0.95633934 0.96659819 0.95546507        nan]\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [1.         1.         0.99600438 1.         1.                nan\n",
      " 1.         1.         0.99978594 1.         1.                nan\n",
      " 1.         1.         1.         1.         1.                nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n0        0.629559      0.074395         0.001217        0.002744     0.1   \n1        2.144651      0.964299         0.000537        0.002011     0.1   \n2        0.053008      0.003526         0.000412        0.000825     0.1   \n3        0.127398      0.004972         0.000000        0.000000     0.1   \n4        0.048062      0.006584         0.001080        0.002754     0.1   \n5        0.000816      0.002066         0.000000        0.000000     0.1   \n6        0.716656      0.088624         0.000533        0.001360       5   \n7        2.704039      1.516779         0.000539        0.002018       5   \n8        0.094637      0.006317         0.000955        0.002077       5   \n9        0.153253      0.011619         0.001621        0.003233       5   \n10       0.055071      0.005314         0.000135        0.000506       5   \n11       0.002036      0.003134         0.000000        0.000000       5   \n12       0.799792      0.078536         0.000807        0.002189      10   \n13       3.448546      1.064220         0.000407        0.001023      10   \n14       0.110232      0.008289         0.000000        0.000000      10   \n15       0.187700      0.117336         0.000679        0.002050      10   \n16       0.111737      0.113498         0.001353        0.002724      10   \n17       0.002171      0.007589         0.000000        0.000000      10   \n\n   param_penalty param_solver  \\\n0             l2    newton-cg   \n1             l2        lbfgs   \n2             l2    liblinear   \n3           none    newton-cg   \n4           none        lbfgs   \n5           none    liblinear   \n6             l2    newton-cg   \n7             l2        lbfgs   \n8             l2    liblinear   \n9           none    newton-cg   \n10          none        lbfgs   \n11          none    liblinear   \n12            l2    newton-cg   \n13            l2        lbfgs   \n14            l2    liblinear   \n15          none    newton-cg   \n16          none        lbfgs   \n17          none    liblinear   \n\n                                               params  split0_test_score  \\\n0   {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-...           0.978632   \n1      {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}           0.978632   \n2   {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...           0.978632   \n3   {'C': 0.1, 'penalty': 'none', 'solver': 'newto...           0.970085   \n4    {'C': 0.1, 'penalty': 'none', 'solver': 'lbfgs'}           0.957265   \n5   {'C': 0.1, 'penalty': 'none', 'solver': 'libli...                NaN   \n6    {'C': 5, 'penalty': 'l2', 'solver': 'newton-cg'}           0.978632   \n7        {'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}           0.978632   \n8    {'C': 5, 'penalty': 'l2', 'solver': 'liblinear'}           0.970085   \n9   {'C': 5, 'penalty': 'none', 'solver': 'newton-...           0.970085   \n10     {'C': 5, 'penalty': 'none', 'solver': 'lbfgs'}           0.957265   \n11  {'C': 5, 'penalty': 'none', 'solver': 'libline...                NaN   \n12  {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}           0.974359   \n13      {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}           0.974359   \n14  {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}           0.970085   \n15  {'C': 10, 'penalty': 'none', 'solver': 'newton...           0.970085   \n16    {'C': 10, 'penalty': 'none', 'solver': 'lbfgs'}           0.957265   \n17  {'C': 10, 'penalty': 'none', 'solver': 'liblin...                NaN   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.965812           0.974359           0.982833   \n1            0.965812           0.974359           0.982833   \n2            0.965812           0.961538           0.969957   \n3            0.957265           0.965812           0.974249   \n4            0.948718           0.965812           0.944206   \n5                 NaN                NaN                NaN   \n6            0.970085           0.965812           0.974249   \n7            0.970085           0.965812           0.974249   \n8            0.965812           0.952991           0.965665   \n9            0.957265           0.965812           0.974249   \n10           0.948718           0.965812           0.944206   \n11                NaN                NaN                NaN   \n12           0.965812           0.965812           0.978541   \n13           0.965812           0.965812           0.978541   \n14           0.965812           0.944444           0.969957   \n15           0.957265           0.965812           0.974249   \n16           0.948718           0.965812           0.944206   \n17                NaN                NaN                NaN   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.969957           0.974359           0.987179   \n1            0.969957           0.974359           0.987179   \n2            0.952790           0.957265           0.982906   \n3            0.961373           0.978632           0.982906   \n4            0.948498           0.970085           0.970085   \n5                 NaN                NaN                NaN   \n6            0.965665           0.974359           0.978632   \n7            0.969957           0.974359           0.978632   \n8            0.952790           0.957265           0.978632   \n9            0.961373           0.978632           0.982906   \n10           0.948498           0.970085           0.970085   \n11                NaN                NaN                NaN   \n12           0.965665           0.978632           0.978632   \n13           0.965665           0.974359           0.978632   \n14           0.952790           0.957265           0.974359   \n15           0.961373           0.978632           0.982906   \n16           0.948498           0.970085           0.970085   \n17                NaN                NaN                NaN   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.970085           0.957082           0.982833   \n1            0.970085           0.957082           0.982833   \n2            0.952991           0.948498           0.982833   \n3            0.974359           0.957082           0.974249   \n4            0.965812           0.922747           0.969957   \n5                 NaN                NaN                NaN   \n6            0.965812           0.965665           0.987124   \n7            0.965812           0.965665           0.987124   \n8            0.927350           0.948498           0.969957   \n9            0.974359           0.957082           0.974249   \n10           0.965812           0.922747           0.969957   \n11                NaN                NaN                NaN   \n12           0.965812           0.965665           0.987124   \n13           0.965812           0.965665           0.987124   \n14           0.935897           0.944206           0.965665   \n15           0.974359           0.957082           0.974249   \n16           0.965812           0.922747           0.969957   \n17                NaN                NaN                NaN   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.970085            0.961538            0.982906   \n1             0.970085            0.961538            0.982906   \n2             0.952991            0.961538            0.974359   \n3             0.965812            0.965812            0.978632   \n4             0.948718            0.957265            0.965812   \n5                  NaN                 NaN                 NaN   \n6             0.957265            0.957265            0.982906   \n7             0.957265            0.957265            0.982906   \n8             0.935897            0.940171            0.974359   \n9             0.965812            0.965812            0.978632   \n10            0.948718            0.957265            0.965812   \n11                 NaN                 NaN                 NaN   \n12            0.957265            0.957265            0.982906   \n13            0.957265            0.957265            0.982906   \n14            0.931624            0.940171            0.974359   \n15            0.965812            0.965812            0.978632   \n16            0.948718            0.957265            0.965812   \n17                 NaN                 NaN                 NaN   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.969957            0.957082         0.972313        0.009115   \n1             0.969957            0.957082         0.972313        0.009115   \n2             0.961373            0.969957         0.964896        0.010858   \n3             0.957082            0.935622         0.966598        0.011553   \n4             0.957082            0.939914         0.955465        0.013029   \n5                  NaN                 NaN              NaN             NaN   \n6             0.965665            0.948498         0.969176        0.010046   \n7             0.965665            0.948498         0.969462        0.010003   \n8             0.965665            0.957082         0.957481        0.014251   \n9             0.957082            0.935622         0.966598        0.011553   \n10            0.957082            0.939914         0.955465        0.013029   \n11                 NaN                 NaN              NaN             NaN   \n12            0.965665            0.948498         0.969177        0.010284   \n13            0.965665            0.944206         0.968606        0.010692   \n14            0.965665            0.952790         0.956339        0.013886   \n15            0.957082            0.935622         0.966598        0.011553   \n16            0.957082            0.939914         0.955465        0.013029   \n17                 NaN                 NaN              NaN             NaN   \n\n    rank_test_score  split0_train_score  split1_train_score  \\\n0                 1            1.000000            1.000000   \n1                 1            1.000000            1.000000   \n2                10            0.994647            0.998929   \n3                 7            1.000000            1.000000   \n4                13            1.000000            1.000000   \n5                17                 NaN                 NaN   \n6                 5            1.000000            1.000000   \n7                 3            1.000000            1.000000   \n8                11            0.998929            1.000000   \n9                 7            1.000000            1.000000   \n10               13            1.000000            1.000000   \n11               16                 NaN                 NaN   \n12                4            1.000000            1.000000   \n13                6            1.000000            1.000000   \n14               12            1.000000            1.000000   \n15                7            1.000000            1.000000   \n16               13            1.000000            1.000000   \n17               18                 NaN                 NaN   \n\n    split2_train_score  split3_train_score  split4_train_score  \\\n0             1.000000            1.000000            1.000000   \n1             1.000000            1.000000            1.000000   \n2             0.994647            0.995722            0.996791   \n3             1.000000            1.000000            1.000000   \n4             1.000000            1.000000            1.000000   \n5                  NaN                 NaN                 NaN   \n6             1.000000            1.000000            1.000000   \n7             1.000000            1.000000            1.000000   \n8             1.000000            1.000000            1.000000   \n9             1.000000            1.000000            1.000000   \n10            1.000000            1.000000            1.000000   \n11                 NaN                 NaN                 NaN   \n12            1.000000            1.000000            1.000000   \n13            1.000000            1.000000            1.000000   \n14            1.000000            1.000000            1.000000   \n15            1.000000            1.000000            1.000000   \n16            1.000000            1.000000            1.000000   \n17                 NaN                 NaN                 NaN   \n\n    split5_train_score  split6_train_score  split7_train_score  \\\n0             1.000000            1.000000            1.000000   \n1             1.000000            1.000000            1.000000   \n2             0.992505            0.996788            0.995717   \n3             1.000000            1.000000            1.000000   \n4             1.000000            1.000000            1.000000   \n5                  NaN                 NaN                 NaN   \n6             1.000000            1.000000            1.000000   \n7             1.000000            1.000000            1.000000   \n8             1.000000            0.998929            1.000000   \n9             1.000000            1.000000            1.000000   \n10            1.000000            1.000000            1.000000   \n11                 NaN                 NaN                 NaN   \n12            1.000000            1.000000            1.000000   \n13            1.000000            1.000000            1.000000   \n14            1.000000            1.000000            1.000000   \n15            1.000000            1.000000            1.000000   \n16            1.000000            1.000000            1.000000   \n17                 NaN                 NaN                 NaN   \n\n    split8_train_score  split9_train_score  split10_train_score  \\\n0             1.000000            1.000000             1.000000   \n1             1.000000            1.000000             1.000000   \n2             0.995722            0.996791             0.995717   \n3             1.000000            1.000000             1.000000   \n4             1.000000            1.000000             1.000000   \n5                  NaN                 NaN                  NaN   \n6             1.000000            1.000000             1.000000   \n7             1.000000            1.000000             1.000000   \n8             1.000000            1.000000             1.000000   \n9             1.000000            1.000000             1.000000   \n10            1.000000            1.000000             1.000000   \n11                 NaN                 NaN                  NaN   \n12            1.000000            1.000000             1.000000   \n13            1.000000            1.000000             1.000000   \n14            1.000000            1.000000             1.000000   \n15            1.000000            1.000000             1.000000   \n16            1.000000            1.000000             1.000000   \n17                 NaN                 NaN                  NaN   \n\n    split11_train_score  split12_train_score  split13_train_score  \\\n0              1.000000             1.000000             1.000000   \n1              1.000000             1.000000             1.000000   \n2              0.996788             0.995717             0.996791   \n3              1.000000             1.000000             1.000000   \n4              1.000000             1.000000             1.000000   \n5                   NaN                  NaN                  NaN   \n6              1.000000             1.000000             1.000000   \n7              1.000000             1.000000             1.000000   \n8              1.000000             1.000000             1.000000   \n9              1.000000             1.000000             1.000000   \n10             1.000000             1.000000             1.000000   \n11                  NaN                  NaN                  NaN   \n12             1.000000             1.000000             1.000000   \n13             1.000000             1.000000             1.000000   \n14             1.000000             1.000000             1.000000   \n15             1.000000             1.000000             1.000000   \n16             1.000000             1.000000             1.000000   \n17                  NaN                  NaN                  NaN   \n\n    split14_train_score  mean_train_score  std_train_score  \n0              1.000000          1.000000         0.000000  \n1              1.000000          1.000000         0.000000  \n2              0.996791          0.996004         0.001381  \n3              1.000000          1.000000         0.000000  \n4              1.000000          1.000000         0.000000  \n5                   NaN               NaN              NaN  \n6              1.000000          1.000000         0.000000  \n7              1.000000          1.000000         0.000000  \n8              0.998930          0.999786         0.000428  \n9              1.000000          1.000000         0.000000  \n10             1.000000          1.000000         0.000000  \n11                  NaN               NaN              NaN  \n12             1.000000          1.000000         0.000000  \n13             1.000000          1.000000         0.000000  \n14             1.000000          1.000000         0.000000  \n15             1.000000          1.000000         0.000000  \n16             1.000000          1.000000         0.000000  \n17                  NaN               NaN              NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_C</th>\n      <th>param_penalty</th>\n      <th>param_solver</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n      <th>split0_train_score</th>\n      <th>split1_train_score</th>\n      <th>split2_train_score</th>\n      <th>split3_train_score</th>\n      <th>split4_train_score</th>\n      <th>split5_train_score</th>\n      <th>split6_train_score</th>\n      <th>split7_train_score</th>\n      <th>split8_train_score</th>\n      <th>split9_train_score</th>\n      <th>split10_train_score</th>\n      <th>split11_train_score</th>\n      <th>split12_train_score</th>\n      <th>split13_train_score</th>\n      <th>split14_train_score</th>\n      <th>mean_train_score</th>\n      <th>std_train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.629559</td>\n      <td>0.074395</td>\n      <td>0.001217</td>\n      <td>0.002744</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'newton-...</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.957082</td>\n      <td>0.982833</td>\n      <td>0.970085</td>\n      <td>0.961538</td>\n      <td>0.982906</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.972313</td>\n      <td>0.009115</td>\n      <td>1</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.144651</td>\n      <td>0.964299</td>\n      <td>0.000537</td>\n      <td>0.002011</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.957082</td>\n      <td>0.982833</td>\n      <td>0.970085</td>\n      <td>0.961538</td>\n      <td>0.982906</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.972313</td>\n      <td>0.009115</td>\n      <td>1</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.053008</td>\n      <td>0.003526</td>\n      <td>0.000412</td>\n      <td>0.000825</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'libline...</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.952991</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.964896</td>\n      <td>0.010858</td>\n      <td>10</td>\n      <td>0.994647</td>\n      <td>0.998929</td>\n      <td>0.994647</td>\n      <td>0.995722</td>\n      <td>0.996791</td>\n      <td>0.992505</td>\n      <td>0.996788</td>\n      <td>0.995717</td>\n      <td>0.995722</td>\n      <td>0.996791</td>\n      <td>0.995717</td>\n      <td>0.996788</td>\n      <td>0.995717</td>\n      <td>0.996791</td>\n      <td>0.996791</td>\n      <td>0.996004</td>\n      <td>0.001381</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.127398</td>\n      <td>0.004972</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'newto...</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.978632</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.957082</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.966598</td>\n      <td>0.011553</td>\n      <td>7</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.048062</td>\n      <td>0.006584</td>\n      <td>0.001080</td>\n      <td>0.002754</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.957265</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.948498</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.922747</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.939914</td>\n      <td>0.955465</td>\n      <td>0.013029</td>\n      <td>13</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000816</td>\n      <td>0.002066</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'libli...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>17</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.716656</td>\n      <td>0.088624</td>\n      <td>0.000533</td>\n      <td>0.001360</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.969176</td>\n      <td>0.010046</td>\n      <td>5</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2.704039</td>\n      <td>1.516779</td>\n      <td>0.000539</td>\n      <td>0.002018</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.969462</td>\n      <td>0.010003</td>\n      <td>3</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.094637</td>\n      <td>0.006317</td>\n      <td>0.000955</td>\n      <td>0.002077</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'liblinear'}</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.978632</td>\n      <td>0.927350</td>\n      <td>0.948498</td>\n      <td>0.969957</td>\n      <td>0.935897</td>\n      <td>0.940171</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.957481</td>\n      <td>0.014251</td>\n      <td>11</td>\n      <td>0.998929</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.998929</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.998930</td>\n      <td>0.999786</td>\n      <td>0.000428</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.153253</td>\n      <td>0.011619</td>\n      <td>0.001621</td>\n      <td>0.003233</td>\n      <td>5</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'newton-...</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.978632</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.957082</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.966598</td>\n      <td>0.011553</td>\n      <td>7</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.055071</td>\n      <td>0.005314</td>\n      <td>0.000135</td>\n      <td>0.000506</td>\n      <td>5</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.957265</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.948498</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.922747</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.939914</td>\n      <td>0.955465</td>\n      <td>0.013029</td>\n      <td>13</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.002036</td>\n      <td>0.003134</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'libline...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>16</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.799792</td>\n      <td>0.078536</td>\n      <td>0.000807</td>\n      <td>0.002189</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.965665</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.969177</td>\n      <td>0.010284</td>\n      <td>4</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3.448546</td>\n      <td>1.064220</td>\n      <td>0.000407</td>\n      <td>0.001023</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.944206</td>\n      <td>0.968606</td>\n      <td>0.010692</td>\n      <td>6</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.110232</td>\n      <td>0.008289</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.944444</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.974359</td>\n      <td>0.935897</td>\n      <td>0.944206</td>\n      <td>0.965665</td>\n      <td>0.931624</td>\n      <td>0.940171</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.956339</td>\n      <td>0.013886</td>\n      <td>12</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.187700</td>\n      <td>0.117336</td>\n      <td>0.000679</td>\n      <td>0.002050</td>\n      <td>10</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'newton...</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.978632</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.957082</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.966598</td>\n      <td>0.011553</td>\n      <td>7</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.111737</td>\n      <td>0.113498</td>\n      <td>0.001353</td>\n      <td>0.002724</td>\n      <td>10</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.957265</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.948498</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.922747</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.939914</td>\n      <td>0.955465</td>\n      <td>0.013029</td>\n      <td>13</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.002171</td>\n      <td>0.007589</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>10</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'liblin...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création du modèle de régression\n",
    "LR_model = LogisticRegression(verbose=False, max_iter=10000)\n",
    "\n",
    "# Dictionnaire contenant les différents paramètres à essayer\n",
    "LR_params = dict()\n",
    "LR_params['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "LR_params['penalty'] = ['l2', 'none']\n",
    "LR_params['C'] = [0.1, 5, 10]\n",
    "\n",
    "# Création de nos itérations\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "# Création de notre modèle de validation croisée\n",
    "model_cv = GridSearchCV(estimator=LR_model,\n",
    "                        param_grid=LR_params,\n",
    "                        scoring='accuracy',\n",
    "                        cv=kFold,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True)\n",
    "\n",
    "# Entrainement du modèle\n",
    "model_cv.fit(x_train, y_train)\n",
    "# cv results\n",
    "pd.set_option('display.max_columns', None)\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[0]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31144b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "La validation croisée nous permet de determiner les paramètres optimaux pour notre modèle d'apprentissage.\n",
    "Les résultats de nos tests avec de nombreux paramètres différents (dont la plupart ne sont pas présent dans l'exemple ci-dessus).\n",
    "* Le **paramètre C** correspond à l'inverse de la force de régularisation des données. Plus ce paramètre est grand, plus le risque d'overfitting est grand. Une valeur de 0.1 semble être optimale.\n",
    "* Le **paramètre de pénalité** permet de réduire les coefficients θ. On remarque une perte de précision si l'on n'applique pas de pénalité. La meilleure pénalité semble être la norme L2.\n",
    "* Le **solveur** correspond à l'algorithme d'optimisation utilisé pour l'entrainement. Dans notre cas, lbfgs permet d'obtenir la precision la plus élevée malgré un temps d'entrainement significativement plus long que ses concurrents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b3aae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Classification naïve bayésienne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9833fcac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n0       0.002513      0.005388         0.000428        0.001602           0   \n1       0.008271      0.011004         0.002058        0.004222      0.0001   \n2       0.004065      0.007621         0.001352        0.003105       0.001   \n3       0.005410      0.007391         0.001961        0.005265        0.01   \n4       0.003923      0.004079         0.000274        0.000698         0.1   \n5       0.004051      0.004113         0.000816        0.002065         0.5   \n6       0.003044      0.003899         0.001574        0.003172         1.0   \n\n              params  split0_test_score  split1_test_score  split2_test_score  \\\n0       {'alpha': 0}           0.837607           0.871795           0.854701   \n1  {'alpha': 0.0001}           0.841880           0.884615           0.854701   \n2   {'alpha': 0.001}           0.846154           0.884615           0.854701   \n3    {'alpha': 0.01}           0.850427           0.884615           0.854701   \n4     {'alpha': 0.1}           0.854701           0.880342           0.854701   \n5     {'alpha': 0.5}           0.850427           0.876068           0.850427   \n6     {'alpha': 1.0}           0.850427           0.871795           0.850427   \n\n   split3_test_score  split4_test_score  split5_test_score  split6_test_score  \\\n0           0.836910           0.845494           0.871795           0.850427   \n1           0.849785           0.854077           0.876068           0.880342   \n2           0.849785           0.858369           0.876068           0.884615   \n3           0.849785           0.858369           0.867521           0.876068   \n4           0.849785           0.858369           0.863248           0.871795   \n5           0.845494           0.845494           0.858974           0.867521   \n6           0.841202           0.845494           0.858974           0.867521   \n\n   split7_test_score  split8_test_score  split9_test_score  \\\n0           0.854701           0.819742           0.875536   \n1           0.863248           0.828326           0.879828   \n2           0.867521           0.836910           0.875536   \n3           0.867521           0.841202           0.875536   \n4           0.867521           0.841202           0.884120   \n5           0.867521           0.836910           0.884120   \n6           0.867521           0.832618           0.879828   \n\n   split10_test_score  split11_test_score  split12_test_score  \\\n0            0.833333            0.854701            0.858974   \n1            0.854701            0.858974            0.867521   \n2            0.854701            0.858974            0.867521   \n3            0.858974            0.858974            0.863248   \n4            0.867521            0.858974            0.854701   \n5            0.871795            0.850427            0.854701   \n6            0.871795            0.841880            0.854701   \n\n   split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0            0.866953            0.871245         0.853594        0.015985   \n1            0.871245            0.884120         0.863296        0.015941   \n2            0.871245            0.884120         0.864722        0.014346   \n3            0.871245            0.884120         0.864154        0.012277   \n4            0.871245            0.888412         0.864442        0.012759   \n5            0.854077            0.875536         0.859300        0.013215   \n6            0.854077            0.875536         0.857587        0.013731   \n\n   rank_test_score  \n0                7  \n1                4  \n2                1  \n3                3  \n4                2  \n5                5  \n6                6  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_alpha</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.002513</td>\n      <td>0.005388</td>\n      <td>0.000428</td>\n      <td>0.001602</td>\n      <td>0</td>\n      <td>{'alpha': 0}</td>\n      <td>0.837607</td>\n      <td>0.871795</td>\n      <td>0.854701</td>\n      <td>0.836910</td>\n      <td>0.845494</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.854701</td>\n      <td>0.819742</td>\n      <td>0.875536</td>\n      <td>0.833333</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.866953</td>\n      <td>0.871245</td>\n      <td>0.853594</td>\n      <td>0.015985</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.008271</td>\n      <td>0.011004</td>\n      <td>0.002058</td>\n      <td>0.004222</td>\n      <td>0.0001</td>\n      <td>{'alpha': 0.0001}</td>\n      <td>0.841880</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.854077</td>\n      <td>0.876068</td>\n      <td>0.880342</td>\n      <td>0.863248</td>\n      <td>0.828326</td>\n      <td>0.879828</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.863296</td>\n      <td>0.015941</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.004065</td>\n      <td>0.007621</td>\n      <td>0.001352</td>\n      <td>0.003105</td>\n      <td>0.001</td>\n      <td>{'alpha': 0.001}</td>\n      <td>0.846154</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.876068</td>\n      <td>0.884615</td>\n      <td>0.867521</td>\n      <td>0.836910</td>\n      <td>0.875536</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.864722</td>\n      <td>0.014346</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.005410</td>\n      <td>0.007391</td>\n      <td>0.001961</td>\n      <td>0.005265</td>\n      <td>0.01</td>\n      <td>{'alpha': 0.01}</td>\n      <td>0.850427</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.867521</td>\n      <td>0.876068</td>\n      <td>0.867521</td>\n      <td>0.841202</td>\n      <td>0.875536</td>\n      <td>0.858974</td>\n      <td>0.858974</td>\n      <td>0.863248</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.864154</td>\n      <td>0.012277</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.003923</td>\n      <td>0.004079</td>\n      <td>0.000274</td>\n      <td>0.000698</td>\n      <td>0.1</td>\n      <td>{'alpha': 0.1}</td>\n      <td>0.854701</td>\n      <td>0.880342</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.863248</td>\n      <td>0.871795</td>\n      <td>0.867521</td>\n      <td>0.841202</td>\n      <td>0.884120</td>\n      <td>0.867521</td>\n      <td>0.858974</td>\n      <td>0.854701</td>\n      <td>0.871245</td>\n      <td>0.888412</td>\n      <td>0.864442</td>\n      <td>0.012759</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.004051</td>\n      <td>0.004113</td>\n      <td>0.000816</td>\n      <td>0.002065</td>\n      <td>0.5</td>\n      <td>{'alpha': 0.5}</td>\n      <td>0.850427</td>\n      <td>0.876068</td>\n      <td>0.850427</td>\n      <td>0.845494</td>\n      <td>0.845494</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.867521</td>\n      <td>0.836910</td>\n      <td>0.884120</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.854701</td>\n      <td>0.854077</td>\n      <td>0.875536</td>\n      <td>0.859300</td>\n      <td>0.013215</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.003044</td>\n      <td>0.003899</td>\n      <td>0.001574</td>\n      <td>0.003172</td>\n      <td>1.0</td>\n      <td>{'alpha': 1.0}</td>\n      <td>0.850427</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.841202</td>\n      <td>0.845494</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.867521</td>\n      <td>0.832618</td>\n      <td>0.879828</td>\n      <td>0.871795</td>\n      <td>0.841880</td>\n      <td>0.854701</td>\n      <td>0.854077</td>\n      <td>0.875536</td>\n      <td>0.857587</td>\n      <td>0.013731</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_model = BernoulliNB()\n",
    "\n",
    "NB_params = {'alpha': [0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0]}\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(NB_model, NB_params, n_iter=7, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(RCV.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[1]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le seul paramètre testé dans ce modèle, alpha, correspond à la force du lissage de Laplace à appliquer au modèle.\n",
    "En d'autres termes, ce paramètre permet de palier la présence d'une caractéristique dans le jeu de test qui n'existe pas dans le jeu d'entrainement.\n",
    "> https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece\n",
    "\n",
    "Au vu des résultats, le paramètre alpha semble être optimal autour de 0.001\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification par arbre de décision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        0.008339      0.013826         0.002086        0.005318   \n1        0.005206      0.007362         0.000000        0.000000   \n2        0.005644      0.009215         0.000000        0.000000   \n3        0.004432      0.006140         0.001480        0.004123   \n4        0.007297      0.011226         0.000000        0.000000   \n5        0.002085      0.005316         0.001041        0.003897   \n6        0.007728      0.007554         0.000000        0.000000   \n7        0.004167      0.006911         0.000000        0.000000   \n8        0.016113      0.004584         0.000135        0.000506   \n9        0.005791      0.005648         0.000672        0.002035   \n10       0.013616      0.004202         0.000536        0.002007   \n11       0.005394      0.004880         0.001213        0.002740   \n12       0.014035      0.005706         0.000136        0.000508   \n13       0.004322      0.004488         0.000540        0.002019   \n14       0.016198      0.010531         0.000538        0.002014   \n15       0.004992      0.004744         0.000273        0.000696   \n16       0.008103      0.004843         0.000672        0.002037   \n17       0.002030      0.003130         0.000137        0.000512   \n18       0.006317      0.004219         0.001243        0.002716   \n19       0.003376      0.003974         0.000000        0.000000   \n20       0.010117      0.006049         0.000674        0.002038   \n21       0.003101      0.003894         0.000273        0.000696   \n22       0.012822      0.006560         0.000000        0.000000   \n23       0.002291      0.003113         0.000674        0.002044   \n24       0.018180      0.002499         0.000266        0.000997   \n25       0.006597      0.003651         0.000541        0.002024   \n26       0.019157      0.002656         0.000548        0.001397   \n27       0.007202      0.005881         0.001067        0.001769   \n28       0.019197      0.006233         0.000266        0.000996   \n29       0.006934      0.006924         0.000534        0.001361   \n30       0.015464      0.003221         0.001067        0.001769   \n31       0.005603      0.002444         0.000798        0.001596   \n\n   param_splitter param_min_samples_leaf param_max_depth param_criterion  \\\n0            best                      1               3            gini   \n1          random                      1               3            gini   \n2            best                      2               3            gini   \n3          random                      2               3            gini   \n4            best                      3               3            gini   \n5          random                      3               3            gini   \n6            best                      4               3            gini   \n7          random                      4               3            gini   \n8            best                      1            None            gini   \n9          random                      1            None            gini   \n10           best                      2            None            gini   \n11         random                      2            None            gini   \n12           best                      3            None            gini   \n13         random                      3            None            gini   \n14           best                      4            None            gini   \n15         random                      4            None            gini   \n16           best                      1               3         entropy   \n17         random                      1               3         entropy   \n18           best                      2               3         entropy   \n19         random                      2               3         entropy   \n20           best                      3               3         entropy   \n21         random                      3               3         entropy   \n22           best                      4               3         entropy   \n23         random                      4               3         entropy   \n24           best                      1            None         entropy   \n25         random                      1            None         entropy   \n26           best                      2            None         entropy   \n27         random                      2            None         entropy   \n28           best                      3            None         entropy   \n29         random                      3            None         entropy   \n30           best                      4            None         entropy   \n31         random                      4            None         entropy   \n\n                                               params  split0_test_score  \\\n0   {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.410256   \n1   {'splitter': 'random', 'min_samples_leaf': 1, ...           0.414530   \n2   {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.410256   \n3   {'splitter': 'random', 'min_samples_leaf': 2, ...           0.461538   \n4   {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.410256   \n5   {'splitter': 'random', 'min_samples_leaf': 3, ...           0.491453   \n6   {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.410256   \n7   {'splitter': 'random', 'min_samples_leaf': 4, ...           0.440171   \n8   {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.807692   \n9   {'splitter': 'random', 'min_samples_leaf': 1, ...           0.769231   \n10  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.764957   \n11  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.769231   \n12  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.756410   \n13  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.816239   \n14  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.730769   \n15  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.786325   \n16  {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.495726   \n17  {'splitter': 'random', 'min_samples_leaf': 1, ...           0.517094   \n18  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.495726   \n19  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.512821   \n20  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.495726   \n21  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.534188   \n22  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.495726   \n23  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.581197   \n24  {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.799145   \n25  {'splitter': 'random', 'min_samples_leaf': 1, ...           0.833333   \n26  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.773504   \n27  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.769231   \n28  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.760684   \n29  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.764957   \n30  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.769231   \n31  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.769231   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.470085           0.410256           0.484979   \n1            0.547009           0.517094           0.553648   \n2            0.470085           0.410256           0.484979   \n3            0.525641           0.564103           0.605150   \n4            0.470085           0.410256           0.484979   \n5            0.491453           0.448718           0.480687   \n6            0.470085           0.410256           0.484979   \n7            0.487179           0.440171           0.502146   \n8            0.816239           0.846154           0.811159   \n9            0.867521           0.837607           0.836910   \n10           0.829060           0.854701           0.793991   \n11           0.820513           0.833333           0.759657   \n12           0.824786           0.833333           0.789700   \n13           0.863248           0.816239           0.789700   \n14           0.841880           0.829060           0.793991   \n15           0.824786           0.824786           0.793991   \n16           0.529915           0.555556           0.575107   \n17           0.465812           0.521368           0.553648   \n18           0.529915           0.555556           0.575107   \n19           0.435897           0.504274           0.553648   \n20           0.529915           0.555556           0.575107   \n21           0.517094           0.568376           0.515021   \n22           0.529915           0.555556           0.575107   \n23           0.547009           0.491453           0.510730   \n24           0.858974           0.824786           0.854077   \n25           0.850427           0.850427           0.888412   \n26           0.846154           0.816239           0.858369   \n27           0.854701           0.833333           0.811159   \n28           0.858974           0.816239           0.832618   \n29           0.841880           0.850427           0.845494   \n30           0.871795           0.816239           0.828326   \n31           0.820513           0.829060           0.815451   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.463519           0.423077           0.457265   \n1            0.484979           0.495726           0.452991   \n2            0.463519           0.423077           0.457265   \n3            0.472103           0.487179           0.478632   \n4            0.463519           0.423077           0.457265   \n5            0.557940           0.551282           0.414530   \n6            0.463519           0.423077           0.457265   \n7            0.390558           0.576923           0.482906   \n8            0.811159           0.846154           0.841880   \n9            0.832618           0.871795           0.777778   \n10           0.781116           0.833333           0.829060   \n11           0.716738           0.794872           0.846154   \n12           0.776824           0.841880           0.837607   \n13           0.789700           0.816239           0.811966   \n14           0.776824           0.846154           0.824786   \n15           0.721030           0.807692           0.829060   \n16           0.553648           0.576923           0.538462   \n17           0.549356           0.525641           0.504274   \n18           0.553648           0.576923           0.538462   \n19           0.515021           0.602564           0.517094   \n20           0.553648           0.576923           0.538462   \n21           0.476395           0.521368           0.555556   \n22           0.553648           0.576923           0.538462   \n23           0.540773           0.474359           0.478632   \n24           0.815451           0.829060           0.829060   \n25           0.824034           0.824786           0.850427   \n26           0.836910           0.854701           0.811966   \n27           0.806867           0.854701           0.850427   \n28           0.819742           0.820513           0.794872   \n29           0.793991           0.833333           0.782051   \n30           0.819742           0.841880           0.803419   \n31           0.832618           0.811966           0.841880   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.457265           0.472103           0.467811   \n1            0.482906           0.476395           0.536481   \n2            0.457265           0.472103           0.472103   \n3            0.508547           0.489270           0.459227   \n4            0.457265           0.472103           0.467811   \n5            0.474359           0.407725           0.463519   \n6            0.457265           0.472103           0.467811   \n7            0.512821           0.493562           0.467811   \n8            0.807692           0.785408           0.781116   \n9            0.799145           0.832618           0.789700   \n10           0.803419           0.742489           0.768240   \n11           0.829060           0.768240           0.824034   \n12           0.803419           0.759657           0.751073   \n13           0.803419           0.785408           0.824034   \n14           0.824786           0.776824           0.755365   \n15           0.850427           0.793991           0.819742   \n16           0.581197           0.532189           0.566524   \n17           0.547009           0.510730           0.484979   \n18           0.581197           0.532189           0.566524   \n19           0.534188           0.497854           0.557940   \n20           0.581197           0.532189           0.566524   \n21           0.427350           0.557940           0.549356   \n22           0.581197           0.532189           0.566524   \n23           0.576923           0.506438           0.472103   \n24           0.867521           0.759657           0.802575   \n25           0.807692           0.815451           0.802575   \n26           0.863248           0.751073           0.824034   \n27           0.807692           0.746781           0.802575   \n28           0.841880           0.742489           0.836910   \n29           0.794872           0.785408           0.841202   \n30           0.850427           0.733906           0.806867   \n31           0.820513           0.793991           0.798283   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.423077            0.452991            0.465812   \n1             0.448718            0.427350            0.504274   \n2             0.423077            0.452991            0.465812   \n3             0.431624            0.448718            0.461538   \n4             0.423077            0.452991            0.465812   \n5             0.495726            0.504274            0.465812   \n6             0.423077            0.452991            0.465812   \n7             0.482906            0.474359            0.585470   \n8             0.773504            0.829060            0.858974   \n9             0.794872            0.824786            0.816239   \n10            0.752137            0.841880            0.829060   \n11            0.811966            0.841880            0.790598   \n12            0.769231            0.820513            0.816239   \n13            0.747863            0.824786            0.837607   \n14            0.773504            0.807692            0.807692   \n15            0.782051            0.854701            0.799145   \n16            0.559829            0.576923            0.581197   \n17            0.448718            0.521368            0.568376   \n18            0.559829            0.576923            0.581197   \n19            0.418803            0.474359            0.551282   \n20            0.559829            0.576923            0.581197   \n21            0.457265            0.559829            0.581197   \n22            0.559829            0.576923            0.581197   \n23            0.448718            0.555556            0.619658   \n24            0.782051            0.901709            0.799145   \n25            0.807692            0.790598            0.846154   \n26            0.769231            0.897436            0.799145   \n27            0.829060            0.833333            0.811966   \n28            0.760684            0.876068            0.786325   \n29            0.833333            0.833333            0.782051   \n30            0.773504            0.893162            0.756410   \n31            0.803419            0.824786            0.807692   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.493562            0.446352         0.453227        0.024958   \n1             0.497854            0.437768         0.485182        0.041496   \n2             0.493562            0.446352         0.453514        0.025147   \n3             0.519313            0.553648         0.497749        0.046556   \n4             0.493562            0.446352         0.453227        0.024958   \n5             0.467811            0.407725         0.474868        0.043379   \n6             0.493562            0.442060         0.452941        0.025060   \n7             0.575107            0.527897         0.495999        0.052403   \n8             0.858369            0.798283         0.818190        0.026691   \n9             0.781116            0.815451         0.816492        0.030095   \n10            0.849785            0.781116         0.803623        0.035841   \n11            0.815451            0.785408         0.800476        0.034699   \n12            0.832618            0.798283         0.800772        0.030724   \n13            0.841202            0.772532         0.809345        0.028093   \n14            0.824034            0.785408         0.799918        0.032108   \n15            0.802575            0.781116         0.804761        0.031516   \n16            0.549356            0.545064         0.554508        0.023036   \n17            0.472103            0.626609         0.521139        0.043455   \n18            0.549356            0.545064         0.554508        0.023036   \n19            0.536481            0.540773         0.516867        0.045712   \n20            0.549356            0.545064         0.554508        0.023036   \n21            0.536481            0.575107         0.528835        0.043230   \n22            0.549356            0.545064         0.554508        0.023036   \n23            0.549356            0.600858         0.530251        0.049947   \n24            0.836910            0.811159         0.824752        0.034740   \n25            0.811159            0.789700         0.826191        0.026168   \n26            0.836910            0.811159         0.823339        0.038061   \n27            0.819742            0.785408         0.814465        0.029685   \n28            0.845494            0.806867         0.813357        0.037033   \n29            0.772532            0.806867         0.810782        0.028986   \n30            0.862661            0.785408         0.814199        0.043725   \n31            0.793991            0.802575         0.811065        0.017820   \n\n    rank_test_score  \n0                30  \n1                27  \n2                29  \n3                25  \n4                30  \n5                28  \n6                32  \n7                26  \n8                 4  \n9                 5  \n10               13  \n11               15  \n12               14  \n13               11  \n14               16  \n15               12  \n16               17  \n17               23  \n18               17  \n19               24  \n20               17  \n21               22  \n22               17  \n23               21  \n24                2  \n25                1  \n26                3  \n27                6  \n28                8  \n29               10  \n30                7  \n31                9  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_splitter</th>\n      <th>param_min_samples_leaf</th>\n      <th>param_max_depth</th>\n      <th>param_criterion</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.008339</td>\n      <td>0.013826</td>\n      <td>0.002086</td>\n      <td>0.005318</td>\n      <td>best</td>\n      <td>1</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453227</td>\n      <td>0.024958</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.005206</td>\n      <td>0.007362</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>random</td>\n      <td>1</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.414530</td>\n      <td>0.547009</td>\n      <td>0.517094</td>\n      <td>0.553648</td>\n      <td>0.484979</td>\n      <td>0.495726</td>\n      <td>0.452991</td>\n      <td>0.482906</td>\n      <td>0.476395</td>\n      <td>0.536481</td>\n      <td>0.448718</td>\n      <td>0.427350</td>\n      <td>0.504274</td>\n      <td>0.497854</td>\n      <td>0.437768</td>\n      <td>0.485182</td>\n      <td>0.041496</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.005644</td>\n      <td>0.009215</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>best</td>\n      <td>2</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.472103</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453514</td>\n      <td>0.025147</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.004432</td>\n      <td>0.006140</td>\n      <td>0.001480</td>\n      <td>0.004123</td>\n      <td>random</td>\n      <td>2</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.461538</td>\n      <td>0.525641</td>\n      <td>0.564103</td>\n      <td>0.605150</td>\n      <td>0.472103</td>\n      <td>0.487179</td>\n      <td>0.478632</td>\n      <td>0.508547</td>\n      <td>0.489270</td>\n      <td>0.459227</td>\n      <td>0.431624</td>\n      <td>0.448718</td>\n      <td>0.461538</td>\n      <td>0.519313</td>\n      <td>0.553648</td>\n      <td>0.497749</td>\n      <td>0.046556</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.007297</td>\n      <td>0.011226</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>best</td>\n      <td>3</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453227</td>\n      <td>0.024958</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.002085</td>\n      <td>0.005316</td>\n      <td>0.001041</td>\n      <td>0.003897</td>\n      <td>random</td>\n      <td>3</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.491453</td>\n      <td>0.491453</td>\n      <td>0.448718</td>\n      <td>0.480687</td>\n      <td>0.557940</td>\n      <td>0.551282</td>\n      <td>0.414530</td>\n      <td>0.474359</td>\n      <td>0.407725</td>\n      <td>0.463519</td>\n      <td>0.495726</td>\n      <td>0.504274</td>\n      <td>0.465812</td>\n      <td>0.467811</td>\n      <td>0.407725</td>\n      <td>0.474868</td>\n      <td>0.043379</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.007728</td>\n      <td>0.007554</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>best</td>\n      <td>4</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.442060</td>\n      <td>0.452941</td>\n      <td>0.025060</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.004167</td>\n      <td>0.006911</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>random</td>\n      <td>4</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.440171</td>\n      <td>0.487179</td>\n      <td>0.440171</td>\n      <td>0.502146</td>\n      <td>0.390558</td>\n      <td>0.576923</td>\n      <td>0.482906</td>\n      <td>0.512821</td>\n      <td>0.493562</td>\n      <td>0.467811</td>\n      <td>0.482906</td>\n      <td>0.474359</td>\n      <td>0.585470</td>\n      <td>0.575107</td>\n      <td>0.527897</td>\n      <td>0.495999</td>\n      <td>0.052403</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.016113</td>\n      <td>0.004584</td>\n      <td>0.000135</td>\n      <td>0.000506</td>\n      <td>best</td>\n      <td>1</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.807692</td>\n      <td>0.816239</td>\n      <td>0.846154</td>\n      <td>0.811159</td>\n      <td>0.811159</td>\n      <td>0.846154</td>\n      <td>0.841880</td>\n      <td>0.807692</td>\n      <td>0.785408</td>\n      <td>0.781116</td>\n      <td>0.773504</td>\n      <td>0.829060</td>\n      <td>0.858974</td>\n      <td>0.858369</td>\n      <td>0.798283</td>\n      <td>0.818190</td>\n      <td>0.026691</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.005791</td>\n      <td>0.005648</td>\n      <td>0.000672</td>\n      <td>0.002035</td>\n      <td>random</td>\n      <td>1</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.769231</td>\n      <td>0.867521</td>\n      <td>0.837607</td>\n      <td>0.836910</td>\n      <td>0.832618</td>\n      <td>0.871795</td>\n      <td>0.777778</td>\n      <td>0.799145</td>\n      <td>0.832618</td>\n      <td>0.789700</td>\n      <td>0.794872</td>\n      <td>0.824786</td>\n      <td>0.816239</td>\n      <td>0.781116</td>\n      <td>0.815451</td>\n      <td>0.816492</td>\n      <td>0.030095</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.013616</td>\n      <td>0.004202</td>\n      <td>0.000536</td>\n      <td>0.002007</td>\n      <td>best</td>\n      <td>2</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.764957</td>\n      <td>0.829060</td>\n      <td>0.854701</td>\n      <td>0.793991</td>\n      <td>0.781116</td>\n      <td>0.833333</td>\n      <td>0.829060</td>\n      <td>0.803419</td>\n      <td>0.742489</td>\n      <td>0.768240</td>\n      <td>0.752137</td>\n      <td>0.841880</td>\n      <td>0.829060</td>\n      <td>0.849785</td>\n      <td>0.781116</td>\n      <td>0.803623</td>\n      <td>0.035841</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.005394</td>\n      <td>0.004880</td>\n      <td>0.001213</td>\n      <td>0.002740</td>\n      <td>random</td>\n      <td>2</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.769231</td>\n      <td>0.820513</td>\n      <td>0.833333</td>\n      <td>0.759657</td>\n      <td>0.716738</td>\n      <td>0.794872</td>\n      <td>0.846154</td>\n      <td>0.829060</td>\n      <td>0.768240</td>\n      <td>0.824034</td>\n      <td>0.811966</td>\n      <td>0.841880</td>\n      <td>0.790598</td>\n      <td>0.815451</td>\n      <td>0.785408</td>\n      <td>0.800476</td>\n      <td>0.034699</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.014035</td>\n      <td>0.005706</td>\n      <td>0.000136</td>\n      <td>0.000508</td>\n      <td>best</td>\n      <td>3</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.756410</td>\n      <td>0.824786</td>\n      <td>0.833333</td>\n      <td>0.789700</td>\n      <td>0.776824</td>\n      <td>0.841880</td>\n      <td>0.837607</td>\n      <td>0.803419</td>\n      <td>0.759657</td>\n      <td>0.751073</td>\n      <td>0.769231</td>\n      <td>0.820513</td>\n      <td>0.816239</td>\n      <td>0.832618</td>\n      <td>0.798283</td>\n      <td>0.800772</td>\n      <td>0.030724</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.004322</td>\n      <td>0.004488</td>\n      <td>0.000540</td>\n      <td>0.002019</td>\n      <td>random</td>\n      <td>3</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.816239</td>\n      <td>0.863248</td>\n      <td>0.816239</td>\n      <td>0.789700</td>\n      <td>0.789700</td>\n      <td>0.816239</td>\n      <td>0.811966</td>\n      <td>0.803419</td>\n      <td>0.785408</td>\n      <td>0.824034</td>\n      <td>0.747863</td>\n      <td>0.824786</td>\n      <td>0.837607</td>\n      <td>0.841202</td>\n      <td>0.772532</td>\n      <td>0.809345</td>\n      <td>0.028093</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.016198</td>\n      <td>0.010531</td>\n      <td>0.000538</td>\n      <td>0.002014</td>\n      <td>best</td>\n      <td>4</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.730769</td>\n      <td>0.841880</td>\n      <td>0.829060</td>\n      <td>0.793991</td>\n      <td>0.776824</td>\n      <td>0.846154</td>\n      <td>0.824786</td>\n      <td>0.824786</td>\n      <td>0.776824</td>\n      <td>0.755365</td>\n      <td>0.773504</td>\n      <td>0.807692</td>\n      <td>0.807692</td>\n      <td>0.824034</td>\n      <td>0.785408</td>\n      <td>0.799918</td>\n      <td>0.032108</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.004992</td>\n      <td>0.004744</td>\n      <td>0.000273</td>\n      <td>0.000696</td>\n      <td>random</td>\n      <td>4</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.786325</td>\n      <td>0.824786</td>\n      <td>0.824786</td>\n      <td>0.793991</td>\n      <td>0.721030</td>\n      <td>0.807692</td>\n      <td>0.829060</td>\n      <td>0.850427</td>\n      <td>0.793991</td>\n      <td>0.819742</td>\n      <td>0.782051</td>\n      <td>0.854701</td>\n      <td>0.799145</td>\n      <td>0.802575</td>\n      <td>0.781116</td>\n      <td>0.804761</td>\n      <td>0.031516</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.008103</td>\n      <td>0.004843</td>\n      <td>0.000672</td>\n      <td>0.002037</td>\n      <td>best</td>\n      <td>1</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.002030</td>\n      <td>0.003130</td>\n      <td>0.000137</td>\n      <td>0.000512</td>\n      <td>random</td>\n      <td>1</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.517094</td>\n      <td>0.465812</td>\n      <td>0.521368</td>\n      <td>0.553648</td>\n      <td>0.549356</td>\n      <td>0.525641</td>\n      <td>0.504274</td>\n      <td>0.547009</td>\n      <td>0.510730</td>\n      <td>0.484979</td>\n      <td>0.448718</td>\n      <td>0.521368</td>\n      <td>0.568376</td>\n      <td>0.472103</td>\n      <td>0.626609</td>\n      <td>0.521139</td>\n      <td>0.043455</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.006317</td>\n      <td>0.004219</td>\n      <td>0.001243</td>\n      <td>0.002716</td>\n      <td>best</td>\n      <td>2</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.003376</td>\n      <td>0.003974</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>random</td>\n      <td>2</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.512821</td>\n      <td>0.435897</td>\n      <td>0.504274</td>\n      <td>0.553648</td>\n      <td>0.515021</td>\n      <td>0.602564</td>\n      <td>0.517094</td>\n      <td>0.534188</td>\n      <td>0.497854</td>\n      <td>0.557940</td>\n      <td>0.418803</td>\n      <td>0.474359</td>\n      <td>0.551282</td>\n      <td>0.536481</td>\n      <td>0.540773</td>\n      <td>0.516867</td>\n      <td>0.045712</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.010117</td>\n      <td>0.006049</td>\n      <td>0.000674</td>\n      <td>0.002038</td>\n      <td>best</td>\n      <td>3</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.003101</td>\n      <td>0.003894</td>\n      <td>0.000273</td>\n      <td>0.000696</td>\n      <td>random</td>\n      <td>3</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.534188</td>\n      <td>0.517094</td>\n      <td>0.568376</td>\n      <td>0.515021</td>\n      <td>0.476395</td>\n      <td>0.521368</td>\n      <td>0.555556</td>\n      <td>0.427350</td>\n      <td>0.557940</td>\n      <td>0.549356</td>\n      <td>0.457265</td>\n      <td>0.559829</td>\n      <td>0.581197</td>\n      <td>0.536481</td>\n      <td>0.575107</td>\n      <td>0.528835</td>\n      <td>0.043230</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.012822</td>\n      <td>0.006560</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>best</td>\n      <td>4</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.002291</td>\n      <td>0.003113</td>\n      <td>0.000674</td>\n      <td>0.002044</td>\n      <td>random</td>\n      <td>4</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.581197</td>\n      <td>0.547009</td>\n      <td>0.491453</td>\n      <td>0.510730</td>\n      <td>0.540773</td>\n      <td>0.474359</td>\n      <td>0.478632</td>\n      <td>0.576923</td>\n      <td>0.506438</td>\n      <td>0.472103</td>\n      <td>0.448718</td>\n      <td>0.555556</td>\n      <td>0.619658</td>\n      <td>0.549356</td>\n      <td>0.600858</td>\n      <td>0.530251</td>\n      <td>0.049947</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.018180</td>\n      <td>0.002499</td>\n      <td>0.000266</td>\n      <td>0.000997</td>\n      <td>best</td>\n      <td>1</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.799145</td>\n      <td>0.858974</td>\n      <td>0.824786</td>\n      <td>0.854077</td>\n      <td>0.815451</td>\n      <td>0.829060</td>\n      <td>0.829060</td>\n      <td>0.867521</td>\n      <td>0.759657</td>\n      <td>0.802575</td>\n      <td>0.782051</td>\n      <td>0.901709</td>\n      <td>0.799145</td>\n      <td>0.836910</td>\n      <td>0.811159</td>\n      <td>0.824752</td>\n      <td>0.034740</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.006597</td>\n      <td>0.003651</td>\n      <td>0.000541</td>\n      <td>0.002024</td>\n      <td>random</td>\n      <td>1</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.833333</td>\n      <td>0.850427</td>\n      <td>0.850427</td>\n      <td>0.888412</td>\n      <td>0.824034</td>\n      <td>0.824786</td>\n      <td>0.850427</td>\n      <td>0.807692</td>\n      <td>0.815451</td>\n      <td>0.802575</td>\n      <td>0.807692</td>\n      <td>0.790598</td>\n      <td>0.846154</td>\n      <td>0.811159</td>\n      <td>0.789700</td>\n      <td>0.826191</td>\n      <td>0.026168</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.019157</td>\n      <td>0.002656</td>\n      <td>0.000548</td>\n      <td>0.001397</td>\n      <td>best</td>\n      <td>2</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.773504</td>\n      <td>0.846154</td>\n      <td>0.816239</td>\n      <td>0.858369</td>\n      <td>0.836910</td>\n      <td>0.854701</td>\n      <td>0.811966</td>\n      <td>0.863248</td>\n      <td>0.751073</td>\n      <td>0.824034</td>\n      <td>0.769231</td>\n      <td>0.897436</td>\n      <td>0.799145</td>\n      <td>0.836910</td>\n      <td>0.811159</td>\n      <td>0.823339</td>\n      <td>0.038061</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.007202</td>\n      <td>0.005881</td>\n      <td>0.001067</td>\n      <td>0.001769</td>\n      <td>random</td>\n      <td>2</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.769231</td>\n      <td>0.854701</td>\n      <td>0.833333</td>\n      <td>0.811159</td>\n      <td>0.806867</td>\n      <td>0.854701</td>\n      <td>0.850427</td>\n      <td>0.807692</td>\n      <td>0.746781</td>\n      <td>0.802575</td>\n      <td>0.829060</td>\n      <td>0.833333</td>\n      <td>0.811966</td>\n      <td>0.819742</td>\n      <td>0.785408</td>\n      <td>0.814465</td>\n      <td>0.029685</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.019197</td>\n      <td>0.006233</td>\n      <td>0.000266</td>\n      <td>0.000996</td>\n      <td>best</td>\n      <td>3</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.760684</td>\n      <td>0.858974</td>\n      <td>0.816239</td>\n      <td>0.832618</td>\n      <td>0.819742</td>\n      <td>0.820513</td>\n      <td>0.794872</td>\n      <td>0.841880</td>\n      <td>0.742489</td>\n      <td>0.836910</td>\n      <td>0.760684</td>\n      <td>0.876068</td>\n      <td>0.786325</td>\n      <td>0.845494</td>\n      <td>0.806867</td>\n      <td>0.813357</td>\n      <td>0.037033</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.006934</td>\n      <td>0.006924</td>\n      <td>0.000534</td>\n      <td>0.001361</td>\n      <td>random</td>\n      <td>3</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.764957</td>\n      <td>0.841880</td>\n      <td>0.850427</td>\n      <td>0.845494</td>\n      <td>0.793991</td>\n      <td>0.833333</td>\n      <td>0.782051</td>\n      <td>0.794872</td>\n      <td>0.785408</td>\n      <td>0.841202</td>\n      <td>0.833333</td>\n      <td>0.833333</td>\n      <td>0.782051</td>\n      <td>0.772532</td>\n      <td>0.806867</td>\n      <td>0.810782</td>\n      <td>0.028986</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.015464</td>\n      <td>0.003221</td>\n      <td>0.001067</td>\n      <td>0.001769</td>\n      <td>best</td>\n      <td>4</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.769231</td>\n      <td>0.871795</td>\n      <td>0.816239</td>\n      <td>0.828326</td>\n      <td>0.819742</td>\n      <td>0.841880</td>\n      <td>0.803419</td>\n      <td>0.850427</td>\n      <td>0.733906</td>\n      <td>0.806867</td>\n      <td>0.773504</td>\n      <td>0.893162</td>\n      <td>0.756410</td>\n      <td>0.862661</td>\n      <td>0.785408</td>\n      <td>0.814199</td>\n      <td>0.043725</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.005603</td>\n      <td>0.002444</td>\n      <td>0.000798</td>\n      <td>0.001596</td>\n      <td>random</td>\n      <td>4</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.769231</td>\n      <td>0.820513</td>\n      <td>0.829060</td>\n      <td>0.815451</td>\n      <td>0.832618</td>\n      <td>0.811966</td>\n      <td>0.841880</td>\n      <td>0.820513</td>\n      <td>0.793991</td>\n      <td>0.798283</td>\n      <td>0.803419</td>\n      <td>0.824786</td>\n      <td>0.807692</td>\n      <td>0.793991</td>\n      <td>0.802575</td>\n      <td>0.811065</td>\n      <td>0.017820</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT_model = DecisionTreeClassifier()\n",
    "\n",
    "DT_params = dict()\n",
    "DT_params['max_depth'] = [3, None]\n",
    "DT_params['min_samples_leaf'] = [1, 2, 3, 4]\n",
    "DT_params['criterion'] = [\"gini\", \"entropy\"]\n",
    "DT_params['splitter'] = [\"best\", \"random\"]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "RCV = RandomizedSearchCV(DT_model, DT_params, n_iter=32, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "DT = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(DT.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[2]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les paramètres testés pour ce modèle sont :\n",
    "* le max_depth, correspondant à la profondeur de l'arbre qui doit être illimité pour garantir une précision maximale du modèle.\n",
    "* le min_samples_leaf, correspondant au nombre d'échantillons minimal par feuille qui doit être de 1.\n",
    "* le criterion est une fonction permettant de mesurer la qualité d'un split, nous obtenons des meilleurs résultats avec 'entropy'.\n",
    "* le splitter est la méthode choisie pour selectionner le split à chaque noeud. Modifier ce paramètre ne semble pas impacter significativement les résultats."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification par boosting de gradient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642551df",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "GB_model = GradientBoostingClassifier()\n",
    "GB_params = dict()\n",
    "GB_params['n_estimators'] = [100, 200, 500]\n",
    "GB_params['max_depth'] = [1, 2, 3, 5]\n",
    "GB_params['learning_rate'] = [0.1, 0.2, 0.3]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(GB_model, GB_params, n_iter=36, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "GB = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(GB.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[3]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour améliorer la précision de notre modèle, nous pouvons faire varrier les trois paramètes suivants :\n",
    "* Le nombre d'estimation\n",
    "* La profondeur maximale\n",
    "* Le taux d'apprentissage\n",
    "\n",
    "#### Le nombre d'estimation\n",
    "\n",
    "Ce paramètre permet de jouer sur le nombre d'arbre du modèle.\n",
    "Plus ce paramètre est élevé, plus le modèle est précis.\n",
    "\n",
    "Néanmoins, une valeur trop élevé peut causer des problèmes de performance lors de l'apprentissage.\n",
    "\n",
    "#### La profondeur maximale\n",
    "\n",
    "Cette valeur définit la profondeur des arbres utilisé.\n",
    "\n",
    "Il est important de choisir une profondeur adaptée à notre jeu d'entrainement au risque de faire de l'`over fitting`.\n",
    "\n",
    "#### Le taux d'apprentissage\n",
    "\n",
    "Il permet d'éviter l'`under fitting` et l'`over fitting`.\n",
    "Un taux d'apprentissage trop élevé pourrait amener à de l'`over fitting` et donc un mauvais résultat avec le jeu de test.\n",
    "A l'inverse, un taux d'apprentissage trop faible conduirait à de l'`under fitting`.\n",
    "\n",
    "---\n",
    "\n",
    "> Source : [https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae](https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae)\n",
    "\n",
    "---\n",
    "\n",
    "#### Analyse des résultats\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv_results[[\"param_n_estimators\",\"param_max_depth\",\"param_learning_rate\",\"mean_test_score\",\"rank_test_score\"]].sort_values(\"rank_test_score\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le meilleur résultat semble être avec la configuration suivante :\n",
    "\n",
    "|  Paramètre  | Valeur  |\n",
    "|---|---|\n",
    "| n_estimators |  100 |\n",
    "| max_depth  | 3  |\n",
    "|  learning_rate | 0.1  |\n",
    "\n",
    "> Nous avons essayé d'augmenté *le nombre d'itération* mais cela a un impacte négligeable sur le résultat."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification par machine à vecteurs de support"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "SVC_model = svm.SVC()\n",
    "\n",
    "SVC_params = dict()\n",
    "SVC_params['gamma'] = [0.01, 0.001, 0.0001]\n",
    "SVC_params['C'] = [0.1, 0.5, 1, 2]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(SVC_model, SVC_params, n_iter=50, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "SVC = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(SVC.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[4]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Paramètres\n",
    "\n",
    "* `gamma` : Permet de définir le niveau de précision du modèle. Le paramètre peut prendre une valeur entre 0 et 1. Plus la valeur est proche de 0, plus le model va s'adapter au jeu d'entrainement ce qui risque de provoquer de l'under fitting. Au contraire, une valeur trop proche de 0, risque de provoquer de l'overfitting.\n",
    "\n",
    "* `C` : Permet de définir l'importance des mauvaises classifications. Il permet de définir la marge entre deux classes.\n",
    "\n",
    "#### Analyse des résultats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv_results[[\"param_gamma\",\"param_C\",\"mean_test_score\",\"rank_test_score\"]].sort_values(\"rank_test_score\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La meilleure configuration semble être :\n",
    "\n",
    "| Paramètre  |  Valeur  |\n",
    "|---|---|\n",
    "| Gamma  | 0.001  |\n",
    "|  C | 1  |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Sources :*\n",
    "* [https://www.quora.com/What-is-regularization-parameter-in-SVM](https://www.quora.com/What-is-regularization-parameter-in-SVM)\n",
    "* [https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python](https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python)\n",
    "* [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification par voisin le plus proche"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "KNN_model = KNeighborsClassifier()\n",
    "\n",
    "KNN_params = dict()\n",
    "KNN_params['n_neighbors'] = [i for i in range(1, 20, 5)]\n",
    "KNN_params['weights'] = [\"uniform\", \"distance\"]\n",
    "KNN_params['algorithm'] = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "KNN_params['leaf_size'] = [1, 10, 30]\n",
    "KNN_params['p'] = [1, 2]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(KNN_model, KNN_params, n_iter=50, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "KNN = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(KNN.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[5]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Paramètres\n",
    "\n",
    "* n_neighbors: Permet de définir le nombre de voisins le plus proche. C'est-à-dire, le nombre de points à prendre dans la décision de classification. Ces points étant au plus proche de l'élément à classifier. C'est généralement un nombre pair.\n",
    "\n",
    "* weights: Permet de définir le poids de chaque point proches pris en compte dans la decision.\n",
    "    * uniform : Tous les points ont le même poids\n",
    "    * distance : Le poids de chaque point est déterminé en fonction de sa distance avec l'élément qu'on essaie de classifier.\n",
    "* algorithm : C'est l'algorithme utilisé pour trouver les points les plus proches de l'élément qu'on essaie de classifier.\n",
    "* leaf_size : Permet de définir la taille des feuilles utilisée dans les algorithmes BallTree et KDTree.\n",
    "\n",
    "#### Analyse des résultats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cv_results[[\"param_n_neighbors\",\"param_weights\",\"param_algorithm\",\"param_leaf_size\",\"mean_test_score\",\"rank_test_score\"]].sort_values(\"rank_test_score\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Il semble difficile de déterminer quelle est la meilleure configuration au vu des résultats mais on peut en extraire une certaine tendance.\n",
    "\n",
    "Le paramètre 'weights' n'a pas beaucoup d'importance.\n",
    "\n",
    "La différence se fait sur les paramètres algotithm et n_neighbors avec comme valeur pour le premier `kd_tree`/`ball_tree`/`auto` et pour le second 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sources :\n",
    "* [https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "* [https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn](https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn)\n",
    "* [https://www.datasklr.com/select-classification-methods/k-nearest-neighbors](https://www.datasklr.com/select-classification-methods/k-nearest-neighbors)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparaison des résultats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Evaluation_Results)\n",
    "df.plot(kind = 'bar')\n",
    "plt.show()\n",
    "\n",
    "Evaluation_Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Au vu des résultats, le modèle ayant la meilleure exactitude semble être la classification machine à vecteurs de support avec pour paramètre 'gamma' = 0.001 et 'C'= 1.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entrainement et test d'un prédicteur\n",
    "\n",
    "Maintenant que l'on connait le modèle le plus efficace, nous allons créer un predicteur afin de répondre à notre problematique."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SVC_definitive_model = svm.SVC(gamma=0.001, C=1)\n",
    "\n",
    "SVC_fitted = SVC_definitive_model.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Matrice de confusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predicted = SVC_fitted.predict(x_test)\n",
    "disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predicted, cmap=plt.cm.gray_r)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La matrice de confusion nous permet de constater que les rares erreurs sont souvent sur les labels 2, 5.\n",
    "Une solution envisageable serait d'ajouter plus de données pour les labels 2, 5 pour entrainer le modèle."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test du prédicteur avec des données externes\n",
    "\n",
    "Essayons à présent de tester notre prédicteur sur des images que nous avons créées grâce au logiciel paint.\n",
    "Les images créées pour le test sont assez différentes des images d'entrainements. En effet, les images d'entrainements ressemblent à des images créées avec une résolution élevée puis \"downscalées\" contrairement à nos images de test qui ont été réalisées directement en 8x8 pixel. Cette différence est visible au niveau de la plage de niveau de gris qui est plus large sur les images d'entrainement que sur les images de test ou un pixel est blanc ou noir uniquement."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image, ImageChops\n",
    "\n",
    "# Import de nos images et inversion des niveaux de gris\n",
    "image_test_1 = np.array(ImageChops.invert(Image.open('test_images/1.bmp'))) - 240\n",
    "image_test_2 = np.array(ImageChops.invert(Image.open('test_images/2.bmp'))) - 240\n",
    "image_test_3 = np.array(ImageChops.invert(Image.open('test_images/3.bmp'))) - 240\n",
    "image_test_4 = np.array(ImageChops.invert(Image.open('test_images/4.bmp'))) - 240\n",
    "image_test_5 = np.array(ImageChops.invert(Image.open('test_images/5.bmp'))) - 240\n",
    "image_test_6 = np.array(ImageChops.invert(Image.open('test_images/6.bmp'))) - 240\n",
    "image_test_7 = np.array(ImageChops.invert(Image.open('test_images/7.bmp'))) - 240\n",
    "image_test_8 = np.array(ImageChops.invert(Image.open('test_images/8.bmp'))) - 240\n",
    "image_test_9 = np.array(ImageChops.invert(Image.open('test_images/9.bmp'))) - 240\n",
    "\n",
    "\n",
    "images_test = np.array([image_test_1, image_test_2, image_test_3, image_test_4, image_test_5, image_test_6, image_test_7, image_test_8, image_test_9])\n",
    "\n",
    "# Création de 10 figures de 10 par 3 pixels\n",
    "_, axes = plt.subplots(nrows=1, ncols=9, figsize=(10, 3))\n",
    "\n",
    "# prédiction du prédicteur\n",
    "y_pred = SVC_fitted.predict(images_test.reshape(len(images_test), -1))\n",
    "\n",
    "# Pour chaque figure, on affiche l'image du chiffre et son label en titre\n",
    "for ax, image, label in zip(axes, images_test, y_pred):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "    ax.set_title(\"Pred: %i\" % label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Malgré les différences entre le jeu de test créé et le jeu d'entrainement, le prédicteur semble capable de reconnaitre les chiffres présents dans les images la plupart du temps. Cependant, après avoir modifié les images de test et réessayer plusieurs fois le test, on remarque que notre prédicteur à du mal à prédire les chiffres 3 et 6."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "Bien que notre prédicteur puisse atteindre 99% de véridicité sur le dataset initial, une fois confronté à des données rédigées à la main sur le logiciel paint sans anticrénelage, ce dernier semble perdre en efficacité. Cette perte s'explique par la spécificité du dataset initial. Afin de palier cette perte, nous pourrions compléter le dataset en en ajoutant une version dupliquée dans laquelle nous supprimons l'antialiasing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}