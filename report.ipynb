{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0b8e1d",
   "metadata": {},
   "source": [
    "# Rapport du projet de résolution de problème\n",
    "\n",
    "- Paul Achard\n",
    "- Julien Faure\n",
    "\n",
    "    \n",
    "- *Date : 20/01/2022*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1070770",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42622bf2",
   "metadata": {},
   "source": [
    "## Problématique\n",
    "\n",
    "Notre problématique est d'identifier un chiffre à partir d'une image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3a0c6",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "- Trouver un modèle permettant d'identifier un chiffre à partir d'une image\n",
    "- Comparer différentes stratégies de solveur pour le modèle trouvé\n",
    "\n",
    "## Analyse du dataset\n",
    "\n",
    "Identifier un chiffre à partir d'une image est une tache qui peut s'avérer très complexe. Afin d'avoir une difficulté raisonnable et adapté au contexte de ce projet, nous avons fixé certains paramètres dans notre dataset.\n",
    "\n",
    "- La résolution de nos images est identiques pour tout le dataset. Cette résolution est **8 pixels par 8 pixels**.\n",
    "- Chaque pixel est codé sur **4 bits**.\n",
    "- Les images contiennent uniquement un chiffre sans élément parasite, sans effet et sans traitement.\n",
    "\n",
    "Nous utilisons le dataset `digits` de `seaborn`.\n",
    "\n",
    "### Forme du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "894968c2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data Shape (1797, 64)\n",
      "Label Data Shape (1797,)\n"
     ]
    }
   ],
   "source": [
    "# Import du dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Affiche le nombre d'images et leur format\n",
    "print(\"Image Data Shape\" , digits.data.shape)\n",
    "\n",
    "# Affiche le nombre de labels\n",
    "print(\"Label Data Shape\", digits.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311b24e7",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir ci-dessus, le dataset est composé de **1797** images labellisées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb1ba9",
   "metadata": {},
   "source": [
    "### Répartition des images du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1826bdc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Text(0.5, 0, 'n° labellisé'), Text(0, 0.5, 'Occurrence')]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphique = sns.countplot(x=digits.target)\n",
    "graphique.set(xlabel=\"n° labellisé\", ylabel = \"Occurrence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee617b1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Les données labellisées sont équitablement distribuées (environ 175 par label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc387e8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Représentation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17d087b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x216 with 10 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Création de 10 figures de 10 par 3 pixels\n",
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 3))\n",
    "\n",
    "# Pour chaque figure, on affiche l'image du chiffre et son label en titre\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "    ax.set_title(\"Label: %i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e6cc1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Représentation des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb283e09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "\n",
      " Type de chaque valeur : <class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "# Affiche le tableau représentant la première image\n",
    "print(digits.images[0])\n",
    "print(\"\\n Type de chaque valeur :\", type(digits.images[0][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa72f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Comme nous l'avons vu précédemment, chaque image possède une résolution de 8x8 pixels en niveaux de gris codés sur 4bits par pixel.\n",
    "\n",
    "Dans notre programme, une image est représentée par une matrice de dimension 8x8. Chaque élément représente un pixel avec un niveau de gris codé sur 4bits (de 0 à 15). Plus la valeur est élevée, plus la couleur est foncée.\n",
    "\n",
    "> Exemple :\n",
    "* 0 : Blanc\n",
    "* 15 : Noir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3675a",
   "metadata": {},
   "source": [
    "### Mise en forme des données d'entrées\n",
    "\n",
    "Pour commencer, nous devons redimenssionner nos données d'entrées.\n",
    "\n",
    "Actuellement, nous avons des données sous la forme d'un tableau de matrice.\n",
    "\n",
    "Il nous faut les mettre sous forme d'une matrice où chaque vecteur correspond aux pixels de l'image.\n",
    "\n",
    "Donc si nous avons 1797 images, la matrice d'entrée est composée de 1797 vecteurs.\n",
    "\n",
    "Avec une résolution de 8x8, la taille d'un vecteur est de 64 valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74b2574d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Redimensionne la matrice en vecteur\n",
    "print(digits.images[0].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e10f7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redimenssionne le tableau de matrice en matrice\n",
    "x = digits.images.reshape((len(digits.images), -1))\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test de différents modèles\n",
    "\n",
    "Pour résoudre notre problème, nous allons essayer les modèles suivants :\n",
    "* [Modèle de regréssion logistique multiclasse](#Mod%C3%A8le-de-regr%C3%A9ssion-logistique-multiclasse)\n",
    "* [Modèle de Bayes](#Mod%C3%A8le-de-Bayes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "c9331fa9",
   "metadata": {},
   "source": [
    "### Jeu de test et jeu d'entraînement\n",
    "\n",
    "Il nous faut maintenant séparer notre jeu de test et notre jeu d'entraînement du dataset.\n",
    "\n",
    "Pour choisir la taille de notre jeu de test, il est nécessaire de faire attention à plusieurs points :\n",
    "* Le temps d'entrainement de notre modèle\n",
    "* La taille de notre dataset\n",
    "\n",
    "Plus la proportion du jeu d'entraînement est faible, plus notre modèle à un niveau de variance élevé. Ainsi, on augmente les chances d'avoir de l'*over fitting*.\n",
    "\n",
    "A contrario, plus la proportion du jeu de test est faible, plus notre modèle à un niveau de variance faible. Ainsi, on augmente les chances d'avoir de l'*under fitting*.\n",
    "\n",
    "Le but est donc de trouver la valeur qui nous permet d'avoir le taux de variance optimal.\n",
    "\n",
    "\n",
    "Pour trouver cette valeur, nous avons appliqué la procédure suivante, nous avons essayé avec plusieurs valeurs en partant de 20% jusqu'à 80% avec un pas de 5%. Nous avons déterminé que la meilleure valeur est **35%**.\n",
    "\n",
    "\n",
    "> [https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio](https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de94e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# séparation du dataset en données \"d'apprentissage\" et de test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.35, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Création du tableau de résultat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "                               Best Mean Test Score\nLogistic Regression (LR)                        0.0\nNaïve Bayes Classifier (NB)                     0.0\nDecision Tree Classifier (DT)                   0.0\nGradient Boosting (GB)                          0.0\nSupport Vector Machines (SVM)                   0.0\nK Nearest Neighbours (KNN)                      0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Best Mean Test Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression (LR)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Naïve Bayes Classifier (NB)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Decision Tree Classifier (DT)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting (GB)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Vector Machines (SVM)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>K Nearest Neighbours (KNN)</th>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluation_Results = pd.DataFrame(np.zeros((6, 1)), columns=['Best Mean Test Score'])\n",
    "Evaluation_Results.index = ['Logistic Regression (LR)', 'Naïve Bayes Classifier (NB)',\n",
    "                            'Decision Tree Classifier (DT)', 'Gradient Boosting (GB)',\n",
    "                            'Support Vector Machines (SVM)', 'K Nearest Neighbours (KNN)']\n",
    "Evaluation_Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "334ad595",
   "metadata": {},
   "source": [
    "### Modèle de regréssion logistique multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc863948",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 18 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.97231332 0.97231332 0.96489613 0.96659819 0.95546507        nan\n",
      " 0.96917575 0.96946187 0.95748138 0.96659819 0.95546507        nan\n",
      " 0.96917697 0.96860595 0.95633934 0.96659819 0.95546507        nan]\n",
      "  warnings.warn(\n",
      "e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [1.         1.         0.99600438 1.         1.                nan\n",
      " 1.         1.         0.99978594 1.         1.                nan\n",
      " 1.         1.         1.         1.         1.                nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n0        0.491747      0.055859         0.000200        0.000400     0.1   \n1        1.546238      0.573845         0.000267        0.000443     0.1   \n2        0.076536      0.020450         0.000734        0.000680     0.1   \n3        0.137124      0.026492         0.000400        0.000490     0.1   \n4        0.045975      0.011279         0.000334        0.000472     0.1   \n5        0.000267      0.000443         0.000000        0.000000     0.1   \n6        0.648922      0.081932         0.000200        0.000400       5   \n7        1.962848      0.965315         0.000467        0.000499       5   \n8        0.102359      0.004061         0.000400        0.000490       5   \n9        0.119709      0.004337         0.000400        0.000490       5   \n10       0.045975      0.007450         0.000534        0.000499       5   \n11       0.000334      0.000472         0.000000        0.000000       5   \n12       0.674914      0.060597         0.000467        0.000499      10   \n13       1.923798      0.749193         0.000200        0.000401      10   \n14       0.109566      0.003225         0.000467        0.000499      10   \n15       0.127483      0.013460         0.000400        0.000490      10   \n16       0.041171      0.001545         0.000334        0.000472      10   \n17       0.000534      0.000499         0.000000        0.000000      10   \n\n   param_penalty param_solver  \\\n0             l2    newton-cg   \n1             l2        lbfgs   \n2             l2    liblinear   \n3           none    newton-cg   \n4           none        lbfgs   \n5           none    liblinear   \n6             l2    newton-cg   \n7             l2        lbfgs   \n8             l2    liblinear   \n9           none    newton-cg   \n10          none        lbfgs   \n11          none    liblinear   \n12            l2    newton-cg   \n13            l2        lbfgs   \n14            l2    liblinear   \n15          none    newton-cg   \n16          none        lbfgs   \n17          none    liblinear   \n\n                                               params  split0_test_score  \\\n0   {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-...           0.978632   \n1      {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}           0.978632   \n2   {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...           0.978632   \n3   {'C': 0.1, 'penalty': 'none', 'solver': 'newto...           0.970085   \n4    {'C': 0.1, 'penalty': 'none', 'solver': 'lbfgs'}           0.957265   \n5   {'C': 0.1, 'penalty': 'none', 'solver': 'libli...                NaN   \n6    {'C': 5, 'penalty': 'l2', 'solver': 'newton-cg'}           0.978632   \n7        {'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}           0.978632   \n8    {'C': 5, 'penalty': 'l2', 'solver': 'liblinear'}           0.970085   \n9   {'C': 5, 'penalty': 'none', 'solver': 'newton-...           0.970085   \n10     {'C': 5, 'penalty': 'none', 'solver': 'lbfgs'}           0.957265   \n11  {'C': 5, 'penalty': 'none', 'solver': 'libline...                NaN   \n12  {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}           0.974359   \n13      {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}           0.974359   \n14  {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}           0.970085   \n15  {'C': 10, 'penalty': 'none', 'solver': 'newton...           0.970085   \n16    {'C': 10, 'penalty': 'none', 'solver': 'lbfgs'}           0.957265   \n17  {'C': 10, 'penalty': 'none', 'solver': 'liblin...                NaN   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.965812           0.974359           0.982833   \n1            0.965812           0.974359           0.982833   \n2            0.965812           0.961538           0.969957   \n3            0.957265           0.965812           0.974249   \n4            0.948718           0.965812           0.944206   \n5                 NaN                NaN                NaN   \n6            0.970085           0.965812           0.974249   \n7            0.970085           0.965812           0.974249   \n8            0.965812           0.952991           0.965665   \n9            0.957265           0.965812           0.974249   \n10           0.948718           0.965812           0.944206   \n11                NaN                NaN                NaN   \n12           0.965812           0.965812           0.978541   \n13           0.965812           0.965812           0.978541   \n14           0.965812           0.944444           0.969957   \n15           0.957265           0.965812           0.974249   \n16           0.948718           0.965812           0.944206   \n17                NaN                NaN                NaN   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.969957           0.974359           0.987179   \n1            0.969957           0.974359           0.987179   \n2            0.952790           0.957265           0.982906   \n3            0.961373           0.978632           0.982906   \n4            0.948498           0.970085           0.970085   \n5                 NaN                NaN                NaN   \n6            0.965665           0.974359           0.978632   \n7            0.969957           0.974359           0.978632   \n8            0.952790           0.957265           0.978632   \n9            0.961373           0.978632           0.982906   \n10           0.948498           0.970085           0.970085   \n11                NaN                NaN                NaN   \n12           0.965665           0.978632           0.978632   \n13           0.965665           0.974359           0.978632   \n14           0.952790           0.957265           0.974359   \n15           0.961373           0.978632           0.982906   \n16           0.948498           0.970085           0.970085   \n17                NaN                NaN                NaN   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.970085           0.957082           0.982833   \n1            0.970085           0.957082           0.982833   \n2            0.952991           0.948498           0.982833   \n3            0.974359           0.957082           0.974249   \n4            0.965812           0.922747           0.969957   \n5                 NaN                NaN                NaN   \n6            0.965812           0.965665           0.987124   \n7            0.965812           0.965665           0.987124   \n8            0.927350           0.948498           0.969957   \n9            0.974359           0.957082           0.974249   \n10           0.965812           0.922747           0.969957   \n11                NaN                NaN                NaN   \n12           0.965812           0.965665           0.987124   \n13           0.965812           0.965665           0.987124   \n14           0.935897           0.944206           0.965665   \n15           0.974359           0.957082           0.974249   \n16           0.965812           0.922747           0.969957   \n17                NaN                NaN                NaN   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.970085            0.961538            0.982906   \n1             0.970085            0.961538            0.982906   \n2             0.952991            0.961538            0.974359   \n3             0.965812            0.965812            0.978632   \n4             0.948718            0.957265            0.965812   \n5                  NaN                 NaN                 NaN   \n6             0.957265            0.957265            0.982906   \n7             0.957265            0.957265            0.982906   \n8             0.935897            0.940171            0.974359   \n9             0.965812            0.965812            0.978632   \n10            0.948718            0.957265            0.965812   \n11                 NaN                 NaN                 NaN   \n12            0.957265            0.957265            0.982906   \n13            0.957265            0.957265            0.982906   \n14            0.931624            0.940171            0.974359   \n15            0.965812            0.965812            0.978632   \n16            0.948718            0.957265            0.965812   \n17                 NaN                 NaN                 NaN   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.969957            0.957082         0.972313        0.009115   \n1             0.969957            0.957082         0.972313        0.009115   \n2             0.961373            0.969957         0.964896        0.010858   \n3             0.957082            0.935622         0.966598        0.011553   \n4             0.957082            0.939914         0.955465        0.013029   \n5                  NaN                 NaN              NaN             NaN   \n6             0.965665            0.948498         0.969176        0.010046   \n7             0.965665            0.948498         0.969462        0.010003   \n8             0.965665            0.957082         0.957481        0.014251   \n9             0.957082            0.935622         0.966598        0.011553   \n10            0.957082            0.939914         0.955465        0.013029   \n11                 NaN                 NaN              NaN             NaN   \n12            0.965665            0.948498         0.969177        0.010284   \n13            0.965665            0.944206         0.968606        0.010692   \n14            0.965665            0.952790         0.956339        0.013886   \n15            0.957082            0.935622         0.966598        0.011553   \n16            0.957082            0.939914         0.955465        0.013029   \n17                 NaN                 NaN              NaN             NaN   \n\n    rank_test_score  split0_train_score  split1_train_score  \\\n0                 1            1.000000            1.000000   \n1                 1            1.000000            1.000000   \n2                10            0.994647            0.998929   \n3                 7            1.000000            1.000000   \n4                13            1.000000            1.000000   \n5                17                 NaN                 NaN   \n6                 5            1.000000            1.000000   \n7                 3            1.000000            1.000000   \n8                11            0.998929            1.000000   \n9                 7            1.000000            1.000000   \n10               13            1.000000            1.000000   \n11               16                 NaN                 NaN   \n12                4            1.000000            1.000000   \n13                6            1.000000            1.000000   \n14               12            1.000000            1.000000   \n15                7            1.000000            1.000000   \n16               13            1.000000            1.000000   \n17               18                 NaN                 NaN   \n\n    split2_train_score  split3_train_score  split4_train_score  \\\n0             1.000000            1.000000            1.000000   \n1             1.000000            1.000000            1.000000   \n2             0.994647            0.995722            0.996791   \n3             1.000000            1.000000            1.000000   \n4             1.000000            1.000000            1.000000   \n5                  NaN                 NaN                 NaN   \n6             1.000000            1.000000            1.000000   \n7             1.000000            1.000000            1.000000   \n8             1.000000            1.000000            1.000000   \n9             1.000000            1.000000            1.000000   \n10            1.000000            1.000000            1.000000   \n11                 NaN                 NaN                 NaN   \n12            1.000000            1.000000            1.000000   \n13            1.000000            1.000000            1.000000   \n14            1.000000            1.000000            1.000000   \n15            1.000000            1.000000            1.000000   \n16            1.000000            1.000000            1.000000   \n17                 NaN                 NaN                 NaN   \n\n    split5_train_score  split6_train_score  split7_train_score  \\\n0             1.000000            1.000000            1.000000   \n1             1.000000            1.000000            1.000000   \n2             0.992505            0.996788            0.995717   \n3             1.000000            1.000000            1.000000   \n4             1.000000            1.000000            1.000000   \n5                  NaN                 NaN                 NaN   \n6             1.000000            1.000000            1.000000   \n7             1.000000            1.000000            1.000000   \n8             1.000000            0.998929            1.000000   \n9             1.000000            1.000000            1.000000   \n10            1.000000            1.000000            1.000000   \n11                 NaN                 NaN                 NaN   \n12            1.000000            1.000000            1.000000   \n13            1.000000            1.000000            1.000000   \n14            1.000000            1.000000            1.000000   \n15            1.000000            1.000000            1.000000   \n16            1.000000            1.000000            1.000000   \n17                 NaN                 NaN                 NaN   \n\n    split8_train_score  split9_train_score  split10_train_score  \\\n0             1.000000            1.000000             1.000000   \n1             1.000000            1.000000             1.000000   \n2             0.995722            0.996791             0.995717   \n3             1.000000            1.000000             1.000000   \n4             1.000000            1.000000             1.000000   \n5                  NaN                 NaN                  NaN   \n6             1.000000            1.000000             1.000000   \n7             1.000000            1.000000             1.000000   \n8             1.000000            1.000000             1.000000   \n9             1.000000            1.000000             1.000000   \n10            1.000000            1.000000             1.000000   \n11                 NaN                 NaN                  NaN   \n12            1.000000            1.000000             1.000000   \n13            1.000000            1.000000             1.000000   \n14            1.000000            1.000000             1.000000   \n15            1.000000            1.000000             1.000000   \n16            1.000000            1.000000             1.000000   \n17                 NaN                 NaN                  NaN   \n\n    split11_train_score  split12_train_score  split13_train_score  \\\n0              1.000000             1.000000             1.000000   \n1              1.000000             1.000000             1.000000   \n2              0.996788             0.995717             0.996791   \n3              1.000000             1.000000             1.000000   \n4              1.000000             1.000000             1.000000   \n5                   NaN                  NaN                  NaN   \n6              1.000000             1.000000             1.000000   \n7              1.000000             1.000000             1.000000   \n8              1.000000             1.000000             1.000000   \n9              1.000000             1.000000             1.000000   \n10             1.000000             1.000000             1.000000   \n11                  NaN                  NaN                  NaN   \n12             1.000000             1.000000             1.000000   \n13             1.000000             1.000000             1.000000   \n14             1.000000             1.000000             1.000000   \n15             1.000000             1.000000             1.000000   \n16             1.000000             1.000000             1.000000   \n17                  NaN                  NaN                  NaN   \n\n    split14_train_score  mean_train_score  std_train_score  \n0              1.000000          1.000000         0.000000  \n1              1.000000          1.000000         0.000000  \n2              0.996791          0.996004         0.001381  \n3              1.000000          1.000000         0.000000  \n4              1.000000          1.000000         0.000000  \n5                   NaN               NaN              NaN  \n6              1.000000          1.000000         0.000000  \n7              1.000000          1.000000         0.000000  \n8              0.998930          0.999786         0.000428  \n9              1.000000          1.000000         0.000000  \n10             1.000000          1.000000         0.000000  \n11                  NaN               NaN              NaN  \n12             1.000000          1.000000         0.000000  \n13             1.000000          1.000000         0.000000  \n14             1.000000          1.000000         0.000000  \n15             1.000000          1.000000         0.000000  \n16             1.000000          1.000000         0.000000  \n17                  NaN               NaN              NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_C</th>\n      <th>param_penalty</th>\n      <th>param_solver</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n      <th>split0_train_score</th>\n      <th>split1_train_score</th>\n      <th>split2_train_score</th>\n      <th>split3_train_score</th>\n      <th>split4_train_score</th>\n      <th>split5_train_score</th>\n      <th>split6_train_score</th>\n      <th>split7_train_score</th>\n      <th>split8_train_score</th>\n      <th>split9_train_score</th>\n      <th>split10_train_score</th>\n      <th>split11_train_score</th>\n      <th>split12_train_score</th>\n      <th>split13_train_score</th>\n      <th>split14_train_score</th>\n      <th>mean_train_score</th>\n      <th>std_train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.491747</td>\n      <td>0.055859</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'newton-...</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.957082</td>\n      <td>0.982833</td>\n      <td>0.970085</td>\n      <td>0.961538</td>\n      <td>0.982906</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.972313</td>\n      <td>0.009115</td>\n      <td>1</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.546238</td>\n      <td>0.573845</td>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.957082</td>\n      <td>0.982833</td>\n      <td>0.970085</td>\n      <td>0.961538</td>\n      <td>0.982906</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.972313</td>\n      <td>0.009115</td>\n      <td>1</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.076536</td>\n      <td>0.020450</td>\n      <td>0.000734</td>\n      <td>0.000680</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'libline...</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.952991</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.964896</td>\n      <td>0.010858</td>\n      <td>10</td>\n      <td>0.994647</td>\n      <td>0.998929</td>\n      <td>0.994647</td>\n      <td>0.995722</td>\n      <td>0.996791</td>\n      <td>0.992505</td>\n      <td>0.996788</td>\n      <td>0.995717</td>\n      <td>0.995722</td>\n      <td>0.996791</td>\n      <td>0.995717</td>\n      <td>0.996788</td>\n      <td>0.995717</td>\n      <td>0.996791</td>\n      <td>0.996791</td>\n      <td>0.996004</td>\n      <td>0.001381</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.137124</td>\n      <td>0.026492</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'newto...</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.978632</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.957082</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.966598</td>\n      <td>0.011553</td>\n      <td>7</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.045975</td>\n      <td>0.011279</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.957265</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.948498</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.922747</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.939914</td>\n      <td>0.955465</td>\n      <td>0.013029</td>\n      <td>13</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'libli...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>17</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.648922</td>\n      <td>0.081932</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.969176</td>\n      <td>0.010046</td>\n      <td>5</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.962848</td>\n      <td>0.965315</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.969462</td>\n      <td>0.010003</td>\n      <td>3</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.102359</td>\n      <td>0.004061</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'liblinear'}</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.978632</td>\n      <td>0.927350</td>\n      <td>0.948498</td>\n      <td>0.969957</td>\n      <td>0.935897</td>\n      <td>0.940171</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.957481</td>\n      <td>0.014251</td>\n      <td>11</td>\n      <td>0.998929</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.998929</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.998930</td>\n      <td>0.999786</td>\n      <td>0.000428</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.119709</td>\n      <td>0.004337</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>5</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'newton-...</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.978632</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.957082</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.966598</td>\n      <td>0.011553</td>\n      <td>7</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.045975</td>\n      <td>0.007450</td>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>5</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.957265</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.948498</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.922747</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.939914</td>\n      <td>0.955465</td>\n      <td>0.013029</td>\n      <td>13</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'libline...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>16</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.674914</td>\n      <td>0.060597</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.965665</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.969177</td>\n      <td>0.010284</td>\n      <td>4</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.923798</td>\n      <td>0.749193</td>\n      <td>0.000200</td>\n      <td>0.000401</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.944206</td>\n      <td>0.968606</td>\n      <td>0.010692</td>\n      <td>6</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.109566</td>\n      <td>0.003225</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.944444</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.974359</td>\n      <td>0.935897</td>\n      <td>0.944206</td>\n      <td>0.965665</td>\n      <td>0.931624</td>\n      <td>0.940171</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.956339</td>\n      <td>0.013886</td>\n      <td>12</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.127483</td>\n      <td>0.013460</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>10</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'newton...</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.978632</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.957082</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.966598</td>\n      <td>0.011553</td>\n      <td>7</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.041171</td>\n      <td>0.001545</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>10</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.957265</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.948498</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.922747</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.939914</td>\n      <td>0.955465</td>\n      <td>0.013029</td>\n      <td>13</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>10</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'liblin...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création du modèle de régression\n",
    "LR_model = LogisticRegression(verbose=False, max_iter=10000)\n",
    "\n",
    "# Dictionnaire contenant les différents paramètres à essayer\n",
    "LR_params = dict()\n",
    "LR_params['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "LR_params['penalty'] = ['l2', 'none']\n",
    "LR_params['C'] = [0.1, 5, 10]\n",
    "\n",
    "# Création de nos itérations\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "# Création de notre modèle de validation croisée\n",
    "model_cv = GridSearchCV(estimator=LR_model,\n",
    "                        param_grid=LR_params,\n",
    "                        scoring='accuracy',\n",
    "                        cv=kFold,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True)\n",
    "\n",
    "# Entrainement du modèle\n",
    "model_cv.fit(x_train, y_train)\n",
    "# cv results\n",
    "pd.set_option('display.max_columns', None)\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[0]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31144b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "La validation croisée nous permet de determiner les paramètres optimaux pour notre modèle d'apprentissage.\n",
    "Les résultats de nos tests avec de nombreux paramètres différents (dont la plupart ne sont pas présent dans l'exemple ci-dessus).\n",
    "* Le **paramètre C** correspond à l'inverse de la force de régularisation des données. Plus ce paramètre est grand, plus le risque d'overfitting est grand. Une valeur de 0.1 semble être optimale.\n",
    "* Le **paramètre de pénalité** permet de réduire les coefficients θ. On remarque une perte de précision si l'on n'applique pas de pénalité. La meilleure pénalité semble être la norme L2.\n",
    "* Le **solveur** correspond à l'algorithme d'optimisation utilisé pour l'entrainement. Dans notre cas, lbfgs permet d'obtenir la precision la plus élevée malgré un temps d'entrainement significativement plus long que ses concurrents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b3aae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Classification naïve bayésienne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9833fcac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n0       0.004337  3.282590e-03         0.000668        0.000597           0   \n1       0.001868  4.992858e-04         0.000600        0.000490      0.0001   \n2       0.001721  4.553784e-04         0.000595        0.000486       0.001   \n3       0.001866  5.470337e-04         0.000467        0.000499        0.01   \n4       0.002001  5.167947e-04         0.000467        0.000499         0.1   \n5       0.002002  9.905775e-07         0.000334        0.000472         0.5   \n6       0.001935  5.738116e-04         0.000334        0.000472         1.0   \n\n              params  split0_test_score  split1_test_score  split2_test_score  \\\n0       {'alpha': 0}           0.837607           0.871795           0.854701   \n1  {'alpha': 0.0001}           0.841880           0.884615           0.854701   \n2   {'alpha': 0.001}           0.846154           0.884615           0.854701   \n3    {'alpha': 0.01}           0.850427           0.884615           0.854701   \n4     {'alpha': 0.1}           0.854701           0.880342           0.854701   \n5     {'alpha': 0.5}           0.850427           0.876068           0.850427   \n6     {'alpha': 1.0}           0.850427           0.871795           0.850427   \n\n   split3_test_score  split4_test_score  split5_test_score  split6_test_score  \\\n0           0.836910           0.845494           0.871795           0.850427   \n1           0.849785           0.854077           0.876068           0.880342   \n2           0.849785           0.858369           0.876068           0.884615   \n3           0.849785           0.858369           0.867521           0.876068   \n4           0.849785           0.858369           0.863248           0.871795   \n5           0.845494           0.845494           0.858974           0.867521   \n6           0.841202           0.845494           0.858974           0.867521   \n\n   split7_test_score  split8_test_score  split9_test_score  \\\n0           0.854701           0.819742           0.875536   \n1           0.863248           0.828326           0.879828   \n2           0.867521           0.836910           0.875536   \n3           0.867521           0.841202           0.875536   \n4           0.867521           0.841202           0.884120   \n5           0.867521           0.836910           0.884120   \n6           0.867521           0.832618           0.879828   \n\n   split10_test_score  split11_test_score  split12_test_score  \\\n0            0.833333            0.854701            0.858974   \n1            0.854701            0.858974            0.867521   \n2            0.854701            0.858974            0.867521   \n3            0.858974            0.858974            0.863248   \n4            0.867521            0.858974            0.854701   \n5            0.871795            0.850427            0.854701   \n6            0.871795            0.841880            0.854701   \n\n   split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0            0.866953            0.871245         0.853594        0.015985   \n1            0.871245            0.884120         0.863296        0.015941   \n2            0.871245            0.884120         0.864722        0.014346   \n3            0.871245            0.884120         0.864154        0.012277   \n4            0.871245            0.888412         0.864442        0.012759   \n5            0.854077            0.875536         0.859300        0.013215   \n6            0.854077            0.875536         0.857587        0.013731   \n\n   rank_test_score  \n0                7  \n1                4  \n2                1  \n3                3  \n4                2  \n5                5  \n6                6  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_alpha</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.004337</td>\n      <td>3.282590e-03</td>\n      <td>0.000668</td>\n      <td>0.000597</td>\n      <td>0</td>\n      <td>{'alpha': 0}</td>\n      <td>0.837607</td>\n      <td>0.871795</td>\n      <td>0.854701</td>\n      <td>0.836910</td>\n      <td>0.845494</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.854701</td>\n      <td>0.819742</td>\n      <td>0.875536</td>\n      <td>0.833333</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.866953</td>\n      <td>0.871245</td>\n      <td>0.853594</td>\n      <td>0.015985</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001868</td>\n      <td>4.992858e-04</td>\n      <td>0.000600</td>\n      <td>0.000490</td>\n      <td>0.0001</td>\n      <td>{'alpha': 0.0001}</td>\n      <td>0.841880</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.854077</td>\n      <td>0.876068</td>\n      <td>0.880342</td>\n      <td>0.863248</td>\n      <td>0.828326</td>\n      <td>0.879828</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.863296</td>\n      <td>0.015941</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.001721</td>\n      <td>4.553784e-04</td>\n      <td>0.000595</td>\n      <td>0.000486</td>\n      <td>0.001</td>\n      <td>{'alpha': 0.001}</td>\n      <td>0.846154</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.876068</td>\n      <td>0.884615</td>\n      <td>0.867521</td>\n      <td>0.836910</td>\n      <td>0.875536</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.864722</td>\n      <td>0.014346</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.001866</td>\n      <td>5.470337e-04</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>0.01</td>\n      <td>{'alpha': 0.01}</td>\n      <td>0.850427</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.867521</td>\n      <td>0.876068</td>\n      <td>0.867521</td>\n      <td>0.841202</td>\n      <td>0.875536</td>\n      <td>0.858974</td>\n      <td>0.858974</td>\n      <td>0.863248</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.864154</td>\n      <td>0.012277</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.002001</td>\n      <td>5.167947e-04</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>0.1</td>\n      <td>{'alpha': 0.1}</td>\n      <td>0.854701</td>\n      <td>0.880342</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.863248</td>\n      <td>0.871795</td>\n      <td>0.867521</td>\n      <td>0.841202</td>\n      <td>0.884120</td>\n      <td>0.867521</td>\n      <td>0.858974</td>\n      <td>0.854701</td>\n      <td>0.871245</td>\n      <td>0.888412</td>\n      <td>0.864442</td>\n      <td>0.012759</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.002002</td>\n      <td>9.905775e-07</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>0.5</td>\n      <td>{'alpha': 0.5}</td>\n      <td>0.850427</td>\n      <td>0.876068</td>\n      <td>0.850427</td>\n      <td>0.845494</td>\n      <td>0.845494</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.867521</td>\n      <td>0.836910</td>\n      <td>0.884120</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.854701</td>\n      <td>0.854077</td>\n      <td>0.875536</td>\n      <td>0.859300</td>\n      <td>0.013215</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.001935</td>\n      <td>5.738116e-04</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>1.0</td>\n      <td>{'alpha': 1.0}</td>\n      <td>0.850427</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.841202</td>\n      <td>0.845494</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.867521</td>\n      <td>0.832618</td>\n      <td>0.879828</td>\n      <td>0.871795</td>\n      <td>0.841880</td>\n      <td>0.854701</td>\n      <td>0.854077</td>\n      <td>0.875536</td>\n      <td>0.857587</td>\n      <td>0.013731</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_model = BernoulliNB()\n",
    "\n",
    "NB_params = {'alpha': [0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0]}\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(NB_model, NB_params, n_iter=7, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "# NB = RCV.fit(x_train, y_train).best_estimator_\n",
    "RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(RCV.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[1]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le seul paramètre testé dans ce modèle, alpha, correspond à la force du lissage de Laplace a appliqué au modèle.\n",
    "En d'autres termes, ce paramètre permet de palier la présence d'une caractéristique dans le jeu de test qui n'existe pas dans le jeu d'entrainement.\n",
    "> https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece\n",
    "\n",
    "Au vu des résultats, le paramètre alpha semble être optimal autour de 0.001\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification par arbre de décision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        0.005205      0.000833         0.000400        0.000612   \n1        0.002065      0.000443         0.000200        0.000400   \n2        0.003870      0.000499         0.000400        0.000490   \n3        0.001735      0.000443         0.000267        0.000443   \n4        0.004003      0.000633         0.000334        0.000472   \n5        0.001668      0.000472         0.000267        0.000443   \n6        0.004004      0.000517         0.000467        0.000499   \n7        0.001668      0.000472         0.000601        0.000490   \n8        0.009342      0.000944         0.000601        0.000490   \n9        0.003737      0.000443         0.000467        0.000499   \n10       0.009275      0.000681         0.000534        0.000499   \n11       0.003870      0.000340         0.000067        0.000250   \n12       0.009142      0.000719         0.000534        0.000499   \n13       0.004004      0.000366         0.000067        0.000250   \n14       0.009275      0.001845         0.000601        0.000490   \n15       0.003803      0.000400         0.000200        0.000400   \n16       0.005472      0.000499         0.000400        0.000490   \n17       0.001668      0.000472         0.000534        0.000499   \n18       0.006005      0.000895         0.000200        0.000400   \n19       0.001802      0.000542         0.000400        0.000490   \n20       0.005271      0.000443         0.000400        0.000490   \n21       0.001935      0.000574         0.000334        0.000472   \n22       0.005405      0.000490         0.000334        0.000472   \n23       0.001868      0.000340         0.000200        0.000400   \n24       0.014813      0.001328         0.000267        0.000574   \n25       0.004871      0.000499         0.000334        0.000472   \n26       0.013078      0.000681         0.000334        0.000472   \n27       0.004738      0.000681         0.000400        0.000490   \n28       0.012678      0.000597         0.000467        0.000499   \n29       0.004204      0.000400         0.000400        0.000490   \n30       0.012545      0.000806         0.000334        0.000472   \n31       0.004071      0.000574         0.000467        0.000499   \n\n   param_splitter param_min_samples_leaf param_max_depth param_criterion  \\\n0            best                      1               3            gini   \n1          random                      1               3            gini   \n2            best                      2               3            gini   \n3          random                      2               3            gini   \n4            best                      3               3            gini   \n5          random                      3               3            gini   \n6            best                      4               3            gini   \n7          random                      4               3            gini   \n8            best                      1            None            gini   \n9          random                      1            None            gini   \n10           best                      2            None            gini   \n11         random                      2            None            gini   \n12           best                      3            None            gini   \n13         random                      3            None            gini   \n14           best                      4            None            gini   \n15         random                      4            None            gini   \n16           best                      1               3         entropy   \n17         random                      1               3         entropy   \n18           best                      2               3         entropy   \n19         random                      2               3         entropy   \n20           best                      3               3         entropy   \n21         random                      3               3         entropy   \n22           best                      4               3         entropy   \n23         random                      4               3         entropy   \n24           best                      1            None         entropy   \n25         random                      1            None         entropy   \n26           best                      2            None         entropy   \n27         random                      2            None         entropy   \n28           best                      3            None         entropy   \n29         random                      3            None         entropy   \n30           best                      4            None         entropy   \n31         random                      4            None         entropy   \n\n                                               params  split0_test_score  \\\n0   {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.410256   \n1   {'splitter': 'random', 'min_samples_leaf': 1, ...           0.525641   \n2   {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.410256   \n3   {'splitter': 'random', 'min_samples_leaf': 2, ...           0.384615   \n4   {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.410256   \n5   {'splitter': 'random', 'min_samples_leaf': 3, ...           0.504274   \n6   {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.410256   \n7   {'splitter': 'random', 'min_samples_leaf': 4, ...           0.534188   \n8   {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.773504   \n9   {'splitter': 'random', 'min_samples_leaf': 1, ...           0.756410   \n10  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.769231   \n11  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.769231   \n12  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.747863   \n13  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.756410   \n14  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.747863   \n15  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.713675   \n16  {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.495726   \n17  {'splitter': 'random', 'min_samples_leaf': 1, ...           0.470085   \n18  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.495726   \n19  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.517094   \n20  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.495726   \n21  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.508547   \n22  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.495726   \n23  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.487179   \n24  {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.816239   \n25  {'splitter': 'random', 'min_samples_leaf': 1, ...           0.807692   \n26  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.786325   \n27  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.764957   \n28  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.777778   \n29  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.799145   \n30  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.756410   \n31  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.747863   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.470085           0.410256           0.484979   \n1            0.500000           0.559829           0.506438   \n2            0.470085           0.410256           0.484979   \n3            0.500000           0.559829           0.480687   \n4            0.470085           0.410256           0.484979   \n5            0.572650           0.517094           0.532189   \n6            0.470085           0.410256           0.484979   \n7            0.465812           0.555556           0.480687   \n8            0.829060           0.858974           0.806867   \n9            0.850427           0.876068           0.828326   \n10           0.850427           0.867521           0.806867   \n11           0.824786           0.816239           0.759657   \n12           0.850427           0.820513           0.789700   \n13           0.773504           0.829060           0.858369   \n14           0.850427           0.837607           0.798283   \n15           0.833333           0.773504           0.832618   \n16           0.529915           0.555556           0.575107   \n17           0.461538           0.564103           0.575107   \n18           0.529915           0.555556           0.575107   \n19           0.500000           0.581197           0.587983   \n20           0.529915           0.555556           0.575107   \n21           0.534188           0.581197           0.553648   \n22           0.529915           0.555556           0.575107   \n23           0.564103           0.512821           0.545064   \n24           0.871795           0.820513           0.858369   \n25           0.888889           0.811966           0.806867   \n26           0.829060           0.820513           0.849785   \n27           0.858974           0.833333           0.824034   \n28           0.863248           0.816239           0.828326   \n29           0.858974           0.794872           0.836910   \n30           0.863248           0.816239           0.832618   \n31           0.850427           0.829060           0.836910   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.463519           0.423077           0.457265   \n1            0.489270           0.525641           0.431624   \n2            0.463519           0.423077           0.457265   \n3            0.480687           0.440171           0.465812   \n4            0.463519           0.423077           0.457265   \n5            0.454936           0.491453           0.444444   \n6            0.463519           0.423077           0.457265   \n7            0.519313           0.487179           0.431624   \n8            0.811159           0.829060           0.863248   \n9            0.828326           0.846154           0.824786   \n10           0.781116           0.816239           0.846154   \n11           0.763948           0.854701           0.786325   \n12           0.772532           0.833333           0.837607   \n13           0.781116           0.816239           0.782051   \n14           0.759657           0.841880           0.841880   \n15           0.798283           0.803419           0.833333   \n16           0.553648           0.576923           0.538462   \n17           0.454936           0.594017           0.474359   \n18           0.553648           0.576923           0.538462   \n19           0.510730           0.508547           0.508547   \n20           0.553648           0.576923           0.538462   \n21           0.523605           0.568376           0.482906   \n22           0.553648           0.576923           0.538462   \n23           0.549356           0.512821           0.495726   \n24           0.819742           0.837607           0.820513   \n25           0.798283           0.854701           0.807692   \n26           0.785408           0.846154           0.820513   \n27           0.824034           0.833333           0.824786   \n28           0.824034           0.816239           0.799145   \n29           0.802575           0.794872           0.799145   \n30           0.798283           0.833333           0.803419   \n31           0.845494           0.824786           0.777778   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.457265           0.472103           0.467811   \n1            0.457265           0.446352           0.630901   \n2            0.457265           0.472103           0.467811   \n3            0.534188           0.489270           0.549356   \n4            0.457265           0.472103           0.472103   \n5            0.491453           0.497854           0.510730   \n6            0.457265           0.472103           0.467811   \n7            0.512821           0.463519           0.442060   \n8            0.786325           0.789700           0.781116   \n9            0.858974           0.815451           0.824034   \n10           0.799145           0.751073           0.746781   \n11           0.884615           0.755365           0.828326   \n12           0.807692           0.763948           0.772532   \n13           0.867521           0.763948           0.806867   \n14           0.816239           0.768240           0.759657   \n15           0.807692           0.785408           0.815451   \n16           0.581197           0.532189           0.566524   \n17           0.512821           0.484979           0.540773   \n18           0.581197           0.532189           0.566524   \n19           0.572650           0.553648           0.545064   \n20           0.581197           0.532189           0.566524   \n21           0.504274           0.549356           0.502146   \n22           0.581197           0.532189           0.566524   \n23           0.568376           0.506438           0.523605   \n24           0.854701           0.759657           0.824034   \n25           0.846154           0.841202           0.841202   \n26           0.858974           0.755365           0.815451   \n27           0.841880           0.789700           0.793991   \n28           0.841880           0.755365           0.815451   \n29           0.807692           0.733906           0.776824   \n30           0.846154           0.729614           0.815451   \n31           0.807692           0.781116           0.832618   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.423077            0.452991            0.465812   \n1             0.551282            0.508547            0.589744   \n2             0.423077            0.452991            0.465812   \n3             0.564103            0.457265            0.500000   \n4             0.423077            0.452991            0.465812   \n5             0.547009            0.559829            0.564103   \n6             0.423077            0.452991            0.465812   \n7             0.482906            0.431624            0.547009   \n8             0.786325            0.850427            0.846154   \n9             0.858974            0.850427            0.850427   \n10            0.764957            0.837607            0.841880   \n11            0.769231            0.769231            0.837607   \n12            0.782051            0.811966            0.816239   \n13            0.756410            0.820513            0.799145   \n14            0.782051            0.811966            0.820513   \n15            0.829060            0.837607            0.816239   \n16            0.559829            0.576923            0.581197   \n17            0.529915            0.521368            0.478632   \n18            0.559829            0.576923            0.581197   \n19            0.508547            0.470085            0.585470   \n20            0.559829            0.576923            0.581197   \n21            0.470085            0.534188            0.555556   \n22            0.559829            0.576923            0.581197   \n23            0.448718            0.568376            0.529915   \n24            0.769231            0.905983            0.803419   \n25            0.816239            0.854701            0.799145   \n26            0.782051            0.897436            0.769231   \n27            0.824786            0.807692            0.850427   \n28            0.773504            0.905983            0.786325   \n29            0.782051            0.820513            0.854701   \n30            0.777778            0.871795            0.773504   \n31            0.829060            0.811966            0.829060   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.493562            0.446352         0.453227        0.024958   \n1             0.523605            0.575107         0.521416        0.052688   \n2             0.493562            0.446352         0.453227        0.024958   \n3             0.536481            0.450644         0.492874        0.048338   \n4             0.493562            0.446352         0.453514        0.025147   \n5             0.557940            0.493562         0.515968        0.037842   \n6             0.493562            0.446352         0.453227        0.024958   \n7             0.523605            0.463519         0.489428        0.039371   \n8             0.841202            0.802575         0.817046        0.029322   \n9             0.841202            0.772532         0.832168        0.030948   \n10            0.849785            0.793991         0.808185        0.038279   \n11            0.819742            0.785408         0.801627        0.038085   \n12            0.854077            0.785408         0.803059        0.031525   \n13            0.802575            0.742489         0.797081        0.035786   \n14            0.836910            0.781116         0.803619        0.033786   \n15            0.828326            0.759657         0.804507        0.033280   \n16            0.549356            0.545064         0.554508        0.023036   \n17            0.532189            0.510730         0.513703        0.041511   \n18            0.549356            0.545064         0.554508        0.023036   \n19            0.502146            0.510730         0.530829        0.035787   \n20            0.549356            0.545064         0.554508        0.023036   \n21            0.566524            0.562232         0.533122        0.032341   \n22            0.549356            0.545064         0.554508        0.023036   \n23            0.459227            0.532189         0.520261        0.035686   \n24            0.836910            0.819742         0.827897        0.035532   \n25            0.845494            0.832618         0.830190        0.025099   \n26            0.815451            0.798283         0.815333        0.036406   \n27            0.806867            0.789700         0.817900        0.024709   \n28            0.836910            0.802575         0.816200        0.036461   \n29            0.849785            0.845494         0.810497        0.033162   \n30            0.849785            0.785408         0.810203        0.039292   \n31            0.789700            0.781116         0.811643        0.028737   \n\n    rank_test_score  \n0                30  \n1                23  \n2                30  \n3                27  \n4                29  \n5                25  \n6                30  \n7                28  \n8                 5  \n9                 1  \n10               11  \n11               15  \n12               14  \n13               16  \n14               13  \n15               12  \n16               17  \n17               26  \n18               17  \n19               22  \n20               17  \n21               21  \n22               17  \n23               24  \n24                3  \n25                2  \n26                7  \n27                4  \n28                6  \n29                9  \n30               10  \n31                8  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_splitter</th>\n      <th>param_min_samples_leaf</th>\n      <th>param_max_depth</th>\n      <th>param_criterion</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.005205</td>\n      <td>0.000833</td>\n      <td>0.000400</td>\n      <td>0.000612</td>\n      <td>best</td>\n      <td>1</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453227</td>\n      <td>0.024958</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.002065</td>\n      <td>0.000443</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>random</td>\n      <td>1</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.525641</td>\n      <td>0.500000</td>\n      <td>0.559829</td>\n      <td>0.506438</td>\n      <td>0.489270</td>\n      <td>0.525641</td>\n      <td>0.431624</td>\n      <td>0.457265</td>\n      <td>0.446352</td>\n      <td>0.630901</td>\n      <td>0.551282</td>\n      <td>0.508547</td>\n      <td>0.589744</td>\n      <td>0.523605</td>\n      <td>0.575107</td>\n      <td>0.521416</td>\n      <td>0.052688</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.003870</td>\n      <td>0.000499</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>best</td>\n      <td>2</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453227</td>\n      <td>0.024958</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.001735</td>\n      <td>0.000443</td>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>random</td>\n      <td>2</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.384615</td>\n      <td>0.500000</td>\n      <td>0.559829</td>\n      <td>0.480687</td>\n      <td>0.480687</td>\n      <td>0.440171</td>\n      <td>0.465812</td>\n      <td>0.534188</td>\n      <td>0.489270</td>\n      <td>0.549356</td>\n      <td>0.564103</td>\n      <td>0.457265</td>\n      <td>0.500000</td>\n      <td>0.536481</td>\n      <td>0.450644</td>\n      <td>0.492874</td>\n      <td>0.048338</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.004003</td>\n      <td>0.000633</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>best</td>\n      <td>3</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.472103</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453514</td>\n      <td>0.025147</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.001668</td>\n      <td>0.000472</td>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>random</td>\n      <td>3</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.504274</td>\n      <td>0.572650</td>\n      <td>0.517094</td>\n      <td>0.532189</td>\n      <td>0.454936</td>\n      <td>0.491453</td>\n      <td>0.444444</td>\n      <td>0.491453</td>\n      <td>0.497854</td>\n      <td>0.510730</td>\n      <td>0.547009</td>\n      <td>0.559829</td>\n      <td>0.564103</td>\n      <td>0.557940</td>\n      <td>0.493562</td>\n      <td>0.515968</td>\n      <td>0.037842</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.004004</td>\n      <td>0.000517</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>4</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453227</td>\n      <td>0.024958</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.001668</td>\n      <td>0.000472</td>\n      <td>0.000601</td>\n      <td>0.000490</td>\n      <td>random</td>\n      <td>4</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.534188</td>\n      <td>0.465812</td>\n      <td>0.555556</td>\n      <td>0.480687</td>\n      <td>0.519313</td>\n      <td>0.487179</td>\n      <td>0.431624</td>\n      <td>0.512821</td>\n      <td>0.463519</td>\n      <td>0.442060</td>\n      <td>0.482906</td>\n      <td>0.431624</td>\n      <td>0.547009</td>\n      <td>0.523605</td>\n      <td>0.463519</td>\n      <td>0.489428</td>\n      <td>0.039371</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.009342</td>\n      <td>0.000944</td>\n      <td>0.000601</td>\n      <td>0.000490</td>\n      <td>best</td>\n      <td>1</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.773504</td>\n      <td>0.829060</td>\n      <td>0.858974</td>\n      <td>0.806867</td>\n      <td>0.811159</td>\n      <td>0.829060</td>\n      <td>0.863248</td>\n      <td>0.786325</td>\n      <td>0.789700</td>\n      <td>0.781116</td>\n      <td>0.786325</td>\n      <td>0.850427</td>\n      <td>0.846154</td>\n      <td>0.841202</td>\n      <td>0.802575</td>\n      <td>0.817046</td>\n      <td>0.029322</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.003737</td>\n      <td>0.000443</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>random</td>\n      <td>1</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.756410</td>\n      <td>0.850427</td>\n      <td>0.876068</td>\n      <td>0.828326</td>\n      <td>0.828326</td>\n      <td>0.846154</td>\n      <td>0.824786</td>\n      <td>0.858974</td>\n      <td>0.815451</td>\n      <td>0.824034</td>\n      <td>0.858974</td>\n      <td>0.850427</td>\n      <td>0.850427</td>\n      <td>0.841202</td>\n      <td>0.772532</td>\n      <td>0.832168</td>\n      <td>0.030948</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.009275</td>\n      <td>0.000681</td>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>2</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.769231</td>\n      <td>0.850427</td>\n      <td>0.867521</td>\n      <td>0.806867</td>\n      <td>0.781116</td>\n      <td>0.816239</td>\n      <td>0.846154</td>\n      <td>0.799145</td>\n      <td>0.751073</td>\n      <td>0.746781</td>\n      <td>0.764957</td>\n      <td>0.837607</td>\n      <td>0.841880</td>\n      <td>0.849785</td>\n      <td>0.793991</td>\n      <td>0.808185</td>\n      <td>0.038279</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.003870</td>\n      <td>0.000340</td>\n      <td>0.000067</td>\n      <td>0.000250</td>\n      <td>random</td>\n      <td>2</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.769231</td>\n      <td>0.824786</td>\n      <td>0.816239</td>\n      <td>0.759657</td>\n      <td>0.763948</td>\n      <td>0.854701</td>\n      <td>0.786325</td>\n      <td>0.884615</td>\n      <td>0.755365</td>\n      <td>0.828326</td>\n      <td>0.769231</td>\n      <td>0.769231</td>\n      <td>0.837607</td>\n      <td>0.819742</td>\n      <td>0.785408</td>\n      <td>0.801627</td>\n      <td>0.038085</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.009142</td>\n      <td>0.000719</td>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>3</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.747863</td>\n      <td>0.850427</td>\n      <td>0.820513</td>\n      <td>0.789700</td>\n      <td>0.772532</td>\n      <td>0.833333</td>\n      <td>0.837607</td>\n      <td>0.807692</td>\n      <td>0.763948</td>\n      <td>0.772532</td>\n      <td>0.782051</td>\n      <td>0.811966</td>\n      <td>0.816239</td>\n      <td>0.854077</td>\n      <td>0.785408</td>\n      <td>0.803059</td>\n      <td>0.031525</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.004004</td>\n      <td>0.000366</td>\n      <td>0.000067</td>\n      <td>0.000250</td>\n      <td>random</td>\n      <td>3</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.756410</td>\n      <td>0.773504</td>\n      <td>0.829060</td>\n      <td>0.858369</td>\n      <td>0.781116</td>\n      <td>0.816239</td>\n      <td>0.782051</td>\n      <td>0.867521</td>\n      <td>0.763948</td>\n      <td>0.806867</td>\n      <td>0.756410</td>\n      <td>0.820513</td>\n      <td>0.799145</td>\n      <td>0.802575</td>\n      <td>0.742489</td>\n      <td>0.797081</td>\n      <td>0.035786</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.009275</td>\n      <td>0.001845</td>\n      <td>0.000601</td>\n      <td>0.000490</td>\n      <td>best</td>\n      <td>4</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.747863</td>\n      <td>0.850427</td>\n      <td>0.837607</td>\n      <td>0.798283</td>\n      <td>0.759657</td>\n      <td>0.841880</td>\n      <td>0.841880</td>\n      <td>0.816239</td>\n      <td>0.768240</td>\n      <td>0.759657</td>\n      <td>0.782051</td>\n      <td>0.811966</td>\n      <td>0.820513</td>\n      <td>0.836910</td>\n      <td>0.781116</td>\n      <td>0.803619</td>\n      <td>0.033786</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.003803</td>\n      <td>0.000400</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>random</td>\n      <td>4</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.713675</td>\n      <td>0.833333</td>\n      <td>0.773504</td>\n      <td>0.832618</td>\n      <td>0.798283</td>\n      <td>0.803419</td>\n      <td>0.833333</td>\n      <td>0.807692</td>\n      <td>0.785408</td>\n      <td>0.815451</td>\n      <td>0.829060</td>\n      <td>0.837607</td>\n      <td>0.816239</td>\n      <td>0.828326</td>\n      <td>0.759657</td>\n      <td>0.804507</td>\n      <td>0.033280</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.005472</td>\n      <td>0.000499</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>best</td>\n      <td>1</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.001668</td>\n      <td>0.000472</td>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>random</td>\n      <td>1</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.470085</td>\n      <td>0.461538</td>\n      <td>0.564103</td>\n      <td>0.575107</td>\n      <td>0.454936</td>\n      <td>0.594017</td>\n      <td>0.474359</td>\n      <td>0.512821</td>\n      <td>0.484979</td>\n      <td>0.540773</td>\n      <td>0.529915</td>\n      <td>0.521368</td>\n      <td>0.478632</td>\n      <td>0.532189</td>\n      <td>0.510730</td>\n      <td>0.513703</td>\n      <td>0.041511</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.006005</td>\n      <td>0.000895</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>best</td>\n      <td>2</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.001802</td>\n      <td>0.000542</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>random</td>\n      <td>2</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.517094</td>\n      <td>0.500000</td>\n      <td>0.581197</td>\n      <td>0.587983</td>\n      <td>0.510730</td>\n      <td>0.508547</td>\n      <td>0.508547</td>\n      <td>0.572650</td>\n      <td>0.553648</td>\n      <td>0.545064</td>\n      <td>0.508547</td>\n      <td>0.470085</td>\n      <td>0.585470</td>\n      <td>0.502146</td>\n      <td>0.510730</td>\n      <td>0.530829</td>\n      <td>0.035787</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.005271</td>\n      <td>0.000443</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>best</td>\n      <td>3</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.001935</td>\n      <td>0.000574</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>random</td>\n      <td>3</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.508547</td>\n      <td>0.534188</td>\n      <td>0.581197</td>\n      <td>0.553648</td>\n      <td>0.523605</td>\n      <td>0.568376</td>\n      <td>0.482906</td>\n      <td>0.504274</td>\n      <td>0.549356</td>\n      <td>0.502146</td>\n      <td>0.470085</td>\n      <td>0.534188</td>\n      <td>0.555556</td>\n      <td>0.566524</td>\n      <td>0.562232</td>\n      <td>0.533122</td>\n      <td>0.032341</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.005405</td>\n      <td>0.000490</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>best</td>\n      <td>4</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.001868</td>\n      <td>0.000340</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>random</td>\n      <td>4</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.487179</td>\n      <td>0.564103</td>\n      <td>0.512821</td>\n      <td>0.545064</td>\n      <td>0.549356</td>\n      <td>0.512821</td>\n      <td>0.495726</td>\n      <td>0.568376</td>\n      <td>0.506438</td>\n      <td>0.523605</td>\n      <td>0.448718</td>\n      <td>0.568376</td>\n      <td>0.529915</td>\n      <td>0.459227</td>\n      <td>0.532189</td>\n      <td>0.520261</td>\n      <td>0.035686</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.014813</td>\n      <td>0.001328</td>\n      <td>0.000267</td>\n      <td>0.000574</td>\n      <td>best</td>\n      <td>1</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.816239</td>\n      <td>0.871795</td>\n      <td>0.820513</td>\n      <td>0.858369</td>\n      <td>0.819742</td>\n      <td>0.837607</td>\n      <td>0.820513</td>\n      <td>0.854701</td>\n      <td>0.759657</td>\n      <td>0.824034</td>\n      <td>0.769231</td>\n      <td>0.905983</td>\n      <td>0.803419</td>\n      <td>0.836910</td>\n      <td>0.819742</td>\n      <td>0.827897</td>\n      <td>0.035532</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.004871</td>\n      <td>0.000499</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>random</td>\n      <td>1</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.807692</td>\n      <td>0.888889</td>\n      <td>0.811966</td>\n      <td>0.806867</td>\n      <td>0.798283</td>\n      <td>0.854701</td>\n      <td>0.807692</td>\n      <td>0.846154</td>\n      <td>0.841202</td>\n      <td>0.841202</td>\n      <td>0.816239</td>\n      <td>0.854701</td>\n      <td>0.799145</td>\n      <td>0.845494</td>\n      <td>0.832618</td>\n      <td>0.830190</td>\n      <td>0.025099</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.013078</td>\n      <td>0.000681</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>best</td>\n      <td>2</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.786325</td>\n      <td>0.829060</td>\n      <td>0.820513</td>\n      <td>0.849785</td>\n      <td>0.785408</td>\n      <td>0.846154</td>\n      <td>0.820513</td>\n      <td>0.858974</td>\n      <td>0.755365</td>\n      <td>0.815451</td>\n      <td>0.782051</td>\n      <td>0.897436</td>\n      <td>0.769231</td>\n      <td>0.815451</td>\n      <td>0.798283</td>\n      <td>0.815333</td>\n      <td>0.036406</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.004738</td>\n      <td>0.000681</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>random</td>\n      <td>2</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.764957</td>\n      <td>0.858974</td>\n      <td>0.833333</td>\n      <td>0.824034</td>\n      <td>0.824034</td>\n      <td>0.833333</td>\n      <td>0.824786</td>\n      <td>0.841880</td>\n      <td>0.789700</td>\n      <td>0.793991</td>\n      <td>0.824786</td>\n      <td>0.807692</td>\n      <td>0.850427</td>\n      <td>0.806867</td>\n      <td>0.789700</td>\n      <td>0.817900</td>\n      <td>0.024709</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.012678</td>\n      <td>0.000597</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>3</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.777778</td>\n      <td>0.863248</td>\n      <td>0.816239</td>\n      <td>0.828326</td>\n      <td>0.824034</td>\n      <td>0.816239</td>\n      <td>0.799145</td>\n      <td>0.841880</td>\n      <td>0.755365</td>\n      <td>0.815451</td>\n      <td>0.773504</td>\n      <td>0.905983</td>\n      <td>0.786325</td>\n      <td>0.836910</td>\n      <td>0.802575</td>\n      <td>0.816200</td>\n      <td>0.036461</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.004204</td>\n      <td>0.000400</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>random</td>\n      <td>3</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.799145</td>\n      <td>0.858974</td>\n      <td>0.794872</td>\n      <td>0.836910</td>\n      <td>0.802575</td>\n      <td>0.794872</td>\n      <td>0.799145</td>\n      <td>0.807692</td>\n      <td>0.733906</td>\n      <td>0.776824</td>\n      <td>0.782051</td>\n      <td>0.820513</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.845494</td>\n      <td>0.810497</td>\n      <td>0.033162</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.012545</td>\n      <td>0.000806</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>best</td>\n      <td>4</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.756410</td>\n      <td>0.863248</td>\n      <td>0.816239</td>\n      <td>0.832618</td>\n      <td>0.798283</td>\n      <td>0.833333</td>\n      <td>0.803419</td>\n      <td>0.846154</td>\n      <td>0.729614</td>\n      <td>0.815451</td>\n      <td>0.777778</td>\n      <td>0.871795</td>\n      <td>0.773504</td>\n      <td>0.849785</td>\n      <td>0.785408</td>\n      <td>0.810203</td>\n      <td>0.039292</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.004071</td>\n      <td>0.000574</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>random</td>\n      <td>4</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.747863</td>\n      <td>0.850427</td>\n      <td>0.829060</td>\n      <td>0.836910</td>\n      <td>0.845494</td>\n      <td>0.824786</td>\n      <td>0.777778</td>\n      <td>0.807692</td>\n      <td>0.781116</td>\n      <td>0.832618</td>\n      <td>0.829060</td>\n      <td>0.811966</td>\n      <td>0.829060</td>\n      <td>0.789700</td>\n      <td>0.781116</td>\n      <td>0.811643</td>\n      <td>0.028737</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT_model = DecisionTreeClassifier()\n",
    "\n",
    "DT_params = dict()\n",
    "DT_params['max_depth'] = [3, None]\n",
    "DT_params['min_samples_leaf'] = [1, 2, 3, 4]\n",
    "DT_params['criterion'] = [\"gini\", \"entropy\"]\n",
    "DT_params['splitter'] = [\"best\", \"random\"]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "RCV = RandomizedSearchCV(DT_model, DT_params, n_iter=32, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "DT = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(DT.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[2]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les paramètres testés pour ce modèle sont :\n",
    "-le max_depth, correspondant à la profondeur de l'arbre qui doit être illimité pour garantir une précision maximale du modèle.\n",
    "-le min_samples_leaf, correspondant au nombre d'échantillons minimal par feuille qui doit être de 1.\n",
    "-le criterion est une fonction permettant de mesurer la qualité d'un split, nous obtenons des meilleurs résultats avec 'entropy'.\n",
    "-le splitter est la méthode choisie pour selectionner le split à chaque noeud. Modifier ce paramètre ne semble pas impacter significativement les résultats."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification par boosting de gradient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "642551df",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        1.782657      0.018488         0.002069        0.000250   \n1        3.563709      0.041753         0.003537        0.000499   \n2        8.987940      0.099468         0.007740        0.000443   \n3        3.003930      0.043445         0.003537        0.000499   \n4        6.005655      0.076185         0.006206        0.000400   \n5       14.643802      0.113957         0.014213        0.000400   \n6        4.335538      0.064869         0.004738        0.000443   \n7        8.534718      0.075940         0.008875        0.000619   \n8       12.051415      0.223604         0.012211        0.000833   \n9        7.235107      0.083668         0.007607        0.000713   \n10       8.759521      0.151750         0.008808        0.000400   \n11      10.628956      0.160397         0.010543        0.000499   \n12       1.829695      0.036297         0.002335        0.000597   \n13       3.674739      0.044383         0.003603        0.000490   \n14       9.184043      0.125984         0.008274        0.000574   \n15       3.071591      0.040004         0.003536        0.000499   \n16       6.167573      0.152733         0.006406        0.000490   \n17       9.404550      0.210319         0.009475        0.000499   \n18       4.329733      0.099621         0.004671        0.000472   \n19       5.799200      0.139014         0.006072        0.000443   \n20       7.593565      0.149349         0.007941        0.000250   \n21       4.449910      0.094663         0.005138        0.000719   \n22       5.000307      0.099910         0.005739        0.000855   \n23       6.790469      0.105016         0.007607        0.000490   \n24       1.857389      0.018594         0.002236        0.000404   \n25       3.680042      0.039397         0.003803        0.000400   \n26       9.007250      0.083105         0.008608        0.000801   \n27       3.071392      0.048642         0.003536        0.000499   \n28       5.292208      0.090125         0.005738        0.000680   \n29       7.099225      0.125361         0.007607        0.000490   \n30       3.769631      0.100205         0.004271        0.000443   \n31       4.320758      0.092689         0.004871        0.000340   \n32       6.070847      0.095832         0.007140        0.001148   \n33       3.162341      0.056497         0.003870        0.000340   \n34       3.748470      0.083335         0.004337        0.000472   \n35       5.185328      0.562457         0.005538        0.001025   \n\n   param_n_estimators param_max_depth param_learning_rate  \\\n0                 100               1                 0.1   \n1                 200               1                 0.1   \n2                 500               1                 0.1   \n3                 100               2                 0.1   \n4                 200               2                 0.1   \n5                 500               2                 0.1   \n6                 100               3                 0.1   \n7                 200               3                 0.1   \n8                 500               3                 0.1   \n9                 100               5                 0.1   \n10                200               5                 0.1   \n11                500               5                 0.1   \n12                100               1                 0.2   \n13                200               1                 0.2   \n14                500               1                 0.2   \n15                100               2                 0.2   \n16                200               2                 0.2   \n17                500               2                 0.2   \n18                100               3                 0.2   \n19                200               3                 0.2   \n20                500               3                 0.2   \n21                100               5                 0.2   \n22                200               5                 0.2   \n23                500               5                 0.2   \n24                100               1                 0.3   \n25                200               1                 0.3   \n26                500               1                 0.3   \n27                100               2                 0.3   \n28                200               2                 0.3   \n29                500               2                 0.3   \n30                100               3                 0.3   \n31                200               3                 0.3   \n32                500               3                 0.3   \n33                100               5                 0.3   \n34                200               5                 0.3   \n35                500               5                 0.3   \n\n                                               params  split0_test_score  \\\n0   {'n_estimators': 100, 'max_depth': 1, 'learnin...           0.905983   \n1   {'n_estimators': 200, 'max_depth': 1, 'learnin...           0.923077   \n2   {'n_estimators': 500, 'max_depth': 1, 'learnin...           0.952991   \n3   {'n_estimators': 100, 'max_depth': 2, 'learnin...           0.961538   \n4   {'n_estimators': 200, 'max_depth': 2, 'learnin...           0.974359   \n5   {'n_estimators': 500, 'max_depth': 2, 'learnin...           0.970085   \n6   {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.952991   \n7   {'n_estimators': 200, 'max_depth': 3, 'learnin...           0.974359   \n8   {'n_estimators': 500, 'max_depth': 3, 'learnin...           0.965812   \n9   {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.905983   \n10  {'n_estimators': 200, 'max_depth': 5, 'learnin...           0.914530   \n11  {'n_estimators': 500, 'max_depth': 5, 'learnin...           0.914530   \n12  {'n_estimators': 100, 'max_depth': 1, 'learnin...           0.935897   \n13  {'n_estimators': 200, 'max_depth': 1, 'learnin...           0.957265   \n14  {'n_estimators': 500, 'max_depth': 1, 'learnin...           0.961538   \n15  {'n_estimators': 100, 'max_depth': 2, 'learnin...           0.974359   \n16  {'n_estimators': 200, 'max_depth': 2, 'learnin...           0.974359   \n17  {'n_estimators': 500, 'max_depth': 2, 'learnin...           0.974359   \n18  {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.970085   \n19  {'n_estimators': 200, 'max_depth': 3, 'learnin...           0.965812   \n20  {'n_estimators': 500, 'max_depth': 3, 'learnin...           0.974359   \n21  {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.918803   \n22  {'n_estimators': 200, 'max_depth': 5, 'learnin...           0.914530   \n23  {'n_estimators': 500, 'max_depth': 5, 'learnin...           0.905983   \n24  {'n_estimators': 100, 'max_depth': 1, 'learnin...           0.952991   \n25  {'n_estimators': 200, 'max_depth': 1, 'learnin...           0.948718   \n26  {'n_estimators': 500, 'max_depth': 1, 'learnin...           0.948718   \n27  {'n_estimators': 100, 'max_depth': 2, 'learnin...           0.978632   \n28  {'n_estimators': 200, 'max_depth': 2, 'learnin...           0.974359   \n29  {'n_estimators': 500, 'max_depth': 2, 'learnin...           0.974359   \n30  {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.957265   \n31  {'n_estimators': 200, 'max_depth': 3, 'learnin...           0.965812   \n32  {'n_estimators': 500, 'max_depth': 3, 'learnin...           0.961538   \n33  {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.914530   \n34  {'n_estimators': 200, 'max_depth': 5, 'learnin...           0.914530   \n35  {'n_estimators': 500, 'max_depth': 5, 'learnin...           0.918803   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.952991           0.940171           0.935622   \n1            0.961538           0.952991           0.957082   \n2            0.961538           0.965812           0.961373   \n3            0.970085           0.935897           0.969957   \n4            0.965812           0.948718           0.961373   \n5            0.965812           0.952991           0.961373   \n6            0.957265           0.940171           0.965665   \n7            0.957265           0.940171           0.969957   \n8            0.961538           0.944444           0.969957   \n9            0.948718           0.940171           0.927039   \n10           0.944444           0.952991           0.939914   \n11           0.944444           0.952991           0.939914   \n12           0.961538           0.952991           0.957082   \n13           0.965812           0.957265           0.948498   \n14           0.957265           0.957265           0.965665   \n15           0.974359           0.948718           0.969957   \n16           0.970085           0.957265           0.965665   \n17           0.965812           0.952991           0.969957   \n18           0.961538           0.944444           0.965665   \n19           0.961538           0.944444           0.965665   \n20           0.957265           0.944444           0.965665   \n21           0.948718           0.957265           0.948498   \n22           0.940171           0.944444           0.948498   \n23           0.935897           0.944444           0.944206   \n24           0.961538           0.952991           0.957082   \n25           0.961538           0.961538           0.961373   \n26           0.957265           0.961538           0.965665   \n27           0.974359           0.952991           0.969957   \n28           0.965812           0.952991           0.969957   \n29           0.965812           0.952991           0.969957   \n30           0.965812           0.952991           0.969957   \n31           0.961538           0.952991           0.969957   \n32           0.961538           0.952991           0.965665   \n33           0.952991           0.948718           0.952790   \n34           0.952991           0.948718           0.957082   \n35           0.952991           0.944444           0.948498   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.922747           0.927350           0.961538   \n1            0.931330           0.957265           0.974359   \n2            0.935622           0.961538           0.952991   \n3            0.939914           0.952991           0.965812   \n4            0.957082           0.965812           0.965812   \n5            0.961373           0.961538           0.970085   \n6            0.952790           0.957265           0.978632   \n7            0.957082           0.952991           0.982906   \n8            0.957082           0.957265           0.982906   \n9            0.944206           0.940171           0.952991   \n10           0.957082           0.944444           0.948718   \n11           0.957082           0.940171           0.952991   \n12           0.931330           0.961538           0.974359   \n13           0.939914           0.965812           0.965812   \n14           0.948498           0.961538           0.961538   \n15           0.957082           0.965812           0.970085   \n16           0.965665           0.957265           0.982906   \n17           0.965665           0.961538           0.974359   \n18           0.944206           0.948718           0.982906   \n19           0.952790           0.957265           0.982906   \n20           0.952790           0.957265           0.982906   \n21           0.961373           0.935897           0.948718   \n22           0.961373           0.940171           0.952991   \n23           0.948498           0.940171           0.948718   \n24           0.935622           0.965812           0.961538   \n25           0.944206           0.957265           0.965812   \n26           0.948498           0.961538           0.965812   \n27           0.957082           0.961538           0.974359   \n28           0.961373           0.965812           0.978632   \n29           0.961373           0.965812           0.978632   \n30           0.952790           0.952991           0.982906   \n31           0.965665           0.957265           0.982906   \n32           0.952790           0.952991           0.982906   \n33           0.974249           0.944444           0.965812   \n34           0.952790           0.944444           0.957265   \n35           0.948498           0.952991           0.961538   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.940171           0.931330           0.927039   \n1            0.948718           0.939914           0.944206   \n2            0.957265           0.948498           0.957082   \n3            0.970085           0.935622           0.957082   \n4            0.974359           0.939914           0.957082   \n5            0.978632           0.961373           0.965665   \n6            0.957265           0.939914           0.961373   \n7            0.974359           0.948498           0.965665   \n8            0.974359           0.939914           0.965665   \n9            0.940171           0.888412           0.948498   \n10           0.948718           0.918455           0.939914   \n11           0.940171           0.927039           0.948498   \n12           0.952991           0.935622           0.944206   \n13           0.961538           0.948498           0.952790   \n14           0.961538           0.952790           0.961373   \n15           0.978632           0.939914           0.969957   \n16           0.978632           0.957082           0.965665   \n17           0.978632           0.957082           0.969957   \n18           0.970085           0.965665           0.961373   \n19           0.965812           0.957082           0.974249   \n20           0.970085           0.957082           0.969957   \n21           0.940171           0.914163           0.939914   \n22           0.948718           0.914163           0.948498   \n23           0.948718           0.935622           0.939914   \n24           0.961538           0.939914           0.948498   \n25           0.961538           0.948498           0.961373   \n26           0.965812           0.944206           0.969957   \n27           0.974359           0.961373           0.965665   \n28           0.974359           0.961373           0.974249   \n29           0.974359           0.961373           0.974249   \n30           0.970085           0.957082           0.961373   \n31           0.974359           0.952790           0.969957   \n32           0.965812           0.961373           0.969957   \n33           0.957265           0.927039           0.927039   \n34           0.944444           0.922747           0.944206   \n35           0.948718           0.935622           0.922747   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.931624            0.897436            0.957265   \n1             0.948718            0.905983            0.974359   \n2             0.961538            0.923077            0.970085   \n3             0.935897            0.952991            0.970085   \n4             0.948718            0.957265            0.974359   \n5             0.957265            0.952991            0.978632   \n6             0.935897            0.952991            0.965812   \n7             0.940171            0.952991            0.970085   \n8             0.948718            0.952991            0.974359   \n9             0.918803            0.944444            0.952991   \n10            0.923077            0.948718            0.970085   \n11            0.935897            0.952991            0.970085   \n12            0.952991            0.910256            0.978632   \n13            0.957265            0.923077            0.970085   \n14            0.957265            0.931624            0.970085   \n15            0.948718            0.952991            0.974359   \n16            0.952991            0.952991            0.978632   \n17            0.957265            0.952991            0.974359   \n18            0.948718            0.952991            0.970085   \n19            0.948718            0.948718            0.970085   \n20            0.948718            0.952991            0.970085   \n21            0.918803            0.948718            0.957265   \n22            0.923077            0.961538            0.961538   \n23            0.927350            0.952991            0.961538   \n24            0.957265            0.914530            0.970085   \n25            0.961538            0.923077            0.970085   \n26            0.952991            0.931624            0.961538   \n27            0.952991            0.952991            0.970085   \n28            0.952991            0.961538            0.974359   \n29            0.952991            0.961538            0.978632   \n30            0.952991            0.957265            0.970085   \n31            0.952991            0.957265            0.965812   \n32            0.952991            0.952991            0.974359   \n33            0.918803            0.961538            0.965812   \n34            0.927350            0.970085            0.970085   \n35            0.923077            0.952991            0.974359   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.948498            0.914163         0.932929        0.017663   \n1             0.969957            0.939914         0.948627        0.018354   \n2             0.969957            0.957082         0.955763        0.012075   \n3             0.978541            0.957082         0.956905        0.013941   \n4             0.978541            0.969957         0.962611        0.010786   \n5             0.974249            0.961373         0.964896        0.007843   \n6             0.957082            0.948498         0.954907        0.010699   \n7             0.969957            0.952790         0.960617        0.012512   \n8             0.974249            0.952790         0.961470        0.012013   \n9             0.944206            0.944206         0.936067        0.017818   \n10            0.952790            0.952790         0.943778        0.014481   \n11            0.961373            0.939914         0.945206        0.013364   \n12            0.974249            0.944206         0.951193        0.017746   \n13            0.969957            0.957082         0.956045        0.012067   \n14            0.974249            0.957082         0.958621        0.009452   \n15            0.969957            0.957082         0.963466        0.011380   \n16            0.969957            0.965665         0.966322        0.009226   \n17            0.969957            0.965665         0.966039        0.007877   \n18            0.969957            0.957082         0.960901        0.010921   \n19            0.978541            0.952790         0.961761        0.011091   \n20            0.978541            0.948498         0.962043        0.011468   \n21            0.948498            0.948498         0.942354        0.014136   \n22            0.965665            0.952790         0.945211        0.015940   \n23            0.952790            0.948498         0.942356        0.012649   \n24            0.974249            0.948498         0.953477        0.014495   \n25            0.969957            0.965665         0.957479        0.011732   \n26            0.974249            0.952790         0.957480        0.010760   \n27            0.969957            0.965665         0.965467        0.008333   \n28            0.969957            0.965665         0.966895        0.007563   \n29            0.969957            0.965665         0.967180        0.007911   \n30            0.974249            0.961373         0.962614        0.008900   \n31            0.969957            0.948498         0.963184        0.009222   \n32            0.978541            0.961373         0.963188        0.009466   \n33            0.948498            0.952790         0.947488        0.017367   \n34            0.948498            0.948498         0.946916        0.015054   \n35            0.957082            0.952790         0.946343        0.014822   \n\n    rank_test_score  \n0                36  \n1                26  \n2                22  \n3                20  \n4                11  \n5                 6  \n6                23  \n7                16  \n8                14  \n9                35  \n10               32  \n11               31  \n12               25  \n13               21  \n14               17  \n15                7  \n16                3  \n17                4  \n18               15  \n19               13  \n20               12  \n21               34  \n22               30  \n23               33  \n24               24  \n25               19  \n26               18  \n27                5  \n28                2  \n29                1  \n30               10  \n31                9  \n32                8  \n33               27  \n34               28  \n35               29  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_n_estimators</th>\n      <th>param_max_depth</th>\n      <th>param_learning_rate</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.782657</td>\n      <td>0.018488</td>\n      <td>0.002069</td>\n      <td>0.000250</td>\n      <td>100</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 1, 'learnin...</td>\n      <td>0.905983</td>\n      <td>0.952991</td>\n      <td>0.940171</td>\n      <td>0.935622</td>\n      <td>0.922747</td>\n      <td>0.927350</td>\n      <td>0.961538</td>\n      <td>0.940171</td>\n      <td>0.931330</td>\n      <td>0.927039</td>\n      <td>0.931624</td>\n      <td>0.897436</td>\n      <td>0.957265</td>\n      <td>0.948498</td>\n      <td>0.914163</td>\n      <td>0.932929</td>\n      <td>0.017663</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.563709</td>\n      <td>0.041753</td>\n      <td>0.003537</td>\n      <td>0.000499</td>\n      <td>200</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 200, 'max_depth': 1, 'learnin...</td>\n      <td>0.923077</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.931330</td>\n      <td>0.957265</td>\n      <td>0.974359</td>\n      <td>0.948718</td>\n      <td>0.939914</td>\n      <td>0.944206</td>\n      <td>0.948718</td>\n      <td>0.905983</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.939914</td>\n      <td>0.948627</td>\n      <td>0.018354</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.987940</td>\n      <td>0.099468</td>\n      <td>0.007740</td>\n      <td>0.000443</td>\n      <td>500</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 500, 'max_depth': 1, 'learnin...</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.961373</td>\n      <td>0.935622</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.957265</td>\n      <td>0.948498</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.923077</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.955763</td>\n      <td>0.012075</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.003930</td>\n      <td>0.043445</td>\n      <td>0.003537</td>\n      <td>0.000499</td>\n      <td>100</td>\n      <td>2</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 2, 'learnin...</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.935897</td>\n      <td>0.969957</td>\n      <td>0.939914</td>\n      <td>0.952991</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.935622</td>\n      <td>0.957082</td>\n      <td>0.935897</td>\n      <td>0.952991</td>\n      <td>0.970085</td>\n      <td>0.978541</td>\n      <td>0.957082</td>\n      <td>0.956905</td>\n      <td>0.013941</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6.005655</td>\n      <td>0.076185</td>\n      <td>0.006206</td>\n      <td>0.000400</td>\n      <td>200</td>\n      <td>2</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 200, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.948718</td>\n      <td>0.961373</td>\n      <td>0.957082</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.939914</td>\n      <td>0.957082</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.962611</td>\n      <td>0.010786</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>14.643802</td>\n      <td>0.113957</td>\n      <td>0.014213</td>\n      <td>0.000400</td>\n      <td>500</td>\n      <td>2</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 500, 'max_depth': 2, 'learnin...</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.961373</td>\n      <td>0.961373</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.978632</td>\n      <td>0.961373</td>\n      <td>0.965665</td>\n      <td>0.957265</td>\n      <td>0.952991</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964896</td>\n      <td>0.007843</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4.335538</td>\n      <td>0.064869</td>\n      <td>0.004738</td>\n      <td>0.000443</td>\n      <td>100</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.952991</td>\n      <td>0.957265</td>\n      <td>0.940171</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.978632</td>\n      <td>0.957265</td>\n      <td>0.939914</td>\n      <td>0.961373</td>\n      <td>0.935897</td>\n      <td>0.952991</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.948498</td>\n      <td>0.954907</td>\n      <td>0.010699</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8.534718</td>\n      <td>0.075940</td>\n      <td>0.008875</td>\n      <td>0.000619</td>\n      <td>200</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 200, 'max_depth': 3, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.957265</td>\n      <td>0.940171</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.952991</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.965665</td>\n      <td>0.940171</td>\n      <td>0.952991</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.960617</td>\n      <td>0.012512</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>12.051415</td>\n      <td>0.223604</td>\n      <td>0.012211</td>\n      <td>0.000833</td>\n      <td>500</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 500, 'max_depth': 3, 'learnin...</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.944444</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.939914</td>\n      <td>0.965665</td>\n      <td>0.948718</td>\n      <td>0.952991</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.952790</td>\n      <td>0.961470</td>\n      <td>0.012013</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>7.235107</td>\n      <td>0.083668</td>\n      <td>0.007607</td>\n      <td>0.000713</td>\n      <td>100</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.905983</td>\n      <td>0.948718</td>\n      <td>0.940171</td>\n      <td>0.927039</td>\n      <td>0.944206</td>\n      <td>0.940171</td>\n      <td>0.952991</td>\n      <td>0.940171</td>\n      <td>0.888412</td>\n      <td>0.948498</td>\n      <td>0.918803</td>\n      <td>0.944444</td>\n      <td>0.952991</td>\n      <td>0.944206</td>\n      <td>0.944206</td>\n      <td>0.936067</td>\n      <td>0.017818</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>8.759521</td>\n      <td>0.151750</td>\n      <td>0.008808</td>\n      <td>0.000400</td>\n      <td>200</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 200, 'max_depth': 5, 'learnin...</td>\n      <td>0.914530</td>\n      <td>0.944444</td>\n      <td>0.952991</td>\n      <td>0.939914</td>\n      <td>0.957082</td>\n      <td>0.944444</td>\n      <td>0.948718</td>\n      <td>0.948718</td>\n      <td>0.918455</td>\n      <td>0.939914</td>\n      <td>0.923077</td>\n      <td>0.948718</td>\n      <td>0.970085</td>\n      <td>0.952790</td>\n      <td>0.952790</td>\n      <td>0.943778</td>\n      <td>0.014481</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>10.628956</td>\n      <td>0.160397</td>\n      <td>0.010543</td>\n      <td>0.000499</td>\n      <td>500</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 500, 'max_depth': 5, 'learnin...</td>\n      <td>0.914530</td>\n      <td>0.944444</td>\n      <td>0.952991</td>\n      <td>0.939914</td>\n      <td>0.957082</td>\n      <td>0.940171</td>\n      <td>0.952991</td>\n      <td>0.940171</td>\n      <td>0.927039</td>\n      <td>0.948498</td>\n      <td>0.935897</td>\n      <td>0.952991</td>\n      <td>0.970085</td>\n      <td>0.961373</td>\n      <td>0.939914</td>\n      <td>0.945206</td>\n      <td>0.013364</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.829695</td>\n      <td>0.036297</td>\n      <td>0.002335</td>\n      <td>0.000597</td>\n      <td>100</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 100, 'max_depth': 1, 'learnin...</td>\n      <td>0.935897</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.931330</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.935622</td>\n      <td>0.944206</td>\n      <td>0.952991</td>\n      <td>0.910256</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.944206</td>\n      <td>0.951193</td>\n      <td>0.017746</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3.674739</td>\n      <td>0.044383</td>\n      <td>0.003603</td>\n      <td>0.000490</td>\n      <td>200</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 200, 'max_depth': 1, 'learnin...</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957265</td>\n      <td>0.948498</td>\n      <td>0.939914</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.948498</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.923077</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.956045</td>\n      <td>0.012067</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>9.184043</td>\n      <td>0.125984</td>\n      <td>0.008274</td>\n      <td>0.000574</td>\n      <td>500</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 500, 'max_depth': 1, 'learnin...</td>\n      <td>0.961538</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.952790</td>\n      <td>0.961373</td>\n      <td>0.957265</td>\n      <td>0.931624</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.958621</td>\n      <td>0.009452</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>3.071591</td>\n      <td>0.040004</td>\n      <td>0.003536</td>\n      <td>0.000499</td>\n      <td>100</td>\n      <td>2</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 100, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.948718</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.978632</td>\n      <td>0.939914</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.952991</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.963466</td>\n      <td>0.011380</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>6.167573</td>\n      <td>0.152733</td>\n      <td>0.006406</td>\n      <td>0.000490</td>\n      <td>200</td>\n      <td>2</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 200, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.965665</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.952991</td>\n      <td>0.952991</td>\n      <td>0.978632</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.966322</td>\n      <td>0.009226</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>9.404550</td>\n      <td>0.210319</td>\n      <td>0.009475</td>\n      <td>0.000499</td>\n      <td>500</td>\n      <td>2</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 500, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.969957</td>\n      <td>0.957265</td>\n      <td>0.952991</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.966039</td>\n      <td>0.007877</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>4.329733</td>\n      <td>0.099621</td>\n      <td>0.004671</td>\n      <td>0.000472</td>\n      <td>100</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.970085</td>\n      <td>0.961538</td>\n      <td>0.944444</td>\n      <td>0.965665</td>\n      <td>0.944206</td>\n      <td>0.948718</td>\n      <td>0.982906</td>\n      <td>0.970085</td>\n      <td>0.965665</td>\n      <td>0.961373</td>\n      <td>0.948718</td>\n      <td>0.952991</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960901</td>\n      <td>0.010921</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>5.799200</td>\n      <td>0.139014</td>\n      <td>0.006072</td>\n      <td>0.000443</td>\n      <td>200</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 200, 'max_depth': 3, 'learnin...</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.944444</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.974249</td>\n      <td>0.948718</td>\n      <td>0.948718</td>\n      <td>0.970085</td>\n      <td>0.978541</td>\n      <td>0.952790</td>\n      <td>0.961761</td>\n      <td>0.011091</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>7.593565</td>\n      <td>0.149349</td>\n      <td>0.007941</td>\n      <td>0.000250</td>\n      <td>500</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 500, 'max_depth': 3, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.957265</td>\n      <td>0.944444</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.970085</td>\n      <td>0.957082</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.952991</td>\n      <td>0.970085</td>\n      <td>0.978541</td>\n      <td>0.948498</td>\n      <td>0.962043</td>\n      <td>0.011468</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>4.449910</td>\n      <td>0.094663</td>\n      <td>0.005138</td>\n      <td>0.000719</td>\n      <td>100</td>\n      <td>5</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.918803</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.948498</td>\n      <td>0.961373</td>\n      <td>0.935897</td>\n      <td>0.948718</td>\n      <td>0.940171</td>\n      <td>0.914163</td>\n      <td>0.939914</td>\n      <td>0.918803</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.948498</td>\n      <td>0.948498</td>\n      <td>0.942354</td>\n      <td>0.014136</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>5.000307</td>\n      <td>0.099910</td>\n      <td>0.005739</td>\n      <td>0.000855</td>\n      <td>200</td>\n      <td>5</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 200, 'max_depth': 5, 'learnin...</td>\n      <td>0.914530</td>\n      <td>0.940171</td>\n      <td>0.944444</td>\n      <td>0.948498</td>\n      <td>0.961373</td>\n      <td>0.940171</td>\n      <td>0.952991</td>\n      <td>0.948718</td>\n      <td>0.914163</td>\n      <td>0.948498</td>\n      <td>0.923077</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.945211</td>\n      <td>0.015940</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>6.790469</td>\n      <td>0.105016</td>\n      <td>0.007607</td>\n      <td>0.000490</td>\n      <td>500</td>\n      <td>5</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 500, 'max_depth': 5, 'learnin...</td>\n      <td>0.905983</td>\n      <td>0.935897</td>\n      <td>0.944444</td>\n      <td>0.944206</td>\n      <td>0.948498</td>\n      <td>0.940171</td>\n      <td>0.948718</td>\n      <td>0.948718</td>\n      <td>0.935622</td>\n      <td>0.939914</td>\n      <td>0.927350</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.952790</td>\n      <td>0.948498</td>\n      <td>0.942356</td>\n      <td>0.012649</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1.857389</td>\n      <td>0.018594</td>\n      <td>0.002236</td>\n      <td>0.000404</td>\n      <td>100</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 100, 'max_depth': 1, 'learnin...</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.939914</td>\n      <td>0.948498</td>\n      <td>0.957265</td>\n      <td>0.914530</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.948498</td>\n      <td>0.953477</td>\n      <td>0.014495</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>3.680042</td>\n      <td>0.039397</td>\n      <td>0.003803</td>\n      <td>0.000400</td>\n      <td>200</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 200, 'max_depth': 1, 'learnin...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.944206</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.948498</td>\n      <td>0.961373</td>\n      <td>0.961538</td>\n      <td>0.923077</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.957479</td>\n      <td>0.011732</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>9.007250</td>\n      <td>0.083105</td>\n      <td>0.008608</td>\n      <td>0.000801</td>\n      <td>500</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 500, 'max_depth': 1, 'learnin...</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.961538</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.969957</td>\n      <td>0.952991</td>\n      <td>0.931624</td>\n      <td>0.961538</td>\n      <td>0.974249</td>\n      <td>0.952790</td>\n      <td>0.957480</td>\n      <td>0.010760</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>3.071392</td>\n      <td>0.048642</td>\n      <td>0.003536</td>\n      <td>0.000499</td>\n      <td>100</td>\n      <td>2</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 100, 'max_depth': 2, 'learnin...</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.965665</td>\n      <td>0.952991</td>\n      <td>0.952991</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.965467</td>\n      <td>0.008333</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>5.292208</td>\n      <td>0.090125</td>\n      <td>0.005738</td>\n      <td>0.000680</td>\n      <td>200</td>\n      <td>2</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 200, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.969957</td>\n      <td>0.961373</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.966895</td>\n      <td>0.007563</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>7.099225</td>\n      <td>0.125361</td>\n      <td>0.007607</td>\n      <td>0.000490</td>\n      <td>500</td>\n      <td>2</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 500, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.969957</td>\n      <td>0.961373</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.967180</td>\n      <td>0.007911</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>3.769631</td>\n      <td>0.100205</td>\n      <td>0.004271</td>\n      <td>0.000443</td>\n      <td>100</td>\n      <td>3</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.952991</td>\n      <td>0.982906</td>\n      <td>0.970085</td>\n      <td>0.957082</td>\n      <td>0.961373</td>\n      <td>0.952991</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.962614</td>\n      <td>0.008900</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>4.320758</td>\n      <td>0.092689</td>\n      <td>0.004871</td>\n      <td>0.000340</td>\n      <td>200</td>\n      <td>3</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 200, 'max_depth': 3, 'learnin...</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.952991</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.948498</td>\n      <td>0.963184</td>\n      <td>0.009222</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>6.070847</td>\n      <td>0.095832</td>\n      <td>0.007140</td>\n      <td>0.001148</td>\n      <td>500</td>\n      <td>3</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 500, 'max_depth': 3, 'learnin...</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.952991</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.952991</td>\n      <td>0.952991</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.961373</td>\n      <td>0.963188</td>\n      <td>0.009466</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>3.162341</td>\n      <td>0.056497</td>\n      <td>0.003870</td>\n      <td>0.000340</td>\n      <td>100</td>\n      <td>5</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.914530</td>\n      <td>0.952991</td>\n      <td>0.948718</td>\n      <td>0.952790</td>\n      <td>0.974249</td>\n      <td>0.944444</td>\n      <td>0.965812</td>\n      <td>0.957265</td>\n      <td>0.927039</td>\n      <td>0.927039</td>\n      <td>0.918803</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.948498</td>\n      <td>0.952790</td>\n      <td>0.947488</td>\n      <td>0.017367</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>3.748470</td>\n      <td>0.083335</td>\n      <td>0.004337</td>\n      <td>0.000472</td>\n      <td>200</td>\n      <td>5</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 200, 'max_depth': 5, 'learnin...</td>\n      <td>0.914530</td>\n      <td>0.952991</td>\n      <td>0.948718</td>\n      <td>0.957082</td>\n      <td>0.952790</td>\n      <td>0.944444</td>\n      <td>0.957265</td>\n      <td>0.944444</td>\n      <td>0.922747</td>\n      <td>0.944206</td>\n      <td>0.927350</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.948498</td>\n      <td>0.948498</td>\n      <td>0.946916</td>\n      <td>0.015054</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>5.185328</td>\n      <td>0.562457</td>\n      <td>0.005538</td>\n      <td>0.001025</td>\n      <td>500</td>\n      <td>5</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 500, 'max_depth': 5, 'learnin...</td>\n      <td>0.918803</td>\n      <td>0.952991</td>\n      <td>0.944444</td>\n      <td>0.948498</td>\n      <td>0.948498</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.948718</td>\n      <td>0.935622</td>\n      <td>0.922747</td>\n      <td>0.923077</td>\n      <td>0.952991</td>\n      <td>0.974359</td>\n      <td>0.957082</td>\n      <td>0.952790</td>\n      <td>0.946343</td>\n      <td>0.014822</td>\n      <td>29</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB_model = GradientBoostingClassifier()\n",
    "GB_params = dict()\n",
    "GB_params['n_estimators'] = [100, 200, 500]\n",
    "GB_params['max_depth'] = [1, 2, 3, 5]\n",
    "GB_params['learning_rate'] = [0.1, 0.2, 0.3]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(GB_model, GB_params, n_iter=36, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "GB = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(GB.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[3]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les paramètres testés pour ce modèle sont :\n",
    "Le training rate qui correspond à la rapidité d'apprentissage du modèle. Le lien ci-dessous explique plus en détail l'importance de cet hyperparamètre. Dans notre cas, une valeur de 0.3 semble donner les meilleurs résultats.\n",
    "> https://www.machinelearningplus.com/machine-learning/an-introduction-to-gradient-boosting-decision-trees/#:~:text=Using%20a%20low%20learning%20rate,0.3%20gives%20the%20best%20results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification par machine à vecteurs de support"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 12 is smaller than n_iter=50. Running 12 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_gamma  \\\n0        0.101892      0.007192         0.031896        0.003246        0.01   \n1        0.066660      0.002896         0.029627        0.005381       0.001   \n2        0.090549      0.003968         0.033030        0.002253      0.0001   \n3        0.106763      0.006656         0.031829        0.001425        0.01   \n4        0.041638      0.001585         0.019217        0.000910       0.001   \n5        0.043640      0.001358         0.026024        0.000817      0.0001   \n6        0.117907      0.002564         0.032162        0.001148        0.01   \n7        0.039703      0.001248         0.018083        0.000680       0.001   \n8        0.033497      0.001823         0.021686        0.000597      0.0001   \n9        0.114771      0.003094         0.032296        0.001063        0.01   \n10       0.040103      0.001653         0.017683        0.001012       0.001   \n11       0.026425      0.002473         0.015454        0.003082      0.0001   \n\n   param_C                       params  split0_test_score  split1_test_score  \\\n0      0.1    {'gamma': 0.01, 'C': 0.1}           0.106838           0.111111   \n1      0.1   {'gamma': 0.001, 'C': 0.1}           0.944444           0.952991   \n2      0.1  {'gamma': 0.0001, 'C': 0.1}           0.773504           0.816239   \n3      0.5    {'gamma': 0.01, 'C': 0.5}           0.209402           0.200855   \n4      0.5   {'gamma': 0.001, 'C': 0.5}           0.982906           0.991453   \n5      0.5  {'gamma': 0.0001, 'C': 0.5}           0.944444           0.944444   \n6        1      {'gamma': 0.01, 'C': 1}           0.696581           0.739316   \n7        1     {'gamma': 0.001, 'C': 1}           0.991453           0.991453   \n8        1    {'gamma': 0.0001, 'C': 1}           0.961538           0.957265   \n9        2      {'gamma': 0.01, 'C': 2}           0.730769           0.782051   \n10       2     {'gamma': 0.001, 'C': 2}           0.987179           0.991453   \n11       2    {'gamma': 0.0001, 'C': 2}           0.982906           0.970085   \n\n    split2_test_score  split3_test_score  split4_test_score  \\\n0            0.111111           0.111588           0.111588   \n1            0.948718           0.961373           0.935622   \n2            0.799145           0.763948           0.793991   \n3            0.226496           0.193133           0.206009   \n4            0.978632           0.987124           0.987124   \n5            0.940171           0.961373           0.944206   \n6            0.782051           0.768240           0.712446   \n7            0.987179           0.991416           0.991416   \n8            0.970085           0.974249           0.952790   \n9            0.811966           0.781116           0.733906   \n10           0.987179           0.991416           0.991416   \n11           0.978632           0.982833           0.982833   \n\n    split5_test_score  split6_test_score  split7_test_score  \\\n0            0.106838           0.111111           0.111111   \n1            0.948718           0.961538           0.957265   \n2            0.773504           0.803419           0.786325   \n3            0.213675           0.226496           0.196581   \n4            0.982906           1.000000           0.982906   \n5            0.935897           0.970085           0.944444   \n6            0.786325           0.760684           0.735043   \n7            0.982906           1.000000           0.995726   \n8            0.952991           0.978632           0.965812   \n9            0.807692           0.786325           0.760684   \n10           0.982906           0.995726           0.991453   \n11           0.970085           0.995726           0.974359   \n\n    split8_test_score  split9_test_score  split10_test_score  \\\n0            0.111588           0.111588            0.106838   \n1            0.944206           0.965665            0.940171   \n2            0.785408           0.798283            0.799145   \n3            0.171674           0.197425            0.179487   \n4            0.969957           0.987124            0.978632   \n5            0.939914           0.961373            0.948718   \n6            0.721030           0.798283            0.705128   \n7            0.978541           0.995708            0.987179   \n8            0.961373           0.965665            0.952991   \n9            0.729614           0.824034            0.709402   \n10           0.978541           0.995708            0.987179   \n11           0.969957           0.969957            0.974359   \n\n    split11_test_score  split12_test_score  split13_test_score  \\\n0             0.111111            0.111111            0.111588   \n1             0.961538            0.957265            0.961373   \n2             0.807692            0.790598            0.763948   \n3             0.205128            0.213675            0.206009   \n4             0.982906            0.982906            0.982833   \n5             0.957265            0.965812            0.948498   \n6             0.726496            0.752137            0.763948   \n7             0.987179            0.987179            0.991416   \n8             0.974359            0.974359            0.974249   \n9             0.756410            0.769231            0.789700   \n10            0.987179            0.995726            0.991416   \n11            0.974359            0.974359            0.978541   \n\n    split14_test_score  mean_test_score  std_test_score  rank_test_score  \n0             0.111588         0.110447        0.001817               12  \n1             0.944206         0.952340        0.008972                6  \n2             0.776824         0.788798        0.015323                8  \n3             0.197425         0.202898        0.014457               11  \n4             0.995708         0.984875        0.006970                3  \n5             0.935622         0.949485        0.010630                7  \n6             0.755365         0.746872        0.029679               10  \n7             0.995708         0.990297        0.005294                1  \n8             0.957082         0.964896        0.008733                5  \n9             0.772532         0.769695        0.032091                9  \n10            0.995708         0.990013        0.004861                2  \n11            0.974249         0.976883        0.006776                4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_gamma</th>\n      <th>param_C</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.101892</td>\n      <td>0.007192</td>\n      <td>0.031896</td>\n      <td>0.003246</td>\n      <td>0.01</td>\n      <td>0.1</td>\n      <td>{'gamma': 0.01, 'C': 0.1}</td>\n      <td>0.106838</td>\n      <td>0.111111</td>\n      <td>0.111111</td>\n      <td>0.111588</td>\n      <td>0.111588</td>\n      <td>0.106838</td>\n      <td>0.111111</td>\n      <td>0.111111</td>\n      <td>0.111588</td>\n      <td>0.111588</td>\n      <td>0.106838</td>\n      <td>0.111111</td>\n      <td>0.111111</td>\n      <td>0.111588</td>\n      <td>0.111588</td>\n      <td>0.110447</td>\n      <td>0.001817</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.066660</td>\n      <td>0.002896</td>\n      <td>0.029627</td>\n      <td>0.005381</td>\n      <td>0.001</td>\n      <td>0.1</td>\n      <td>{'gamma': 0.001, 'C': 0.1}</td>\n      <td>0.944444</td>\n      <td>0.952991</td>\n      <td>0.948718</td>\n      <td>0.961373</td>\n      <td>0.935622</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.957265</td>\n      <td>0.944206</td>\n      <td>0.965665</td>\n      <td>0.940171</td>\n      <td>0.961538</td>\n      <td>0.957265</td>\n      <td>0.961373</td>\n      <td>0.944206</td>\n      <td>0.952340</td>\n      <td>0.008972</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.090549</td>\n      <td>0.003968</td>\n      <td>0.033030</td>\n      <td>0.002253</td>\n      <td>0.0001</td>\n      <td>0.1</td>\n      <td>{'gamma': 0.0001, 'C': 0.1}</td>\n      <td>0.773504</td>\n      <td>0.816239</td>\n      <td>0.799145</td>\n      <td>0.763948</td>\n      <td>0.793991</td>\n      <td>0.773504</td>\n      <td>0.803419</td>\n      <td>0.786325</td>\n      <td>0.785408</td>\n      <td>0.798283</td>\n      <td>0.799145</td>\n      <td>0.807692</td>\n      <td>0.790598</td>\n      <td>0.763948</td>\n      <td>0.776824</td>\n      <td>0.788798</td>\n      <td>0.015323</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.106763</td>\n      <td>0.006656</td>\n      <td>0.031829</td>\n      <td>0.001425</td>\n      <td>0.01</td>\n      <td>0.5</td>\n      <td>{'gamma': 0.01, 'C': 0.5}</td>\n      <td>0.209402</td>\n      <td>0.200855</td>\n      <td>0.226496</td>\n      <td>0.193133</td>\n      <td>0.206009</td>\n      <td>0.213675</td>\n      <td>0.226496</td>\n      <td>0.196581</td>\n      <td>0.171674</td>\n      <td>0.197425</td>\n      <td>0.179487</td>\n      <td>0.205128</td>\n      <td>0.213675</td>\n      <td>0.206009</td>\n      <td>0.197425</td>\n      <td>0.202898</td>\n      <td>0.014457</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.041638</td>\n      <td>0.001585</td>\n      <td>0.019217</td>\n      <td>0.000910</td>\n      <td>0.001</td>\n      <td>0.5</td>\n      <td>{'gamma': 0.001, 'C': 0.5}</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.978632</td>\n      <td>0.987124</td>\n      <td>0.987124</td>\n      <td>0.982906</td>\n      <td>1.000000</td>\n      <td>0.982906</td>\n      <td>0.969957</td>\n      <td>0.987124</td>\n      <td>0.978632</td>\n      <td>0.982906</td>\n      <td>0.982906</td>\n      <td>0.982833</td>\n      <td>0.995708</td>\n      <td>0.984875</td>\n      <td>0.006970</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.043640</td>\n      <td>0.001358</td>\n      <td>0.026024</td>\n      <td>0.000817</td>\n      <td>0.0001</td>\n      <td>0.5</td>\n      <td>{'gamma': 0.0001, 'C': 0.5}</td>\n      <td>0.944444</td>\n      <td>0.944444</td>\n      <td>0.940171</td>\n      <td>0.961373</td>\n      <td>0.944206</td>\n      <td>0.935897</td>\n      <td>0.970085</td>\n      <td>0.944444</td>\n      <td>0.939914</td>\n      <td>0.961373</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.948498</td>\n      <td>0.935622</td>\n      <td>0.949485</td>\n      <td>0.010630</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.117907</td>\n      <td>0.002564</td>\n      <td>0.032162</td>\n      <td>0.001148</td>\n      <td>0.01</td>\n      <td>1</td>\n      <td>{'gamma': 0.01, 'C': 1}</td>\n      <td>0.696581</td>\n      <td>0.739316</td>\n      <td>0.782051</td>\n      <td>0.768240</td>\n      <td>0.712446</td>\n      <td>0.786325</td>\n      <td>0.760684</td>\n      <td>0.735043</td>\n      <td>0.721030</td>\n      <td>0.798283</td>\n      <td>0.705128</td>\n      <td>0.726496</td>\n      <td>0.752137</td>\n      <td>0.763948</td>\n      <td>0.755365</td>\n      <td>0.746872</td>\n      <td>0.029679</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.039703</td>\n      <td>0.001248</td>\n      <td>0.018083</td>\n      <td>0.000680</td>\n      <td>0.001</td>\n      <td>1</td>\n      <td>{'gamma': 0.001, 'C': 1}</td>\n      <td>0.991453</td>\n      <td>0.991453</td>\n      <td>0.987179</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.982906</td>\n      <td>1.000000</td>\n      <td>0.995726</td>\n      <td>0.978541</td>\n      <td>0.995708</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.991416</td>\n      <td>0.995708</td>\n      <td>0.990297</td>\n      <td>0.005294</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.033497</td>\n      <td>0.001823</td>\n      <td>0.021686</td>\n      <td>0.000597</td>\n      <td>0.0001</td>\n      <td>1</td>\n      <td>{'gamma': 0.0001, 'C': 1}</td>\n      <td>0.961538</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.952790</td>\n      <td>0.952991</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.961373</td>\n      <td>0.965665</td>\n      <td>0.952991</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.964896</td>\n      <td>0.008733</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.114771</td>\n      <td>0.003094</td>\n      <td>0.032296</td>\n      <td>0.001063</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>{'gamma': 0.01, 'C': 2}</td>\n      <td>0.730769</td>\n      <td>0.782051</td>\n      <td>0.811966</td>\n      <td>0.781116</td>\n      <td>0.733906</td>\n      <td>0.807692</td>\n      <td>0.786325</td>\n      <td>0.760684</td>\n      <td>0.729614</td>\n      <td>0.824034</td>\n      <td>0.709402</td>\n      <td>0.756410</td>\n      <td>0.769231</td>\n      <td>0.789700</td>\n      <td>0.772532</td>\n      <td>0.769695</td>\n      <td>0.032091</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.040103</td>\n      <td>0.001653</td>\n      <td>0.017683</td>\n      <td>0.001012</td>\n      <td>0.001</td>\n      <td>2</td>\n      <td>{'gamma': 0.001, 'C': 2}</td>\n      <td>0.987179</td>\n      <td>0.991453</td>\n      <td>0.987179</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.991453</td>\n      <td>0.978541</td>\n      <td>0.995708</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.995726</td>\n      <td>0.991416</td>\n      <td>0.995708</td>\n      <td>0.990013</td>\n      <td>0.004861</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.026425</td>\n      <td>0.002473</td>\n      <td>0.015454</td>\n      <td>0.003082</td>\n      <td>0.0001</td>\n      <td>2</td>\n      <td>{'gamma': 0.0001, 'C': 2}</td>\n      <td>0.982906</td>\n      <td>0.970085</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.970085</td>\n      <td>0.995726</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.974249</td>\n      <td>0.976883</td>\n      <td>0.006776</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "SVC_model = svm.SVC()\n",
    "\n",
    "SVC_params = dict()\n",
    "SVC_params['gamma'] = [0.01, 0.001, 0.0001]\n",
    "SVC_params['C'] = [0.1, 0.5, 1, 2]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(SVC_model, SVC_params, n_iter=50, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "SVC = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(SVC.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[4]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification par voisin le plus proche"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        0.000734  4.425872e-04         0.019017        0.001864   \n1        0.010543  5.765620e-03         0.019151        0.002581   \n2        0.001268  1.570709e-03         0.013212        0.002590   \n3        0.000934  5.739587e-04         0.004271        0.000773   \n4        0.001801  1.642605e-03         0.011744        0.001808   \n5        0.000801  4.003526e-04         0.005939        0.001124   \n6        0.017549  1.148033e-03         0.043239        0.003490   \n7        0.006739  1.063587e-03         0.016081        0.001182   \n8        0.026758  2.381772e-03         0.177027        0.004956   \n9        0.000401  4.906610e-04         0.017949        0.000773   \n10       0.000734  5.740307e-04         0.018884        0.001205   \n11       0.017349  4.719324e-04         0.036700        0.001852   \n12       0.000734  4.426543e-04         0.013812        0.000654   \n13       0.001001  1.367303e-07         0.016281        0.000929   \n14       0.026624  2.680832e-03         0.101826        0.002544   \n15       0.009275  4.425585e-04         0.016482        0.000619   \n16       0.000801  4.003446e-04         0.018817        0.000981   \n17       0.013079  9.292771e-04         0.020886        0.001025   \n18       0.000667  4.718087e-04         0.005739        0.000772   \n19       0.000734  4.425872e-04         0.005472        0.000499   \n20       0.000601  4.903167e-04         0.005739        0.000443   \n21       0.000801  4.003526e-04         0.018950        0.001844   \n22       0.025623  1.452818e-03         0.062323        0.001950   \n23       0.000667  4.718649e-04         0.005472        0.000499   \n24       0.013679  6.997615e-04         0.029493        0.001025   \n25       0.025644  2.826426e-03         0.156548        0.018541   \n26       0.000667  4.719213e-04         0.019219        0.002819   \n27       0.006472  4.993507e-04         0.019618        0.001021   \n28       0.013145  4.992976e-04         0.022420        0.001144   \n29       0.012878  1.500870e-03         0.022487        0.001025   \n30       0.009342  1.300783e-03         0.020819        0.000833   \n31       0.019351  4.031418e-03         0.038101        0.005738   \n32       0.013746  1.063453e-03         0.033230        0.001223   \n33       0.006673  7.895058e-04         0.022020        0.001864   \n34       0.000667  4.718537e-04         0.010943        0.000772   \n35       0.000934  2.496470e-04         0.011410        0.003140   \n36       0.000801  4.003764e-04         0.019084        0.002697   \n37       0.001268  1.570593e-03         0.003737        0.000443   \n38       0.000801  4.003923e-04         0.019217        0.001223   \n39       0.000801  4.003764e-04         0.010476        0.000499   \n40       0.018248  2.184867e-03         0.030597        0.004488   \n41       0.000801  4.005116e-04         0.010743        0.000772   \n42       0.000867  3.402280e-04         0.005538        0.000619   \n43       0.000801  4.003924e-04         0.010876        0.000958   \n44       0.000795  3.978898e-04         0.011778        0.000911   \n45       0.000800  4.001637e-04         0.011145        0.001638   \n46       0.000867  3.401118e-04         0.017217        0.001412   \n47       0.012047  1.887204e-03         0.030629        0.008695   \n48       0.000868  3.403091e-04         0.013412        0.000801   \n49       0.015632  2.392356e-03         0.033434        0.010154   \n\n   param_weights param_p param_n_neighbors param_leaf_size param_algorithm  \\\n0        uniform       1                16              30            auto   \n1       distance       1                 6              10       ball_tree   \n2       distance       1                 1              10           brute   \n3       distance       2                 1              30            auto   \n4        uniform       2                 6              30           brute   \n5       distance       2                11               1            auto   \n6        uniform       2                11              10         kd_tree   \n7       distance       1                 1              30       ball_tree   \n8        uniform       2                16               1         kd_tree   \n9        uniform       1                 6              30           brute   \n10       uniform       1                16              10           brute   \n11      distance       2                16              10         kd_tree   \n12      distance       1                16              10            auto   \n13       uniform       1                 1              10            auto   \n14       uniform       1                16               1         kd_tree   \n15      distance       2                 1              10       ball_tree   \n16       uniform       1                16              10            auto   \n17      distance       2                 1               1       ball_tree   \n18      distance       2                16              10            auto   \n19      distance       2                11               1           brute   \n20      distance       2                16              30            auto   \n21       uniform       1                 6               1            auto   \n22      distance       1                 1               1         kd_tree   \n23      distance       2                11              30           brute   \n24       uniform       1                11               1       ball_tree   \n25       uniform       2                11               1         kd_tree   \n26       uniform       1                11               1           brute   \n27       uniform       2                 1              30       ball_tree   \n28      distance       1                 6               1       ball_tree   \n29       uniform       2                 1              30         kd_tree   \n30       uniform       2                 1              10       ball_tree   \n31      distance       2                 6              10         kd_tree   \n32       uniform       2                11               1       ball_tree   \n33       uniform       1                 6              30       ball_tree   \n34       uniform       2                11              30           brute   \n35       uniform       2                16               1            auto   \n36       uniform       1                11              30            auto   \n37      distance       2                 1              10            auto   \n38       uniform       1                 6              10           brute   \n39       uniform       2                16              30           brute   \n40      distance       1                 1              10         kd_tree   \n41       uniform       2                16              10           brute   \n42      distance       2                 6              10           brute   \n43       uniform       2                 6               1           brute   \n44      distance       1                 1              30            auto   \n45       uniform       2                11              10           brute   \n46       uniform       1                 1              10           brute   \n47      distance       2                11               1       ball_tree   \n48      distance       1                 6              10           brute   \n49       uniform       1                 1              10         kd_tree   \n\n                                               params  split0_test_score  \\\n0   {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.948718   \n1   {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.987179   \n2   {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n3   {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n4   {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.982906   \n5   {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n6   {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n7   {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n8   {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.944444   \n9   {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n10  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.948718   \n11  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.957265   \n12  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.961538   \n13  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n14  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.944444   \n15  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n16  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.948718   \n17  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n18  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.957265   \n19  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n20  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.957265   \n21  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n22  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n23  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n24  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.965812   \n25  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n26  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.965812   \n27  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.987179   \n28  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.987179   \n29  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.987179   \n30  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.987179   \n31  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.982906   \n32  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n33  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.982906   \n34  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n35  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.940171   \n36  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.965812   \n37  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n38  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n39  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.940171   \n40  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n41  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.940171   \n42  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.982906   \n43  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.982906   \n44  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n45  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n46  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n47  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n48  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.982906   \n49  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.961538           0.970085           0.969957   \n1            0.970085           0.974359           0.982833   \n2            0.978632           0.970085           0.987124   \n3            0.987179           0.978632           0.978541   \n4            0.991453           0.970085           0.965665   \n5            0.982906           0.965812           0.978541   \n6            0.978632           0.965812           0.974249   \n7            0.978632           0.970085           0.987124   \n8            0.970085           0.965812           0.969957   \n9            0.974359           0.965812           0.969957   \n10           0.961538           0.970085           0.969957   \n11           0.970085           0.970085           0.974249   \n12           0.961538           0.970085           0.974249   \n13           0.978632           0.970085           0.987124   \n14           0.961538           0.970085           0.974249   \n15           0.987179           0.978632           0.978541   \n16           0.961538           0.970085           0.969957   \n17           0.987179           0.978632           0.978541   \n18           0.970085           0.970085           0.974249   \n19           0.982906           0.965812           0.978541   \n20           0.970085           0.970085           0.974249   \n21           0.974359           0.965812           0.969957   \n22           0.978632           0.970085           0.987124   \n23           0.982906           0.965812           0.978541   \n24           0.974359           0.957265           0.965665   \n25           0.978632           0.965812           0.974249   \n26           0.970085           0.957265           0.965665   \n27           0.987179           0.978632           0.978541   \n28           0.970085           0.974359           0.982833   \n29           0.987179           0.978632           0.978541   \n30           0.987179           0.978632           0.978541   \n31           0.991453           0.970085           0.974249   \n32           0.978632           0.965812           0.974249   \n33           0.974359           0.965812           0.969957   \n34           0.978632           0.965812           0.974249   \n35           0.970085           0.965812           0.969957   \n36           0.970085           0.957265           0.965665   \n37           0.987179           0.978632           0.978541   \n38           0.974359           0.965812           0.969957   \n39           0.970085           0.965812           0.969957   \n40           0.978632           0.970085           0.987124   \n41           0.970085           0.965812           0.969957   \n42           0.991453           0.970085           0.974249   \n43           0.991453           0.970085           0.965665   \n44           0.978632           0.970085           0.987124   \n45           0.978632           0.965812           0.974249   \n46           0.978632           0.970085           0.987124   \n47           0.982906           0.965812           0.978541   \n48           0.970085           0.974359           0.982833   \n49           0.978632           0.970085           0.987124   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.957082           0.961538           0.970085   \n1            0.982833           0.982906           0.995726   \n2            0.982833           0.974359           0.982906   \n3            0.991416           0.978632           1.000000   \n4            0.995708           0.978632           0.995726   \n5            0.969957           0.974359           0.982906   \n6            0.965665           0.965812           0.978632   \n7            0.982833           0.974359           0.982906   \n8            0.957082           0.970085           0.974359   \n9            0.978541           0.965812           0.982906   \n10           0.957082           0.961538           0.970085   \n11           0.957082           0.978632           0.978632   \n12           0.965665           0.974359           0.978632   \n13           0.982833           0.974359           0.982906   \n14           0.957082           0.961538           0.970085   \n15           0.991416           0.978632           1.000000   \n16           0.957082           0.961538           0.970085   \n17           0.991416           0.978632           1.000000   \n18           0.957082           0.978632           0.978632   \n19           0.969957           0.974359           0.982906   \n20           0.957082           0.978632           0.978632   \n21           0.978541           0.965812           0.982906   \n22           0.982833           0.974359           0.982906   \n23           0.969957           0.974359           0.982906   \n24           0.957082           0.957265           0.974359   \n25           0.965665           0.965812           0.978632   \n26           0.957082           0.961538           0.974359   \n27           0.991416           0.978632           1.000000   \n28           0.982833           0.982906           0.991453   \n29           0.991416           0.978632           1.000000   \n30           0.991416           0.978632           1.000000   \n31           0.995708           0.982906           1.000000   \n32           0.965665           0.965812           0.978632   \n33           0.982833           0.965812           0.982906   \n34           0.965665           0.965812           0.978632   \n35           0.957082           0.970085           0.974359   \n36           0.957082           0.961538           0.974359   \n37           0.991416           0.978632           1.000000   \n38           0.978541           0.965812           0.982906   \n39           0.957082           0.970085           0.974359   \n40           0.982833           0.974359           0.982906   \n41           0.957082           0.970085           0.974359   \n42           0.995708           0.982906           1.000000   \n43           0.995708           0.978632           0.995726   \n44           0.982833           0.974359           0.982906   \n45           0.965665           0.965812           0.978632   \n46           0.982833           0.974359           0.982906   \n47           0.969957           0.974359           0.982906   \n48           0.982833           0.982906           0.991453   \n49           0.982833           0.974359           0.982906   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.944444           0.952790           0.969957   \n1            0.991453           0.957082           0.991416   \n2            0.995726           0.965665           0.982833   \n3            1.000000           0.978541           0.987124   \n4            0.974359           0.969957           0.978541   \n5            0.974359           0.969957           0.982833   \n6            0.974359           0.961373           0.974249   \n7            0.991453           0.965665           0.982833   \n8            0.952991           0.957082           0.965665   \n9            0.974359           0.948498           0.982833   \n10           0.944444           0.952790           0.969957   \n11           0.974359           0.961373           0.969957   \n12           0.961538           0.961373           0.978541   \n13           0.995726           0.965665           0.982833   \n14           0.948718           0.952790           0.969957   \n15           1.000000           0.978541           0.987124   \n16           0.944444           0.952790           0.969957   \n17           1.000000           0.978541           0.987124   \n18           0.974359           0.961373           0.969957   \n19           0.974359           0.969957           0.982833   \n20           0.974359           0.961373           0.969957   \n21           0.974359           0.948498           0.982833   \n22           0.991453           0.965665           0.982833   \n23           0.974359           0.969957           0.982833   \n24           0.961538           0.957082           0.978541   \n25           0.974359           0.961373           0.974249   \n26           0.961538           0.961373           0.978541   \n27           1.000000           0.978541           0.987124   \n28           0.991453           0.957082           0.991416   \n29           1.000000           0.978541           0.987124   \n30           1.000000           0.978541           0.987124   \n31           0.995726           0.969957           0.978541   \n32           0.974359           0.965665           0.974249   \n33           0.974359           0.948498           0.982833   \n34           0.974359           0.965665           0.974249   \n35           0.952991           0.957082           0.965665   \n36           0.961538           0.961373           0.978541   \n37           1.000000           0.978541           0.987124   \n38           0.974359           0.948498           0.982833   \n39           0.952991           0.957082           0.965665   \n40           0.991453           0.965665           0.982833   \n41           0.952991           0.957082           0.965665   \n42           0.995726           0.969957           0.978541   \n43           0.974359           0.969957           0.978541   \n44           0.995726           0.965665           0.982833   \n45           0.974359           0.965665           0.974249   \n46           0.995726           0.965665           0.982833   \n47           0.974359           0.969957           0.982833   \n48           0.991453           0.957082           0.991416   \n49           0.991453           0.965665           0.982833   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.948718            0.965812            0.961538   \n1             0.965812            0.974359            0.978632   \n2             0.974359            0.974359            0.974359   \n3             0.987179            0.987179            1.000000   \n4             0.978632            0.978632            0.978632   \n5             0.982906            0.974359            0.987179   \n6             0.965812            0.974359            0.978632   \n7             0.974359            0.974359            0.974359   \n8             0.965812            0.970085            0.965812   \n9             0.965812            0.961538            0.974359   \n10            0.948718            0.965812            0.961538   \n11            0.974359            0.974359            0.974359   \n12            0.961538            0.965812            0.974359   \n13            0.974359            0.974359            0.974359   \n14            0.948718            0.965812            0.961538   \n15            0.987179            0.987179            1.000000   \n16            0.948718            0.965812            0.961538   \n17            0.987179            0.987179            1.000000   \n18            0.974359            0.974359            0.974359   \n19            0.982906            0.974359            0.987179   \n20            0.974359            0.974359            0.974359   \n21            0.965812            0.961538            0.974359   \n22            0.974359            0.974359            0.974359   \n23            0.982906            0.974359            0.987179   \n24            0.961538            0.961538            0.965812   \n25            0.965812            0.974359            0.978632   \n26            0.957265            0.961538            0.965812   \n27            0.987179            0.987179            1.000000   \n28            0.965812            0.974359            0.978632   \n29            0.987179            0.987179            1.000000   \n30            0.987179            0.987179            1.000000   \n31            0.991453            0.982906            0.991453   \n32            0.965812            0.974359            0.978632   \n33            0.965812            0.961538            0.974359   \n34            0.965812            0.974359            0.982906   \n35            0.965812            0.970085            0.965812   \n36            0.957265            0.961538            0.965812   \n37            0.987179            0.987179            1.000000   \n38            0.965812            0.961538            0.974359   \n39            0.965812            0.970085            0.965812   \n40            0.974359            0.974359            0.974359   \n41            0.965812            0.970085            0.965812   \n42            0.991453            0.982906            0.991453   \n43            0.978632            0.978632            0.978632   \n44            0.974359            0.974359            0.974359   \n45            0.965812            0.974359            0.982906   \n46            0.974359            0.974359            0.974359   \n47            0.982906            0.974359            0.982906   \n48            0.965812            0.974359            0.978632   \n49            0.974359            0.974359            0.974359   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.969957            0.957082         0.960620        0.008569   \n1             0.982833            0.987124         0.980309        0.010108   \n2             0.978541            0.987124         0.979169        0.007303   \n3             0.991416            0.991416         0.988296        0.007410   \n4             0.974249            0.978541         0.979448        0.008604   \n5             0.978541            0.978541         0.976883        0.006013   \n6             0.974249            0.969957         0.970889        0.005895   \n7             0.978541            0.987124         0.978884        0.006710   \n8             0.974249            0.961373         0.964326        0.008067   \n9             0.982833            0.974249         0.972033        0.009103   \n10            0.969957            0.957082         0.960620        0.008569   \n11            0.978541            0.969957         0.970886        0.006869   \n12            0.974249            0.961373         0.968323        0.006588   \n13            0.978541            0.987124         0.979169        0.007303   \n14            0.969957            0.957082         0.960906        0.008940   \n15            0.991416            0.991416         0.988296        0.007410   \n16            0.969957            0.957082         0.960620        0.008569   \n17            0.991416            0.991416         0.988296        0.007410   \n18            0.978541            0.969957         0.970886        0.006869   \n19            0.978541            0.978541         0.976883        0.006013   \n20            0.978541            0.969957         0.970886        0.006869   \n21            0.982833            0.974249         0.972033        0.009103   \n22            0.978541            0.987124         0.978884        0.006710   \n23            0.978541            0.978541         0.976883        0.006013   \n24            0.974249            0.965665         0.965185        0.006961   \n25            0.974249            0.969957         0.970889        0.005895   \n26            0.974249            0.969957         0.965472        0.006516   \n27            0.991416            0.991416         0.988296        0.007410   \n28            0.982833            0.987124         0.980024        0.009722   \n29            0.991416            0.991416         0.988296        0.007410   \n30            0.991416            0.991416         0.988296        0.007410   \n31            0.982833            0.991416         0.985440        0.009099   \n32            0.974249            0.969957         0.971175        0.005519   \n33            0.974249            0.974249         0.972032        0.009238   \n34            0.974249            0.969957         0.971460        0.005987   \n35            0.974249            0.961373         0.964041        0.008806   \n36            0.974249            0.969957         0.965472        0.006516   \n37            0.991416            0.991416         0.988296        0.007410   \n38            0.982833            0.974249         0.972033        0.009103   \n39            0.974249            0.961373         0.964041        0.008806   \n40            0.978541            0.987124         0.978884        0.006710   \n41            0.974249            0.961373         0.964041        0.008806   \n42            0.982833            0.991416         0.985440        0.009099   \n43            0.974249            0.978541         0.979448        0.008604   \n44            0.978541            0.987124         0.979169        0.007303   \n45            0.974249            0.969957         0.971460        0.005987   \n46            0.978541            0.987124         0.979169        0.007303   \n47            0.978541            0.978541         0.976598        0.005606   \n48            0.982833            0.987124         0.979739        0.009570   \n49            0.978541            0.987124         0.978884        0.006710   \n\n    rank_test_score  \n0                48  \n1                10  \n2                15  \n3                 1  \n4                13  \n5                23  \n6                34  \n7                19  \n8                43  \n9                27  \n10               48  \n11               36  \n12               39  \n13               15  \n14               47  \n15                1  \n16               48  \n17                1  \n18               36  \n19               23  \n20               36  \n21               27  \n22               19  \n23               23  \n24               42  \n25               34  \n26               40  \n27                1  \n28               11  \n29                1  \n30                1  \n31                8  \n32               33  \n33               30  \n34               31  \n35               44  \n36               40  \n37                1  \n38               27  \n39               44  \n40               19  \n41               44  \n42                8  \n43               13  \n44               15  \n45               31  \n46               15  \n47               26  \n48               12  \n49               19  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_weights</th>\n      <th>param_p</th>\n      <th>param_n_neighbors</th>\n      <th>param_leaf_size</th>\n      <th>param_algorithm</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000734</td>\n      <td>4.425872e-04</td>\n      <td>0.019017</td>\n      <td>0.001864</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.944444</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960620</td>\n      <td>0.008569</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.010543</td>\n      <td>5.765620e-03</td>\n      <td>0.019151</td>\n      <td>0.002581</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>6</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.991453</td>\n      <td>0.957082</td>\n      <td>0.991416</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.987124</td>\n      <td>0.980309</td>\n      <td>0.010108</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.001268</td>\n      <td>1.570709e-03</td>\n      <td>0.013212</td>\n      <td>0.002590</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.979169</td>\n      <td>0.007303</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000934</td>\n      <td>5.739587e-04</td>\n      <td>0.004271</td>\n      <td>0.000773</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.001801</td>\n      <td>1.642605e-03</td>\n      <td>0.011744</td>\n      <td>0.001808</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>6</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.965665</td>\n      <td>0.995708</td>\n      <td>0.978632</td>\n      <td>0.995726</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.978541</td>\n      <td>0.979448</td>\n      <td>0.008604</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000801</td>\n      <td>4.003526e-04</td>\n      <td>0.005939</td>\n      <td>0.001124</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976883</td>\n      <td>0.006013</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.017549</td>\n      <td>1.148033e-03</td>\n      <td>0.043239</td>\n      <td>0.003490</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.970889</td>\n      <td>0.005895</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.006739</td>\n      <td>1.063587e-03</td>\n      <td>0.016081</td>\n      <td>0.001182</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.026758</td>\n      <td>2.381772e-03</td>\n      <td>0.177027</td>\n      <td>0.004956</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>16</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.944444</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964326</td>\n      <td>0.008067</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.000401</td>\n      <td>4.906610e-04</td>\n      <td>0.017949</td>\n      <td>0.000773</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.974249</td>\n      <td>0.972033</td>\n      <td>0.009103</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.000734</td>\n      <td>5.740307e-04</td>\n      <td>0.018884</td>\n      <td>0.001205</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.944444</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960620</td>\n      <td>0.008569</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.017349</td>\n      <td>4.719324e-04</td>\n      <td>0.036700</td>\n      <td>0.001852</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>16</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.970886</td>\n      <td>0.006869</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.000734</td>\n      <td>4.426543e-04</td>\n      <td>0.013812</td>\n      <td>0.000654</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>16</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.978541</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.968323</td>\n      <td>0.006588</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.001001</td>\n      <td>1.367303e-07</td>\n      <td>0.016281</td>\n      <td>0.000929</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.979169</td>\n      <td>0.007303</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.026624</td>\n      <td>2.680832e-03</td>\n      <td>0.101826</td>\n      <td>0.002544</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.944444</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.948718</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960906</td>\n      <td>0.008940</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.009275</td>\n      <td>4.425585e-04</td>\n      <td>0.016482</td>\n      <td>0.000619</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.000801</td>\n      <td>4.003446e-04</td>\n      <td>0.018817</td>\n      <td>0.000981</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.944444</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960620</td>\n      <td>0.008569</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.013079</td>\n      <td>9.292771e-04</td>\n      <td>0.020886</td>\n      <td>0.001025</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.000667</td>\n      <td>4.718087e-04</td>\n      <td>0.005739</td>\n      <td>0.000772</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>16</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.970886</td>\n      <td>0.006869</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.000734</td>\n      <td>4.425872e-04</td>\n      <td>0.005472</td>\n      <td>0.000499</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976883</td>\n      <td>0.006013</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.000601</td>\n      <td>4.903167e-04</td>\n      <td>0.005739</td>\n      <td>0.000443</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>16</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.970886</td>\n      <td>0.006869</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.000801</td>\n      <td>4.003526e-04</td>\n      <td>0.018950</td>\n      <td>0.001844</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.974249</td>\n      <td>0.972033</td>\n      <td>0.009103</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.025623</td>\n      <td>1.452818e-03</td>\n      <td>0.062323</td>\n      <td>0.001950</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.000667</td>\n      <td>4.718649e-04</td>\n      <td>0.005472</td>\n      <td>0.000499</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976883</td>\n      <td>0.006013</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.013679</td>\n      <td>6.997615e-04</td>\n      <td>0.029493</td>\n      <td>0.001025</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>11</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.957265</td>\n      <td>0.974359</td>\n      <td>0.961538</td>\n      <td>0.957082</td>\n      <td>0.978541</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965185</td>\n      <td>0.006961</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.025644</td>\n      <td>2.826426e-03</td>\n      <td>0.156548</td>\n      <td>0.018541</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.970889</td>\n      <td>0.005895</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.000667</td>\n      <td>4.719213e-04</td>\n      <td>0.019219</td>\n      <td>0.002819</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>11</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.978541</td>\n      <td>0.957265</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.965472</td>\n      <td>0.006516</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.006472</td>\n      <td>4.993507e-04</td>\n      <td>0.019618</td>\n      <td>0.001021</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>1</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.013145</td>\n      <td>4.992976e-04</td>\n      <td>0.022420</td>\n      <td>0.001144</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.991453</td>\n      <td>0.957082</td>\n      <td>0.991416</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.987124</td>\n      <td>0.980024</td>\n      <td>0.009722</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.012878</td>\n      <td>1.500870e-03</td>\n      <td>0.022487</td>\n      <td>0.001025</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>1</td>\n      <td>30</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.009342</td>\n      <td>1.300783e-03</td>\n      <td>0.020819</td>\n      <td>0.000833</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>1</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.019351</td>\n      <td>4.031418e-03</td>\n      <td>0.038101</td>\n      <td>0.005738</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>6</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.995708</td>\n      <td>0.982906</td>\n      <td>1.000000</td>\n      <td>0.995726</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.991453</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.982833</td>\n      <td>0.991416</td>\n      <td>0.985440</td>\n      <td>0.009099</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.013746</td>\n      <td>1.063453e-03</td>\n      <td>0.033230</td>\n      <td>0.001223</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.971175</td>\n      <td>0.005519</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.006673</td>\n      <td>7.895058e-04</td>\n      <td>0.022020</td>\n      <td>0.001864</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.974249</td>\n      <td>0.972032</td>\n      <td>0.009238</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.000667</td>\n      <td>4.718537e-04</td>\n      <td>0.010943</td>\n      <td>0.000772</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.971460</td>\n      <td>0.005987</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.000934</td>\n      <td>2.496470e-04</td>\n      <td>0.011410</td>\n      <td>0.003140</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>16</td>\n      <td>1</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.940171</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964041</td>\n      <td>0.008806</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.000801</td>\n      <td>4.003764e-04</td>\n      <td>0.019084</td>\n      <td>0.002697</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>11</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.978541</td>\n      <td>0.957265</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.965472</td>\n      <td>0.006516</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.001268</td>\n      <td>1.570593e-03</td>\n      <td>0.003737</td>\n      <td>0.000443</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.000801</td>\n      <td>4.003923e-04</td>\n      <td>0.019217</td>\n      <td>0.001223</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.974249</td>\n      <td>0.972033</td>\n      <td>0.009103</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.000801</td>\n      <td>4.003764e-04</td>\n      <td>0.010476</td>\n      <td>0.000499</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>16</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.940171</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964041</td>\n      <td>0.008806</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.018248</td>\n      <td>2.184867e-03</td>\n      <td>0.030597</td>\n      <td>0.004488</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>0.000801</td>\n      <td>4.005116e-04</td>\n      <td>0.010743</td>\n      <td>0.000772</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>16</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.940171</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964041</td>\n      <td>0.008806</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>0.000867</td>\n      <td>3.402280e-04</td>\n      <td>0.005538</td>\n      <td>0.000619</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>6</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.995708</td>\n      <td>0.982906</td>\n      <td>1.000000</td>\n      <td>0.995726</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.991453</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.982833</td>\n      <td>0.991416</td>\n      <td>0.985440</td>\n      <td>0.009099</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>0.000801</td>\n      <td>4.003924e-04</td>\n      <td>0.010876</td>\n      <td>0.000958</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>6</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.965665</td>\n      <td>0.995708</td>\n      <td>0.978632</td>\n      <td>0.995726</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.978541</td>\n      <td>0.979448</td>\n      <td>0.008604</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>0.000795</td>\n      <td>3.978898e-04</td>\n      <td>0.011778</td>\n      <td>0.000911</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.979169</td>\n      <td>0.007303</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>0.000800</td>\n      <td>4.001637e-04</td>\n      <td>0.011145</td>\n      <td>0.001638</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.971460</td>\n      <td>0.005987</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>0.000867</td>\n      <td>3.401118e-04</td>\n      <td>0.017217</td>\n      <td>0.001412</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.979169</td>\n      <td>0.007303</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>0.012047</td>\n      <td>1.887204e-03</td>\n      <td>0.030629</td>\n      <td>0.008695</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976598</td>\n      <td>0.005606</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>0.000868</td>\n      <td>3.403091e-04</td>\n      <td>0.013412</td>\n      <td>0.000801</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>6</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.982906</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.991453</td>\n      <td>0.957082</td>\n      <td>0.991416</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.987124</td>\n      <td>0.979739</td>\n      <td>0.009570</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>0.015632</td>\n      <td>2.392356e-03</td>\n      <td>0.033434</td>\n      <td>0.010154</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_model = KNeighborsClassifier()\n",
    "\n",
    "KNN_params = dict()\n",
    "KNN_params['n_neighbors'] = [i for i in range(1, 20, 5)]\n",
    "KNN_params['weights'] = [\"uniform\", \"distance\"]\n",
    "KNN_params['algorithm'] = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "KNN_params['leaf_size'] = [1, 10, 30]\n",
    "KNN_params['p'] = [1, 2]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(KNN_model, KNN_params, n_iter=50, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "KNN = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(KNN.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[5]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparaison des résultats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAGLCAYAAADXp2mfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA54UlEQVR4nO3dd5xdVbn/8c83QyAJCS2AVwhNxAAmhBJCEEIRxKAYmj8E6SLBey0gqIDSxHIVvAhIkdBBBFFBkCK9KyWhV4kUCSIEpAQIJeH7+2Ptk5xMTmYmzJ7Z56w879drXjN77zM5z8nMPGftVZ4l24QQQmh9faoOIIQQQjkioYcQQiYioYcQQiYioYcQQiYioYcQQiYWquqJl156aa+88spVPX0IIbSkSZMmvWx7mUbXKkvoK6+8MhMnTqzq6UMIoSVJenZe16LLJYQQMhEJPYQQMhEJPYQQMlFZH3oj77//PlOmTOGdd96pOpTQIvr168eQIUPo27dv1aGEULlOE7qks4BtgJdsD2twXcAJwOeAt4G9bN/7YYKZMmUKgwYNYuWVVyb9syHMm21eeeUVpkyZwiqrrFJ1OCFUritdLucAYzu4vjWwWvExHjj1wwbzzjvvMHjw4EjmoUskMXjw4LijC6HQaUK3fSvwnw4esi1wnpM7gSUkffTDBhTJPMyP+H0JYbYyBkWXB56rO55SnJuLpPGSJkqaOHXq1BKeOoQQQk2vDorangBMABg5cmSnhdhXPuTKUp//mZ99vtPHtLW1MXz4cGzT1tbGSSedxKc+9an5fq7jjz+e8ePHM2DAgLmubbbZZjz11FM8++yzs1qY2223Hddffz1vvvnmfD9XV5x99tmccMIJADz66KMMHTqUtrY2xo4dy89+9rMu/RsdvaYrrriCww8/nA8++ID333+f/fffn/3226/U1xBC6FgZCf15YIW64yHFuZbUv39/7r//fgCuueYaDj30UG655Zb5/neOP/54dtttt4bJD2CJJZbgjjvuYOONN+a1117jhRde6E7Yndp7773Ze++9gbRK96abbmLppZeer39jXq/p/fffZ/z48dx9990MGTKEd999l2eeeaZb8drGNn36xMzaZlF2A6szXWmAlSmH11fGX8vlwB5KRgOv2+7Z7NRL3njjDZZccslZx8ceeyzrr78+a621FkceeSQAb731Fp///OcZMWIEw4YN43e/+x0nnngi//rXv9h8883ZfPPNG/7bO++8MxdddBEAl1xyCTvssMMc1xs9F6SW/HrrrccnP/lJJkyYMOv8wIED+cEPfsCIESMYPXo0L774YpdeYxmvadq0acyYMYPBgwcDsMgiizB06FAAXnzxRbbffntGjBjBiBEj+Otf/wrAcccdx7Bhwxg2bBjHH388AM888wxDhw5ljz32YNiwYTz33HPz/H8IIcytK9MWLwQ2A5aWNAU4EugLYPvXwFWkKYuTSdMW9+6pYHvD9OnTWXvttXnnnXd44YUXuPHGGwG49tprefLJJ7n77ruxzbhx47j11luZOnUqyy23HFdemd7dX3/9dRZffHGOO+64DlvBW2yxBfvuuy8zZ87koosuYsKECfzoRz/q8Lk22WQTzjrrLJZaaimmT5/O+uuvz4477sjgwYN56623GD16ND/5yU/43ve+x+mnn85hhx3W4Wst6zUttdRSjBs3jpVWWoktttiCbbbZhl122YU+ffrwrW99i0033ZRLL72UmTNn8uabbzJp0iTOPvts7rrrLmyzwQYbsOmmm7Lkkkvy5JNPcu655zJ69OgO/x9CCHPrNKHb3qWT6wa+XlpEPejBKa91+phF+vXnvCtuBuCBSXezxx578PDDD3Pttddy7bXXss466wDw5ptv8uSTTzJmzBgOOuggDj74YLbZZhvGjBnTpVja2trYeOONueiii5g+fTr1lSfn9VybbLIJJ554IpdeeikAzz33HE8++SSDBw9m4YUXZptttgFgvfXW47rrrus0hjJf0xlnnMFDDz3E9ddfzy9+8Quuu+46zjnnHG688UbOO++8Wa958cUX5/bbb2f77bdn0UUXBWCHHXbgtttum/WmMHr06E7/H0IIc2uqlaLNZsR6o3j55ZeZOnUqtjn00EMbDvTde++9XHXVVRx22GFsscUWHHHEEV3693feeWe23357jjrqqDnOz+u5br75Zq6//nr+9re/MWDAADbbbLNZc7D79u07a4C1ra2NGTNmdPr8Zb+m4cOHM3z4cHbffXdWWWUVzjnnnE6/p71aku8svhDC3CKhd+DpyX9n5syZDB48mM9+9rMcfvjh7LrrrgwcOJDnn3+evn37MmPGDJZaail22203llhiCc444wwABg0axLRp0zoceBwzZgyHHnoou+wy503QvJ7r9ddfZ8kll2TAgAE8/vjj3Hnnnd16fWW9pjfffJOJEyey2WabAXD//fez0korAalr6dRTT+WAAw6Y1eUyZswY9tprLw455BBsc+mll3L++ed3Ob5ll122W6+7p+QwqBZaW1Mn9Ea/sF3pNumOd9+Zzk6fTV0Mtjn33HNpa2tjq6224rHHHmPDDTcE0iDkb37zGyZPnsx3v/td+vTpQ9++fTn11LRQdvz48YwdO5bllluOm266qeFzSeI73/nOXOfn9Vxjx47l17/+NWussQZDhw6d1TXxYZX1mmxzzDHHsN9++9G/f38WXXTRWa3zE044gfHjx3PmmWfS1tbGqaeeyoYbbshee+3FqFGjAPjqV7/KOuusM9fMmHnF16wJPYSqKXWB976RI0e6/QYXjz32GGussUaH39fTCb29tYYs0avPF+ZfV35vekPuLfR4feX6sK9P0iTbIxtdi0m+IYSQiUjoIYSQiaZL6FV1AYXWFL8vIczWVAm9X79+vPLKK/FHGrqkVg+9X79+VYcSQlNoqlkuQ4YMYcqUKXRUifHFV6f3YkTw2LT+vfp8Yf7UdiwKITRZQu/bt2+nO89s3SIj0SGE0NuaqsslhBDChxcJPYQQMhEJPYQQMhEJPYQQMhEJPYQQMhEJPYQQMhEJPYQQMhEJPYQQMhEJPYQQMhEJPYQQMhEJPYQQMhEJPYQQMtFUxblC62yDFUJoPtFCDyGETERCDyGETERCDyGETERCDyGETERCDyGETERCDyGETMS0xdCrYlpmCD0nWughhJCJSOghhJCJLiV0SWMlPSFpsqRDGlxfUdJNku6T9KCkz5UfagghhI50mtAltQEnA1sDawK7SFqz3cMOAy62vQ6wM3BK2YGGEELoWFda6KOAybafsv0ecBGwbbvHGFis+Hpx4F/lhRhCCKErujLLZXngubrjKcAG7R5zFHCtpG8CiwJblhJdCCGELitrUHQX4BzbQ4DPAedLmuvfljRe0kRJE6dOnVrSU4cQQoCuJfTngRXqjocU5+rtA1wMYPtvQD9g6fb/kO0JtkfaHrnMMst8uIhDCCE01JWEfg+wmqRVJC1MGvS8vN1j/glsASBpDVJCjyZ4CCH0ok4Tuu0ZwDeAa4DHSLNZHpF0tKRxxcMOAvaV9ABwIbCXbfdU0CGEEObWpaX/tq8Crmp37oi6rx8FNio3tBBCCPMjVoqGEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImupTQJY2V9ISkyZIOmcdjdpL0qKRHJP223DBDCCF0ZqHOHiCpDTgZ+AwwBbhH0uW2H617zGrAocBGtl+VtGxPBRxCCKGxrrTQRwGTbT9l+z3gImDbdo/ZFzjZ9qsAtl8qN8wQQgid6UpCXx54ru54SnGu3ieAT0i6Q9KdksY2+ockjZc0UdLEqVOnfriIQwghNFTWoOhCwGrAZsAuwOmSlmj/INsTbI+0PXKZZZYp6alDCCFA1xL688AKdcdDinP1pgCX237f9tPA30kJPoQQQi/pSkK/B1hN0iqSFgZ2Bi5v95g/kVrnSFqa1AXzVHlhhhBC6EynCd32DOAbwDXAY8DFth+RdLSkccXDrgFekfQocBPwXduv9FTQIYQQ5tbptEUA21cBV7U7d0Td1wYOLD5CCCFUIFaKhhBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJiKhhxBCJrqU0CWNlfSEpMmSDungcTtKsqSR5YUYQgihKzpN6JLagJOBrYE1gV0krdngcYOA/YG7yg4yhBBC57rSQh8FTLb9lO33gIuAbRs87kfAz4F3SowvhBBCF3UloS8PPFd3PKU4N4ukdYEVbF/Z0T8kabykiZImTp06db6DDSGEMG/dHhSV1Ac4Djios8fanmB7pO2RyyyzTHefOoQQQp2uJPTngRXqjocU52oGAcOAmyU9A4wGLo+B0RBC6F1dSej3AKtJWkXSwsDOwOW1i7Zft7207ZVtrwzcCYyzPbFHIg4hhNBQpwnd9gzgG8A1wGPAxbYfkXS0pHE9HWAIIYSuWagrD7J9FXBVu3NHzOOxm3U/rBBCCPMrVoqGEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImIqGHEEImupTQJY2V9ISkyZIOaXD9QEmPSnpQ0g2SVio/1BBCCB3pNKFLagNOBrYG1gR2kbRmu4fdB4y0vRbwB+CYsgMNIYTQsa600EcBk20/Zfs94CJg2/oH2L7J9tvF4Z3AkHLDDCGE0JmuJPTlgefqjqcU5+ZlH+DqRhckjZc0UdLEqVOndj3KEEIInSp1UFTSbsBI4NhG121PsD3S9shlllmmzKcOIYQF3kJdeMzzwAp1x0OKc3OQtCXwA2BT2++WE14IIYSu6koL/R5gNUmrSFoY2Bm4vP4BktYBTgPG2X6p/DBDCCF0ptOEbnsG8A3gGuAx4GLbj0g6WtK44mHHAgOB30u6X9Ll8/jnQggh9JCudLlg+yrgqnbnjqj7esuS4wohhDCfYqVoCCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkIhJ6CCFkoksJXdJYSU9ImizpkAbXF5H0u+L6XZJWLj3SEEIIHeo0oUtqA04GtgbWBHaRtGa7h+0DvGr748AvgZ+XHWgIIYSOdaWFPgqYbPsp2+8BFwHbtnvMtsC5xdd/ALaQpPLCDCGE0BnZ7vgB0heBsba/WhzvDmxg+xt1j3m4eMyU4vgfxWNebvdvjQfGF4dDgSfKeiFdsDTwcqePal3x+lpXzq8N4vWVbSXbyzS6sFAvBoHtCcCE3nzOGkkTbY+s4rl7Q7y+1pXza4N4fb2pK10uzwMr1B0PKc41fIykhYDFgVfKCDCEEELXdCWh3wOsJmkVSQsDOwOXt3vM5cCexddfBG50Z305IYQQStVpl4vtGZK+AVwDtAFn2X5E0tHARNuXA2cC50uaDPyHlPSbTSVdPb0oXl/ryvm1Qby+XtPpoGgIIYTWECtFQwghE5HQQwghE5HQQwghE706D703SeoDjACWA6YDD9t+qdqoyiNpWWAj6l4faZD6g0oDK0nurw9A0qLAO7ZnVh1L2SQtyeyf3TOZ/dya9nczu0FRSasCBwNbAk8CU4F+wCeAt4HTgHOb4T//w5C0OXAIsBRwH/ASs1/fqqTSC/9n+43KguyGnF9f0cjYGdgVWB94F1iEtMrwSuA025Ori7B7JC0OfB3YBViY2X97HwHuBE6xfVN1EXZPK/xu5pjQLwROBW5rPxde0kdIv2yv2j630fc3O0nHAr+y/c8G1xYCtgHabP+x14MrQc6vT9ItwPXAZaQ7xg+K80sBmwNfBi61/ZvqovzwJF0HnAf82fZr7a6tB+wOPGT7zArC67ZW+N3MLqGH1iZpB9uXVB1HT5DU1/b73X1MCPOywCR0SVsB37X9mapj6S5Jm5LuMh6UtBOwCfAP0i3tu9VG1z2S7rW9btVx9AZJA0glqZ+1PbXqeLpLUoc/N9v39lYsPUHSHh1dt31eb8UyL9kNikr6NPBr0oDFn0i12c8GBPykusjKIelkYC1gEUl/BwYCfyEN0pxF6p8NTUjSOOBE0mrqw0j7DLwIrCzp4FbtBqwzkTRAWKs8WF9C28Cnez2icq0/j/PjgOVJ3U2Vyq6FLuk+4NvA30ibcvwGOMT2SZUGVhJJj9peU1I/UlG0ZW3PLOrPP2h7eMUhdoukt4FGA4MCbHutXg6pNJIeAP4fqXjdTcBatp8qZk3ckMHP7gBSLafXSfsmXGr7zUqD6iHF39uupAkYjwI/sf1gtVFl2EIn/dHfXHz9J0nP55LMC+8A2H5H0rO1KW+2LSmHvtengS9UHUQP+cD23wEkPW37KQDbL0maUW1o3Wf7eOB4SR8jzea5QdKzwE9t319lbGUpBj/3Ar5DmrnzRdu9ua9Dh3JM6EtI2qHueKH64wwG3JaVdCCpxVr7muK4YdH7FvOe7WerDqKH9CnmZ/cBPii+rnVLZLPIr7jruAzoT5rZ8gng/kqDKoGkrwP7AzeQNvR5ptqI5pZjl8vZHVy27a/0WjA9QNKRHV23/cPeiqUnSDqpfjesnEh6BviAOfuWa2z7Y70bUbnqWubbAs+Rul2utD290sBKIukD0tzzqaQxgVmXaJLuwOwSekck7diK85cXNJKGkrYqXL049RhwejPd2oa5FQnvQdI8+zeYM+lh+7gq4iqLpJU6ut4Md5YLWkL/p+0Vq46jOyQd0cFl2/5RrwXTAyRtCFxCWtF7H6n1sw6wL7CD7TsrDK9bJLUB/WsDhZJGk1ZUAtxne1plwZVA0lG0S+L1Wv3usRUsaAn9OdsrdP7I5iXpoAanFwX2AQbbHtjLIZVK0tXAz+sGtmvnNyXNVtq6ksBKIOkXwEu2jymOnyZN8+sH3Gv74Crj6y5J69u+p+o4ekrx85qrq6X42rZX7f2o5rSgJfSWb6HXkzSINEizD3AxqY5ESxcgk/R325+Yx7UnbA/t7ZjKUkypXd/2jNqx7XWKKXC32d642gi7p3h9A0l95xfafrTikEolaXC7U32AnUgzXu61vWPvRzWn7Ga5SHqIxrd9IhUJanlF7Y8DSfNgzwXWtf1qtVGVpqNuh7d6LYqe0aeWzAsHw6wppy19ZwVQvDkNJQ2M/qGYRnshcFEzzgiZX7ZfgVlF1nYHvkuavfP5Znnzyq6F3goDF91RFAjagbSP4cm5LdyQ9BKphTfXJWAn2y37pizpMWBU+77yokrhXbZXb/ydrUnSCFJy3wn4t+2NKg6pWyT1Bb5CWrh4O/CzZquOmWNCV/sqix/mMc2qmEnwLjCDxlOnFqsksJJI2rOj6628PL5YM7Al8LVaxb6iAXIqcKPtX1QZX5mKVuwWpOqmnwP+Znv7aqPqHklTSH93xwNzVVxshjUuOSb0m4E/ApfVl7mUtDCwMbAncJPtcyoJMCzQJH0N+D5pIBvgTVJL79TqoiqPpDGkJL4d8BDpbusS269XGVcZJJ3DvGfxNMUalxwTej/SbdGuwCrAa6RZBG3AtaSKhPdVFmA3SRrYWTdLVx7TrCSdDpxg++EG1xYFvgS8a/uCXg+uRMWANq0+VbGepOeAZ0lJ/OJWH6BvT9Li83pjkjTS9sTejmmuOHJL6PWKPq+lgeluV3C/VUm6gTQQcxkwyfZbxfmPkTZJ2Im0COcPlQXZDZLWJrVgh5Om9NV2vVkNWIxUUfLXbsEywZJ2A37reeyWpbTb1kdt3967kZVD0krtx6iK8gavtWoXZz1J9wBbtZ+AIOkzwFnNMCU6u1ku9Zw2Cnih6jjKZHsLSZ8D9gM2Kv5gZgBPkLYx29P2v6uMsTuKIk47FbM+RgIfJe3b+FgGK0UHA/dJmgRMYvab1ceBTUllZw+pLrxu21PSxbYfl7QIqazzCGCGpC/bvr7i+LprAnCTpM+4qF8v6cukstyfrzSyQtYt9BCaTbFa9NOk+vWz3qyAq91ga7NWIukRYFgxDXM8qS99S1JxrnNtj6o0wBJI2h34HrAVqfvvazRRoa6sW+ghNBuncsfXFR+5ea+ua+WzpPnnM4HHirKzLc/2+ZLeIZWl+Cewse2XO/m2XpPFf3IIoSm8K2kYaRemzUkrKGsGVBNSeeoWLYr0egYDNxYrfZui2mK2Cb2ogf5zYFnSDyCLedoLgqJb4ue2v9Ppg0MzOQD4A6ku/y9tPw1QjPm07MyyOttUHUBnsu1DlzQZ+ILtx6qOpWxFwnskt5WF9STdaXt01XGEUNMKixazbaEDL+aYzCH1w0p6QtKKrT6Q1oH7JF0O/J66Gi7NsBqvu+p2mar3Omka6v29HE5pimmZF8wrobX6tEzSDJdOFy0C51QTXt4JfaKk3wF/Ii2VB/JICIUlgUck3c2cCW9cdSGVqh/wCnPuFG9SrfRWN7L4+HNxvA1pY4ivSfp9rbxuCxoM3J/xtMyxpEWLF0pqtGjx+KoXLebc5dJoK7qmWJ5bhqI++Fxs39LbsYT5I+lW4HOevdHFQNIagrGkVvqaVcbXHTlPy6zXrIsWs03oC4KisNNqtq+XNABoy2UpuaRPkIpWfcT2MElrAeNs/7ji0LpN0uPA8GLhG8UinAdsr16rkV5thKFVZbPTeHuShki6VNJLxccfJQ2pOq6ySNqXNKPgtOLU8qTupVycDhwKvA9g+0FSKdYcXADcJelIpU2/7wB+W9SqaYq62qE1ZZvQgbOBy4Hlio8/F+dy8XXSbe0bALafJE3RzMUA23e3Ozej4SNbjNO+r/uR+mBfI5XTPdr2W7Z3rTK20NpyHhRdxnZ9Aj9H0gFVBdMD3rX9XlrTAMVKvJz6z14uZkUYQNIXyasuz73A8xR/g5nPWMpKcSc13fYHRdfg6qQxgvcrDi3rFvorknaT1FZ87EaaNZGLWyR9H+hfVHv7PbNnTeTg66TupNUlPU9atPLflUZUEknfJK2mvA64gjQgekWlQZVI0v6SFlNypqR7JW1VdVwluhXoJ2l50uyW3alwqmK9bAdFiwHDXwEbklp5fwW+lUsrqNgRZh9SkSAB1wBn5FCmtF7RGuqTy2AvzFr0toGLPSpzI+kB2yMkfZbUtXQ4cL7tdSsOrRSS7rW9bvHG3N/2MZLut7121bFl2+VS1GXOZU72XIqa2qcXH9mQtJvt37RffFPrWrJ9XCWBles50kKiXKn4/DlSIn9EtR9gHiRpQ9ImOvsU59oqjGeW7BK6pO8V75i/okGfsu1vVRBWaYp60zvVFQqaQzMUCOqmWhGnQZVG0bOeAm6WdCVzLnrL4c0KYJKka0k7hh1a7M7UcFOPFrU/aQbWpcWb1cdIK0Qrl11CJy1iAKh8O6geckDxuekLBX1IqxafH7X9+0oj6Tn/LD4WLj5ysw+wNvCU7bclDQb2rjakchQLp8bVr8i2/RTQFA3FbPvQ6xX9zQNtv1F1LN1V1393vu3dq46nbMWdx1qkFZNZ9LkuaIrulV2Bj9k+WtKKwH81mIbakpq5cFyOLXQAJP2WtJvITOAeYDFJJ9g+ttrIum3hYturTxUlgueQQa2avwCvAgMl1b8Bt3z5Y0nH2z5A0p9p3F2Wy5jPKaQulk8DRwPTgD8C61cZVImatnBcti302qizpF2BdUlFgSa1eh+zpI1JrZ+dSAun6uVUq+Yy29tWHUeZJK1ne1LudXjq7iJnlTGozXypOrYyNHOdqGxb6EDfooDOdsBJtt+X1PLvXkXp0dslTbR9ZtXx9JTckjmA7UnFl2vbPqH+mqT9gSwSOvB+0ddcWxS2DBkNitpu2vGAnBP6acAzwAPArcW89Bz60D9t+0bg1Ry7XCTdbntjSdOYvd1XTUt3udTZEzih3bm9GpxrVScClwLLSvoJ8EXgsGpDKk/RQm/UZVZ5Cz3bLpdGJC1ku6XrgUj6oe0jm/m2LzQmaRfgy6TNEG6ru7QYMNP2FpUE1gMkrQ5sQXpDviGnzWYk7Vh32A/YHvhXM0yJzjahF7ewZ5MGZM4A1gEOsX1tpYGFLinquEyx/a6kzUgzX85rptrT86u4S1wF+F/m3OhhGvBgqzc26hVdLh+hrhcgl1Xa7RWz6G63/amqY8m5lstXimmKW5F299kd+Fm1IZWnXb2MMzKsl/FHYKakjwMTgBWA31YbUvfYftb2zcCWwG3FIOgLwBDm7FpqabnXqmlgNZqk0mnOfei5Lz/+iu0TinoZg0lvWOeTigXl4APbMyRtD/zK9q8k5bBzPKTiTmMkLUn6ed0DfIk0eykH+wNDM65VUz++Y+DfwMGVBlXIOaHnvvy4/g3rvAzfsN4v+pz3BL5QnOtbYTxlUrGCch/glFpxp6qDKlHWtWpsN21ZipwTerbLjwu5v2HtTVoY9hPbTyttynt+xTGVpWmLO5Uk91o1SBoHbFIc3my7KbqUck7oBtYk1Tw5GliUNCKdi/ZvWEuR0RuW7Ucp6mMUXRODbP+82qhKcwBNWtypJFnXqpH0M9Kq1wuKU/tL+pTt71cYFpD3LJdTKZYf216j1l9pO4vlx5I2Au63/ZbS5h3rAicUZYNbnqSbSeWPFwImAS8Bd9g+sKPvayWSBgLYfrPqWELXSXqQtDjsg+K4DbivGVah5zzLZQPbXwfeAbD9Knm1Fk4F3pY0AjgI+AdwXrUhlWrxYpbSDqQxgg1Is0NanqThxQDvI8CjkiZJ+mTVcXWXpOOLz3+WdHn7j4rDK9sSdV8vXlUQ7eXc5ZL18mNghm1L2pZU2uDMYpAtFwtJ+iipZs0Pqg6mZKcBB9q+CaCYZ386UPk85m6qjXH8otIoet7/kgp03USanLAJc64rqEzOCT3r5cfANEmHArsBmxSLG3KZBQJp3OMa0oKNe4p+5icrjqksi9aSOYDtm5W22mtptVo1uRQZmxfbFxZdguuTGowH2/53tVElWfahF8ltNPAf8l1+/F+kZeT32L6tqDm9me2cul2yJOlS4F5mt2h3A9azvX11UZWnGN85CliJ1GislT7+WJVxlamoo7QxKaHfbvvSikMCMk3oAPWlO0PrkdSPNJPnk9TNTsqhVk0xQP9DUkKAVNflqGKcp+VJehz4Nmkwe2btfC4LjSSdAnwcuLA49SXgH8WYXaVyTui/AP4GXOIMX6Sk0cCvgDVIg71twJu2m2aApjsk/R54nHQXcjRpzvZjtvevNLASFWsHnNssF0l3FYPYWSresNao5ZWiR+AR22tUG1neCX0aae75DNJMl5bf8aaepInAzqRdU0YCewCfsH1opYGVpHaHJelB22sVte1vc5Nu/TU/JA0nzUhaqjj1MrCn7Yeri6r7JNW2DNyJ1MC4hDkXFt1bRVxlk3QF8PXaFOGi6NpJtr/Q8Xf2vGwHRZt5eW5ZbE+W1GZ7JnB2MRUui4QOvF98fk3SMFK9jKYogFSCRrNcJtD6s1z+r93xyLqvTdqSrmVp9taBg4DHJN1dHG8ANMV+qdkm9LrWQr3XgWczKVP6tqSFgfslHUOq2pfTuoIJRV/z4aSt9gYCR1QbUmlyneWyedUx9LCmn46Zc5fLnaTVkw8Vp4YDD5MWAfx3q9dFL27zXiJNVfw26XWdYntypYGFTi0As1x+ChxTq11fvDEfZDunacNNKeeEfglwuO1HiuM1SYNr3yMNlK5dYXhhHiR1uLQ/hwJPC8Asl7lmmKnYOLqqmMpUTFn8OakLUDTR+Fy2XS6kAcJHage2H5W0uu2nWrnKrKSHaLCfYU0z1JPopgVh7ONVisJjmWqTtIjtdwEk9QcWqTimMh0DfKEZ17XknNAfKQp0XVQcf4lUN2MRZg+4taJtqg6gJ9n+YdUx9BRJSwNfB14FzgKOBcaQ6vAclFF32QXADZq97+3ewLkVxlO2F5sxmUPeXS79gf9h9m3tHcAppCmMA1p17m+xJdtHbN/R7vxGwL9t/6OayMoh6Vhgsu3T2p3fD1jFdlPUzPgwivr1E0l3IVsA55AGfMcAu9rerLLgSiZpa9JrBLjO9jVVxlOGoqsFYFPgv4A/Mee0zEsqCGsO2SZ0mJXUV7T9RNWxlKWYA3uo7YfanR8O/LQZ5sJ2h6RJwMj2i8GKxRsP2h5WTWTdJ+kB2yOKnaWetb1i3bX7Y1ynudXdcTTiZljFnG2XS7GjyLGkVZSrSFobONr2uEoD676PtE/mALYfkrRyBfGUbZFGK3ttf6BWHvxIZkL6y5f0crtr2VQCnccq5reaYdCwO2w3/QYy2SZ04EhgFHAzgO37lbYxa3VLdHCtf28F0YOmS1rN9hyVFSWtBkyvKKayfKyoC666rymOc/jdrDmJBquYK42oRJJObHD6dWCi7ct6O556OSf0922/3q5Rl0P/0kRJ+9o+vf6kpK+SiiG1uiOAqyX9mNmvZyRpBewBVQVVkm3rvm6/SKXpF63Mj8xXMfcDVie9YQHsCDwNjJC0ue0Dqgos54T+iKQvk6ZQrUaaJvbXimMqwwHApZJ2Zc6EtzDQ8gtTbF8taTvgu8A3i9MPAzs26mpqJbnXCa+T+yrmtYCNijer2naXt5EmYFT6O5rtoKikAaSdbrYi3dL+BfhRbW5sq5O0OVAbIHzE9o1VxhNCTe6rmCU9AYyy/XpxvDhwt+2hVZftzjahtydpKPAd2/tWHUsIoXUVWz0eRhqfq21B91NSffSjbH+3sthyS+iS1iL1Ry5Hmid6MmmQZgPg/2z/srroQgBJ/8/27zs712okPdjR9QxWMc9S7Hc7qji8x/a/qoynJseEfhdwKmlzi61JAzHnAkfYfqfK2EKAxnVNcqh1Iul+0sSD3wJ/pt2spFr98FZVlA55fB6VXJui3nuOCX2OBRqSnnJGexnWFOVWpxfzsz9BGnW/2nYrlzWYpXhNp5Lm3Q8r7rzG2f5xxaF9aMXqyc+RNoD4Xd2lxYA1bY9q+I0tRNLqwC7AF4BHScn92hxKVkuaYHu8pJsaXLbtyuu955jQHyf9QtXmK15A2sZM0BzvomUoVlSOAZYklTW4B3jP9q6VBlYSSbeQZrqcVhtkkvRwi68UHQGsTar6WV/bfRpwUy7VFmskfYnU5flz28dWHc+CIMeE3ujds6Yp3kXLULtFl/RNoL/tY3JaPi7pHtvr188ayOX1Seqby51Ue5KWJy0q2p5UhOxi4NJWrZ3USDGD7kBSWZHxxbToobavqDi0/OahO/9dU2okaUPS5sn7FOfaKoynbC9LWpViMZikL5LmM+dglKSjgJVIf4O1etot3TVY3FUNIiXxvYFXiksLS1rK9n8qC65cZ5PWgNS2DHyetMio8oSeXQt9QSFpU+Ag4A7bP5f0MeAA21nU2S5eT22fzVdJK/F2s/1MlXGVoegW/DYpKcysnbf9yjy/qQVIeobZq7HrE0sWb1g1kibaHtnu7vEB2yMqjy0SemuTNMD221XH0VOKwd8+tqdVHUtZJN1le4Oq4wgfjqS/kkoD31F0e64KXNgMg9o5LcddoEjaUNKjwOPF8QhJp1QcVmkkfUTSmcAfbE+TtGaxoCMHN0k6tvgZrlv7qDqo0GVHklaeryDpAuAG0taWlcu2hS5pe+DGuuW5SwCb2f5TlXGVpZhv/0Xg8lxmgdSTdDWpr/IHRQ3xhYD7bA+vOLRua+Zpb6FrJA0GRpO6k+603b4cciVyTuhzzYious5CmWq37c3Yj1eGnGe5hNYkacWOrtv+Z2/FMi/ZzXKp06g7KafX+5ykTwGW1BfYH2jKfQ4/pLeKVlBtlstoUs3plifpI6TaH8vZ3lrSmsCGts+sOLRuk9RGKha3etWx9IArSb+P9TW5DSwDLEsTzDLLuQ99oqTjJK1afBxHHvXCa75G2nB4edK0qbWL41wcSNpvc1VJdwDnMbucbqs7B7iGVG8I4O+0fq13AIqSsk901pptRbaH216r+DyctBr2DuBNmuTnl1OLtb1vAocze4n1deSV8JzLqtD2ilbepsXHUFKL6ImMFuMsbftiSYcC2J4haWZn39RCliTtR3A38FbtpFt/+0dg1u5ZP6Ao+Ad8q1l+N7NN6LbfAlp2h/guuLMohnQW8BdnNBhie6akXYrKmI9UHU8PyLY7qXB41QH0BEnDSIn8k8AxwD61TS6aRXaDopKOt32ApD/TYMu5jFoJArYEvgKsT1qdd47tv1caWEkk/ZK0QcLvmLOV1/K1eIopir8ibVDyMKkP9ou2Oyw/20qKcYL1i8O7bb9UZTxlKO6iniP1pc+VyJthUV+OCX0925OKlZRzcYbbgCntXvQbYFHgAeAQ23+rNqoPR9K1trfKfWpfMQ0zx+4kJO0EHMvsDSDGAN+1/Ycq4+ouSXt2dN32ub0Vy7xkl9BrJO1v+4TOzrWq4pZ9N2B34EXgTNIg4trA72235C7yOU0tbU/Sp23fKGmHRtdtX9LbMfUESQ8An6m1yiUtA1yfy5TaZpZtHzqwJ9A+ee/V4Fyr+htwPrCd7Sl15ydK+nVFMZVh8XklPGj5pLcpcCNpdkR7Blr5tdXr066L5RXynlHXNLJroUvahVT/fGPSTtw1iwEzbW9RSWAlk6ScBkJrJL0CXMacc31rbPsrvRxSmE+SjgXWIu2xCfAl4CHbTbE8Pmc5JvSVgFWA/2XOWS7TgAedwc4pMOs29nukEfd+tfOt3sesDLZimxdJB3Z03fZxvRVLTyvusjYuDm+zfWmV8ZRJ0ka27+jsXBWy63Jx2rfwWUlbMvcWbQ9VG12pLiDNANmGtMhoT2BqpRGVo1HLPBeDis9DSTNALi+OvwDcXUlEPUDSz20fTF0XUt25HPwKaN/oaHSu12XXQq9R/lu0TbK9nqQHXeymXqt/UnVs3SFpmO2Hq46jJ0m6Ffh8rSSwpEHAlbY3qTaycjS6y6r/PW1VxYYynyKtCv1l3aXFgO2bYdA3uxZ6Hdl+uyi5eoqLLdqqDqpEtWluL0j6PPAvYKkK4ylF7sm88BHgvbrj94pzLU3SfwP/QyrXUD+nfhDw12qiKtXCwEBS3hxUd/4NUuXTymWd0JX3Fm0/lrQ4adeiX5FaCd+uNqTQRecBd0uq9StvB1Q+h7kEvwWupsH4lTPYfq5Yw3KLpHOKrl0k9QEG2n6j2uiSnLtcst6iLbQ2Sesxe9DwVtv3VRlPmYpSBo/UdSktBqxh+65qIyuHpN+Sxq1mkrpyFwNOsH1spYGRcULPlaR+pGlgrwJ/Js10GQP8A/hRsxTa7y5JGwFHkdlGyvUkLcucM5Qqr6ddBkn3AevWptUWrdiJucxeqtXll7QraSD0EGBSM4wRZNflsgDUcjmP1H++KOkO5GHgJFJr7xzSrJccnEmDjZRzIGkcqUrfcsBLwIqkrQQ/WWVcJZpjjUQx0yynXNO32INgO+Ak2+9LaoqWcU7/yTXnF59/UWkUPWdN28OKP5Aptms1a/5SLLnOxeu2r646iB7yI9L2ZdfbXqeoxbNbxTGV6SlJ3wJOLY7/B3iqwnjKdhrwDKlu0q3F2pfoQw/zr35KWPvpYTktypH0M9Ig9iXAu7XzmVRbnGh7ZPEGvE7Rgs1p+8BlgROBT5Pukm8gjV+1fMXFeZG0UDMsWsyxhQ6ApIeYu8vldWAi8GPbr/R+VKUYIulEUp9y7WuK4+WrC6t0GxSfR9adMylJtLrXJA0EbgUukPQSdSWCW12RuHeuOo6eMq8tBEndhJXKtoUu6RhS3+tvi1M7AwOAfwMb225UIKnptUIJz9AxSYsC00kFq3YFFgcuaOFGxhyKldmnAh8pugfXAsbZ/nHFoZVC0tXA2cAPbI8ouj/vc9qWrlI5J/RGq9Xutb2upIea4T8/zFsxx/5IoLZ68hbgaNstvbNPsb3e9bY3rzqWniLpFuC7wGm1UsiSHrY9rNrIylFbkV1f6rk286Xi0LIuadkmaVTtQNL6zF5YVHlfV+jUWaSCajsVH2+QWkUtzWnLsg+KN6xcDbDdvjZNTn9zTbuFYLZ96MBXgbOKvkqREsI+xe3u/1YaWeiKVW3vWHf8w4xKN7wJPCTpOubcXi+XRW8vS1qV2Qnvi8AL1YZUqgNJhdVWlXQHxRaC1YaUZJvQbd8DDK+1hNrdql9cTVRhPkyXtLHt22HWQqPpFcdUlkvIZzOLRr4OTABWl/Q88DRprKDlFV1mmxYfTbeFYM596Fn2wdYsAANPa5PqmyxO+qP5D7CX7Zzm2mdF0qOkSQgX2v5HcTfcp1YCIBeS7rY9qvNH9r6cE/ofSasoa7M+dgdG2J7n9matJPeBp5qiDgjNUvyoOyRtCwyxfXJxfBfpdh3ge279TZRHkGaT7UTadu5C4He2/1VpYCWT9EugL2k/gvous8rXSOSc0OcadW6WkegyNPNIe3dI2s32b+a1u08r7+pT9LfubPu54vh+YAtSGYezncn2iDBroPBLwI6kOkO/tX16tVGVQ9JNDU7bTbBbWLZ96OTdBwv5DjwtWnwe1OGjWtPCtWReuL2Ye/5K0T2RDdt3AndKuoy0GcRJQBYJvZmnnObcQh9BKmRVmx72KrCn7Qfn/V2toygHPIG0g8qrFANPtTrNoflImmz74/O49g/bq/Z2TD2hmCK8C6l1/jRwEfD7XBZOARSbyrTfz/fo6iJKsp2HbrtWG2MtYK2iW6LyW6ISPWt7S1If7Oq2N84pmUs6RtJikvpKukHSVEmtXsDqLkn7tj8paT8y2FNU0k8l/QM4BXge2Mj2ZrZ/nVky/zWpO+mbpAH7/0cq81y5bFvojUj6p+0Vq46jDJL+CfyFNDBzozP7QdbVnN6eVBL4QNJGEC1bwKooWvUnUrGx2gDaesAiwHa2X6wotFJIOoI0w+XJqmPpSSr2R637PBC42vaYqmPLuQ+9kZx2lF+dlOi+Dpwp6QrgotqYQQZqv5ufJ92uvy619o+vKFr1KUmfZnbt8ytt31hhWKVphi6HXlIbi3tb0nKkGT0frTCeWRa0hJ5NK9b226QFUhdLWhI4gTTXPpd9U6+Q9Djpj+e/JS0DvFNxTKUoEngWSXwBdYWkJYBjSXdaBs6oNKJCdl0ukqbROHEL6G87mzcxpX1TvwSMJZUF/p3tP1YbVXkkLUXa6GKmpAHAYrb/XXVcIdRIWgTo1ywLFrNL6AsKSc8A95Fa6ZfbzqKetqRP275RUsMFYLZzXjKfBUk3tJ9T3+hcqyoaFwcBK9reV9JqwFDbV1Qc2gLX5dLyJI23PYE0c6flV082sCmpO6JRvXqTdw2Ulqa0gfkAYOmiG7A26LEYeW2+cjZpr9sNi+Pngd8DkdDDfHu1+PzjRoOErV6xz/aRxee9q44lzLf9gANIm19PYnZCf4O0sCgXq9r+kqRdII1nqUlG7COht55Vis+TKo2ih0n6KXCM7deK4yWBg2wfVmlgYZ5snyDpJOD7tn9UdTw96D1J/Zm9SntV6va9rVL0obcYSV+w/eeq4+hp9TVq6s5lswl2zhr97HIi6TPAYcCawLXARqRKoDdXGRdEQm9ZxTS+g0m/VPXLj7NYDSvpQWB92+8Wx/2BibY/2fF3hqpJ+gXwN+CS3Ba81RQ7Fo0mdSvdafvlikMCosullV1AWiX6eeBrwJ7A1EojKtcFwA2SatvO7c3sUsihue1HWtk7U9J0UtKz7cWqDat7JLVfZf5Q8XmApBVt/7O3Y2ovWugtStIk2+vVlh8X5+6xvX7VsZVF0lhgy+LwOtvXVBlPWLBJeojUb14/AGpSPaVlbVe+qC9a6K2rtuXVC0Xlt38BS1UYT094DJhh+3pJAyQNym33m1xJGsfs3cJuboY52t1le3j9saSVSd2eWwI/rSKm9qKF3qIkbQPcBqwA/Io01/eHti+vNLCSFFUJxwNL2V61WLzx61wWp+RM0s+A9UndZpBK6U60fWh1UZWn+F38AbAB8H/AuY49RUOYt2I3n1HAXXU7Mj3UvpUUmk8xoL227Q+K4zbgvlrXYKuSNIyUyD8JHEOqLDmz2qjmFF0uLaYoUTovzmj+77u236ut15C0EBkVV1sALEHa2BtmbzLT6h4AngOuJDU2RtWvJ2qGRX2R0FtPo5otiwL7AIOBXBL6LZK+D/Qv5v3+D5D9/PtM/C9wX7H3pkh96YdUG1IpvlJ1AJ2JLpcWJmkQsD8pmV8M/F9Rc7vlSepDel1bkZLCNcAZuc5rzo2kj5L60Q3cE1Uye0e00FtQUVb2QGBX0tzsdW2/2vF3tRbbH0j6E/An2znNr19QbAhsTEroCwGXVhvOgiHbPUVzJelY4B5gGjDc9lE5JXMlR0l6GXgCeKLYT7SjsYPQRCSdQlrs9hDwMLCfpJOrjWrBEF0uLUbSB6RCQDOYc5Awl9V4BwJbA+NtP12c+xhwKvAX27+sMr7QuWKnqTVq3WNF99kjtteoNrLukbSC7efmcW2bZphrHy30FmO7j+3+tgfZXqzuY1CrJ/PC7sAutWQOYPspYDdgj8qiCvNjMlC/TH6F4lyru65YTDQHSV8hbQFZuUjoodn0bVToqOhH71tBPGH+DQIek3SzpJuBR4HFJF0uqZUXvh0IXFssLAJA0qHAt0kbs1QuBkVDs3nvQ14LzSPL8Q7bV0l6F7ha0nbAV0nz0TdplnGs6EMPTUXSTBrPtRdpM95opbcASf9FSnbZTVuUNIY0a+evwE6236k4pFkioYcQSiXpq6RW+o2kN+JNgaNtn1VpYN0kaRqzqy0uQiqQN5MmmpAQCT2EUCpJTwCfsv1KcTwY+KvtodVGlr8YFA0hlO0V0jqJmmnFudDDooUeQiiVpPOA4cBlpC6KbYEHiw9sH1dddHmLWS4hhLL9o/iouaz4PKiCWBYo0UIPIYRMRAs9hFCqomzuXC1F25+uIJwFSiT0EELZvlP3dT9gR1LtodDDosslhNDjJN1te1TVceQuWughhFIV9fpr+gDrkc82dE0tEnoIoWyTmL2icgbwNGn3qdDDosslhBAyEStFQwilkLR+UZSrdryHpMskndiuGyb0kEjoIYSynEZR4ljSJsDPgPOA14EJFca1wIg+9BBCWdps/6f4+kvABNt/BP4o6f7qwlpwRAs9hFCWNkm1RuIWpPK5NdF47AXxnxxCKMuFwC2SXgamA7cBSPo4qdsl9LCY5RJCKI2k0cBHgWttv1Wc+wQw0Pa9lQa3AIiEHkIImYg+9BBCyEQk9BBCyEQk9BBCyEQk9BBCyMT/B6WDBZim4tFCAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                               Best Mean Test Score\nLogistic Regression (LR)                   0.972313\nNaïve Bayes Classifier (NB)                0.864722\nDecision Tree Classifier (DT)              0.832168\nGradient Boosting (GB)                     0.967180\nSupport Vector Machines (SVM)              0.990297\nK Nearest Neighbours (KNN)                 0.988296",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Best Mean Test Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression (LR)</th>\n      <td>0.972313</td>\n    </tr>\n    <tr>\n      <th>Naïve Bayes Classifier (NB)</th>\n      <td>0.864722</td>\n    </tr>\n    <tr>\n      <th>Decision Tree Classifier (DT)</th>\n      <td>0.832168</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting (GB)</th>\n      <td>0.967180</td>\n    </tr>\n    <tr>\n      <th>Support Vector Machines (SVM)</th>\n      <td>0.990297</td>\n    </tr>\n    <tr>\n      <th>K Nearest Neighbours (KNN)</th>\n      <td>0.988296</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(Evaluation_Results)\n",
    "df.plot(kind = 'bar')\n",
    "plt.show()\n",
    "\n",
    "Evaluation_Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Au vu des résultats, le modèle ayant la meilleure exactitude semble être la classification machine à vecteurs de support.\n",
    "Plusieurs sets d'hyperparamètres donnent les mêmes résultats, cependant les paramètres  sont significativement plus rapide à entrainer que les autres en se basant sur le mean_score_time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entrainement du modèle choisi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "KNN_definitive_model = KNeighborsClassifier(weights='distance', p=2, n_neighbors=1, leaf_size=10, algorithm='auto')\n",
    "\n",
    "#KNN_fitted = KNN_definitive_model.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predicted)\n",
    "#disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "#print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Citation :\n",
    "Stephanie Glen. \"Regularization: Simple Definition, L1 & L2 Penalties\" From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/regularization/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}