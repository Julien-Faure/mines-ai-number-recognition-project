{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0b8e1d",
   "metadata": {},
   "source": [
    "# Rapport du projet de résolution de problème\n",
    "\n",
    "- Paul Achard\n",
    "- Julien Faure\n",
    "\n",
    "    \n",
    "- *Date : 20/01/2022*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1070770",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import np as np\n",
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42622bf2",
   "metadata": {},
   "source": [
    "## Problématique\n",
    "\n",
    "Notre problématique est d'identifier un chiffre à partir d'une image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3a0c6",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "- Trouver un modèle permettant d'identifier un chiffre à partir d'une image\n",
    "- Comparer différentes stratégies de solveur pour le modèle trouvé\n",
    "\n",
    "## Analyse du dataset\n",
    "\n",
    "Identifier un chiffre à partir d'une image est une tache qui peut s'avérer très complexe. Afin d'avoir une difficulté raisonnable et adapté au contexte de ce projet, nous avons fixé certains paramètres dans notre dataset.\n",
    "\n",
    "- La résolution de nos images est identiques pour tout le dataset. Cette résolution est **8 pixels par 8 pixels**.\n",
    "- Chaque pixel est codé sur **4 bits**.\n",
    "- Les images contiennent uniquement un chiffre sans élément parasite, sans effet et sans traitement.\n",
    "\n",
    "Nous utilisons le dataset `digits` de `seaborn`.\n",
    "\n",
    "### Forme du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "894968c2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data Shape (1797, 64)\n",
      "Label Data Shape (1797,)\n"
     ]
    }
   ],
   "source": [
    "# Import du dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Affiche le nombre d'images et leur format\n",
    "print(\"Image Data Shape\" , digits.data.shape)\n",
    "\n",
    "# Affiche le nombre de labels\n",
    "print(\"Label Data Shape\", digits.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311b24e7",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir ci-dessus, le dataset est composé de **1797** images labellisées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb1ba9",
   "metadata": {},
   "source": [
    "### Répartition des images du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1826bdc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Text(0.5, 0, 'n° labellisé'), Text(0, 0.5, 'Occurrence')]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphique = sns.countplot(x=digits.target)\n",
    "graphique.set(xlabel=\"n° labellisé\", ylabel = \"Occurrence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee617b1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Les données labellisées sont équitablement distribuées (environ 175 par label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc387e8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Représentation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "17d087b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x216 with 10 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Création de 10 figures de 10 par 3 pixels\n",
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 3))\n",
    "\n",
    "# Pour chaque figure, on affiche l'image du chiffre et son label en titre\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "    ax.set_title(\"Label: %i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e6cc1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Représentation des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb283e09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "\n",
      " Type de chaque valeur : <class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "# Affiche le tableau représentant la première image\n",
    "print(digits.images[0])\n",
    "print(\"\\n Type de chaque valeur :\", type(digits.images[0][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa72f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Comme nous l'avons vu précédemment, chaque image possède une résolution de 8x8 pixels en niveaux de gris codés sur 4bits par pixel.\n",
    "\n",
    "Dans notre programme, une image est représentée par une matrice de dimension 8x8. Chaque élément représente un pixel avec un niveau de gris codé sur 4bits (de 0 à 15). Plus la valeur est élevée, plus la couleur est foncée.\n",
    "\n",
    "> Exemple :\n",
    "* 0 : Blanc\n",
    "* 15 : Noir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3675a",
   "metadata": {},
   "source": [
    "### Mise en forme des données d'entrées\n",
    "\n",
    "Pour commencer, nous devons redimenssionner nos données d'entrées.\n",
    "\n",
    "Actuellement, nous avons des données sous la forme d'un tableau de matrice.\n",
    "\n",
    "Il nous faut les mettre sous forme d'une matrice où chaque vecteur correspond aux pixels de l'image.\n",
    "\n",
    "Donc si nous avons 1797 images, la matrice d'entrée est composée de 1797 vecteurs.\n",
    "\n",
    "Avec une résolution de 8x8, la taille d'un vecteur est de 64 valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74b2574d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Redimensionne la matrice en vecteur\n",
    "print(digits.images[0].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e10f7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redimenssionne le tableau de matrice en matrice\n",
    "x = digits.images.reshape((len(digits.images), -1))\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Préparation des données\n",
    "\n",
    "### Jeu de test et jeu d'entraînement\n",
    "\n",
    "Il nous faut maintenant séparer notre jeu de test et notre jeu d'entraînement du dataset.\n",
    "\n",
    "Pour choisir la taille de notre jeu de test, il est nécessaire de faire attention à plusieurs points :\n",
    "* Le temps d'entrainement de notre modèle\n",
    "* La taille de notre dataset\n",
    "\n",
    "Plus la proportion du jeu d'entraînement est faible, plus notre modèle à un niveau de variance élevé. Ainsi, on augmente les chances d'avoir de l'*over fitting*.\n",
    "\n",
    "A contrario, plus la proportion du jeu de test est faible, plus notre modèle à un niveau de variance faible. Ainsi, on augmente les chances d'avoir de l'*under fitting*.\n",
    "\n",
    "Le but est donc de trouver la valeur qui nous permet d'avoir le taux de variance optimal.\n",
    "\n",
    "\n",
    "Pour trouver cette valeur, nous avons appliqué la procédure suivante, nous avons essayé avec plusieurs valeurs en partant de 20% jusqu'à 80% avec un pas de 5%. Nous avons déterminé que la meilleure valeur est **35%**.\n",
    "\n",
    "\n",
    "> [https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio](https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# séparation du dataset en données \"d'apprentissage\" et de test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.35, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Création du tableau de résultat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "                               Best Mean Test Score\nLogistic Regression (LR)                        0.0\nNaïve Bayes Classifier (NB)                     0.0\nDecision Tree Classifier (DT)                   0.0\nGradient Boosting (GB)                          0.0\nSupport Vector Machines (SVM)                   0.0\nK Nearest Neighbours (KNN)                      0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Best Mean Test Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression (LR)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Naïve Bayes Classifier (NB)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Decision Tree Classifier (DT)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting (GB)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Vector Machines (SVM)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>K Nearest Neighbours (KNN)</th>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluation_Results = pd.DataFrame(np.zeros((6, 1)), columns=['Best Mean Test Score'])\n",
    "Evaluation_Results.index = ['Logistic Regression (LR)', 'Naïve Bayes Classifier (NB)',\n",
    "                            'Decision Tree Classifier (DT)', 'Gradient Boosting (GB)',\n",
    "                            'Support Vector Machines (SVM)', 'K Nearest Neighbours (KNN)']\n",
    "Evaluation_Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test de différents modèles\n",
    "\n",
    "Pour résoudre notre problème, nous allons essayer les modèles suivants :\n",
    "* [Modèle de regréssion logistique multiclasse](#Mod%C3%A8le-de-regr%C3%A9ssion-logistique-multiclasse)\n",
    "* [Modèle de Bayes](#Mod%C3%A8le-de-Bayes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "334ad595",
   "metadata": {},
   "source": [
    "### Modèle de regréssion logistique multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc863948",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 18 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.97231332 0.97231332 0.96489613 0.96659819 0.95546507        nan\n",
      " 0.96917575 0.96946187 0.95748138 0.96659819 0.95546507        nan\n",
      " 0.96917697 0.96860595 0.95633934 0.96659819 0.95546507        nan]\n",
      "  warnings.warn(\n",
      "e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [1.         1.         0.99600438 1.         1.                nan\n",
      " 1.         1.         0.99978594 1.         1.                nan\n",
      " 1.         1.         1.         1.         1.                nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n0        0.483861      0.055574         0.000534        0.000499     0.1   \n1        1.535126      0.577567         0.000734        0.000443     0.1   \n2        0.065526      0.005009         0.000400        0.000490     0.1   \n3        0.129451      0.009933         0.000400        0.000491     0.1   \n4        0.046576      0.008974         0.000534        0.000619     0.1   \n5        0.000534      0.000499         0.000000        0.000000     0.1   \n6        0.679550      0.091812         0.000467        0.000499       5   \n7        1.961961      0.982320         0.000534        0.000619       5   \n8        0.104629      0.005542         0.000334        0.000472       5   \n9        0.122645      0.004001         0.000400        0.000490       5   \n10       0.039836      0.001425         0.000400        0.000490       5   \n11       0.000200      0.000400         0.000000        0.000000       5   \n12       0.678349      0.063620         0.000267        0.000443      10   \n13       1.982335      0.724591         0.000391        0.000480      10   \n14       0.110967      0.004459         0.000267        0.000443      10   \n15       0.122046      0.003583         0.000200        0.000400      10   \n16       0.041373      0.002017         0.000400        0.000490      10   \n17       0.000267      0.000443         0.000000        0.000000      10   \n\n   param_penalty param_solver  \\\n0             l2    newton-cg   \n1             l2        lbfgs   \n2             l2    liblinear   \n3           none    newton-cg   \n4           none        lbfgs   \n5           none    liblinear   \n6             l2    newton-cg   \n7             l2        lbfgs   \n8             l2    liblinear   \n9           none    newton-cg   \n10          none        lbfgs   \n11          none    liblinear   \n12            l2    newton-cg   \n13            l2        lbfgs   \n14            l2    liblinear   \n15          none    newton-cg   \n16          none        lbfgs   \n17          none    liblinear   \n\n                                               params  split0_test_score  \\\n0   {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-...           0.978632   \n1      {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}           0.978632   \n2   {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...           0.978632   \n3   {'C': 0.1, 'penalty': 'none', 'solver': 'newto...           0.970085   \n4    {'C': 0.1, 'penalty': 'none', 'solver': 'lbfgs'}           0.957265   \n5   {'C': 0.1, 'penalty': 'none', 'solver': 'libli...                NaN   \n6    {'C': 5, 'penalty': 'l2', 'solver': 'newton-cg'}           0.978632   \n7        {'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}           0.978632   \n8    {'C': 5, 'penalty': 'l2', 'solver': 'liblinear'}           0.970085   \n9   {'C': 5, 'penalty': 'none', 'solver': 'newton-...           0.970085   \n10     {'C': 5, 'penalty': 'none', 'solver': 'lbfgs'}           0.957265   \n11  {'C': 5, 'penalty': 'none', 'solver': 'libline...                NaN   \n12  {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}           0.974359   \n13      {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}           0.974359   \n14  {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}           0.970085   \n15  {'C': 10, 'penalty': 'none', 'solver': 'newton...           0.970085   \n16    {'C': 10, 'penalty': 'none', 'solver': 'lbfgs'}           0.957265   \n17  {'C': 10, 'penalty': 'none', 'solver': 'liblin...                NaN   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.965812           0.974359           0.982833   \n1            0.965812           0.974359           0.982833   \n2            0.965812           0.961538           0.969957   \n3            0.957265           0.965812           0.974249   \n4            0.948718           0.965812           0.944206   \n5                 NaN                NaN                NaN   \n6            0.970085           0.965812           0.974249   \n7            0.970085           0.965812           0.974249   \n8            0.965812           0.952991           0.965665   \n9            0.957265           0.965812           0.974249   \n10           0.948718           0.965812           0.944206   \n11                NaN                NaN                NaN   \n12           0.965812           0.965812           0.978541   \n13           0.965812           0.965812           0.978541   \n14           0.965812           0.944444           0.969957   \n15           0.957265           0.965812           0.974249   \n16           0.948718           0.965812           0.944206   \n17                NaN                NaN                NaN   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.969957           0.974359           0.987179   \n1            0.969957           0.974359           0.987179   \n2            0.952790           0.957265           0.982906   \n3            0.961373           0.978632           0.982906   \n4            0.948498           0.970085           0.970085   \n5                 NaN                NaN                NaN   \n6            0.965665           0.974359           0.978632   \n7            0.969957           0.974359           0.978632   \n8            0.952790           0.957265           0.978632   \n9            0.961373           0.978632           0.982906   \n10           0.948498           0.970085           0.970085   \n11                NaN                NaN                NaN   \n12           0.965665           0.978632           0.978632   \n13           0.965665           0.974359           0.978632   \n14           0.952790           0.957265           0.974359   \n15           0.961373           0.978632           0.982906   \n16           0.948498           0.970085           0.970085   \n17                NaN                NaN                NaN   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.970085           0.957082           0.982833   \n1            0.970085           0.957082           0.982833   \n2            0.952991           0.948498           0.982833   \n3            0.974359           0.957082           0.974249   \n4            0.965812           0.922747           0.969957   \n5                 NaN                NaN                NaN   \n6            0.965812           0.965665           0.987124   \n7            0.965812           0.965665           0.987124   \n8            0.927350           0.948498           0.969957   \n9            0.974359           0.957082           0.974249   \n10           0.965812           0.922747           0.969957   \n11                NaN                NaN                NaN   \n12           0.965812           0.965665           0.987124   \n13           0.965812           0.965665           0.987124   \n14           0.935897           0.944206           0.965665   \n15           0.974359           0.957082           0.974249   \n16           0.965812           0.922747           0.969957   \n17                NaN                NaN                NaN   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.970085            0.961538            0.982906   \n1             0.970085            0.961538            0.982906   \n2             0.952991            0.961538            0.974359   \n3             0.965812            0.965812            0.978632   \n4             0.948718            0.957265            0.965812   \n5                  NaN                 NaN                 NaN   \n6             0.957265            0.957265            0.982906   \n7             0.957265            0.957265            0.982906   \n8             0.935897            0.940171            0.974359   \n9             0.965812            0.965812            0.978632   \n10            0.948718            0.957265            0.965812   \n11                 NaN                 NaN                 NaN   \n12            0.957265            0.957265            0.982906   \n13            0.957265            0.957265            0.982906   \n14            0.931624            0.940171            0.974359   \n15            0.965812            0.965812            0.978632   \n16            0.948718            0.957265            0.965812   \n17                 NaN                 NaN                 NaN   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.969957            0.957082         0.972313        0.009115   \n1             0.969957            0.957082         0.972313        0.009115   \n2             0.961373            0.969957         0.964896        0.010858   \n3             0.957082            0.935622         0.966598        0.011553   \n4             0.957082            0.939914         0.955465        0.013029   \n5                  NaN                 NaN              NaN             NaN   \n6             0.965665            0.948498         0.969176        0.010046   \n7             0.965665            0.948498         0.969462        0.010003   \n8             0.965665            0.957082         0.957481        0.014251   \n9             0.957082            0.935622         0.966598        0.011553   \n10            0.957082            0.939914         0.955465        0.013029   \n11                 NaN                 NaN              NaN             NaN   \n12            0.965665            0.948498         0.969177        0.010284   \n13            0.965665            0.944206         0.968606        0.010692   \n14            0.965665            0.952790         0.956339        0.013886   \n15            0.957082            0.935622         0.966598        0.011553   \n16            0.957082            0.939914         0.955465        0.013029   \n17                 NaN                 NaN              NaN             NaN   \n\n    rank_test_score  split0_train_score  split1_train_score  \\\n0                 1            1.000000            1.000000   \n1                 1            1.000000            1.000000   \n2                10            0.994647            0.998929   \n3                 7            1.000000            1.000000   \n4                13            1.000000            1.000000   \n5                17                 NaN                 NaN   \n6                 5            1.000000            1.000000   \n7                 3            1.000000            1.000000   \n8                11            0.998929            1.000000   \n9                 7            1.000000            1.000000   \n10               13            1.000000            1.000000   \n11               16                 NaN                 NaN   \n12                4            1.000000            1.000000   \n13                6            1.000000            1.000000   \n14               12            1.000000            1.000000   \n15                7            1.000000            1.000000   \n16               13            1.000000            1.000000   \n17               18                 NaN                 NaN   \n\n    split2_train_score  split3_train_score  split4_train_score  \\\n0             1.000000            1.000000            1.000000   \n1             1.000000            1.000000            1.000000   \n2             0.994647            0.995722            0.996791   \n3             1.000000            1.000000            1.000000   \n4             1.000000            1.000000            1.000000   \n5                  NaN                 NaN                 NaN   \n6             1.000000            1.000000            1.000000   \n7             1.000000            1.000000            1.000000   \n8             1.000000            1.000000            1.000000   \n9             1.000000            1.000000            1.000000   \n10            1.000000            1.000000            1.000000   \n11                 NaN                 NaN                 NaN   \n12            1.000000            1.000000            1.000000   \n13            1.000000            1.000000            1.000000   \n14            1.000000            1.000000            1.000000   \n15            1.000000            1.000000            1.000000   \n16            1.000000            1.000000            1.000000   \n17                 NaN                 NaN                 NaN   \n\n    split5_train_score  split6_train_score  split7_train_score  \\\n0             1.000000            1.000000            1.000000   \n1             1.000000            1.000000            1.000000   \n2             0.992505            0.996788            0.995717   \n3             1.000000            1.000000            1.000000   \n4             1.000000            1.000000            1.000000   \n5                  NaN                 NaN                 NaN   \n6             1.000000            1.000000            1.000000   \n7             1.000000            1.000000            1.000000   \n8             1.000000            0.998929            1.000000   \n9             1.000000            1.000000            1.000000   \n10            1.000000            1.000000            1.000000   \n11                 NaN                 NaN                 NaN   \n12            1.000000            1.000000            1.000000   \n13            1.000000            1.000000            1.000000   \n14            1.000000            1.000000            1.000000   \n15            1.000000            1.000000            1.000000   \n16            1.000000            1.000000            1.000000   \n17                 NaN                 NaN                 NaN   \n\n    split8_train_score  split9_train_score  split10_train_score  \\\n0             1.000000            1.000000             1.000000   \n1             1.000000            1.000000             1.000000   \n2             0.995722            0.996791             0.995717   \n3             1.000000            1.000000             1.000000   \n4             1.000000            1.000000             1.000000   \n5                  NaN                 NaN                  NaN   \n6             1.000000            1.000000             1.000000   \n7             1.000000            1.000000             1.000000   \n8             1.000000            1.000000             1.000000   \n9             1.000000            1.000000             1.000000   \n10            1.000000            1.000000             1.000000   \n11                 NaN                 NaN                  NaN   \n12            1.000000            1.000000             1.000000   \n13            1.000000            1.000000             1.000000   \n14            1.000000            1.000000             1.000000   \n15            1.000000            1.000000             1.000000   \n16            1.000000            1.000000             1.000000   \n17                 NaN                 NaN                  NaN   \n\n    split11_train_score  split12_train_score  split13_train_score  \\\n0              1.000000             1.000000             1.000000   \n1              1.000000             1.000000             1.000000   \n2              0.996788             0.995717             0.996791   \n3              1.000000             1.000000             1.000000   \n4              1.000000             1.000000             1.000000   \n5                   NaN                  NaN                  NaN   \n6              1.000000             1.000000             1.000000   \n7              1.000000             1.000000             1.000000   \n8              1.000000             1.000000             1.000000   \n9              1.000000             1.000000             1.000000   \n10             1.000000             1.000000             1.000000   \n11                  NaN                  NaN                  NaN   \n12             1.000000             1.000000             1.000000   \n13             1.000000             1.000000             1.000000   \n14             1.000000             1.000000             1.000000   \n15             1.000000             1.000000             1.000000   \n16             1.000000             1.000000             1.000000   \n17                  NaN                  NaN                  NaN   \n\n    split14_train_score  mean_train_score  std_train_score  \n0              1.000000          1.000000         0.000000  \n1              1.000000          1.000000         0.000000  \n2              0.996791          0.996004         0.001381  \n3              1.000000          1.000000         0.000000  \n4              1.000000          1.000000         0.000000  \n5                   NaN               NaN              NaN  \n6              1.000000          1.000000         0.000000  \n7              1.000000          1.000000         0.000000  \n8              0.998930          0.999786         0.000428  \n9              1.000000          1.000000         0.000000  \n10             1.000000          1.000000         0.000000  \n11                  NaN               NaN              NaN  \n12             1.000000          1.000000         0.000000  \n13             1.000000          1.000000         0.000000  \n14             1.000000          1.000000         0.000000  \n15             1.000000          1.000000         0.000000  \n16             1.000000          1.000000         0.000000  \n17                  NaN               NaN              NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_C</th>\n      <th>param_penalty</th>\n      <th>param_solver</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n      <th>split0_train_score</th>\n      <th>split1_train_score</th>\n      <th>split2_train_score</th>\n      <th>split3_train_score</th>\n      <th>split4_train_score</th>\n      <th>split5_train_score</th>\n      <th>split6_train_score</th>\n      <th>split7_train_score</th>\n      <th>split8_train_score</th>\n      <th>split9_train_score</th>\n      <th>split10_train_score</th>\n      <th>split11_train_score</th>\n      <th>split12_train_score</th>\n      <th>split13_train_score</th>\n      <th>split14_train_score</th>\n      <th>mean_train_score</th>\n      <th>std_train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.483861</td>\n      <td>0.055574</td>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'newton-...</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.957082</td>\n      <td>0.982833</td>\n      <td>0.970085</td>\n      <td>0.961538</td>\n      <td>0.982906</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.972313</td>\n      <td>0.009115</td>\n      <td>1</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.535126</td>\n      <td>0.577567</td>\n      <td>0.000734</td>\n      <td>0.000443</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.957082</td>\n      <td>0.982833</td>\n      <td>0.970085</td>\n      <td>0.961538</td>\n      <td>0.982906</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.972313</td>\n      <td>0.009115</td>\n      <td>1</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.065526</td>\n      <td>0.005009</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'libline...</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.952991</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.964896</td>\n      <td>0.010858</td>\n      <td>10</td>\n      <td>0.994647</td>\n      <td>0.998929</td>\n      <td>0.994647</td>\n      <td>0.995722</td>\n      <td>0.996791</td>\n      <td>0.992505</td>\n      <td>0.996788</td>\n      <td>0.995717</td>\n      <td>0.995722</td>\n      <td>0.996791</td>\n      <td>0.995717</td>\n      <td>0.996788</td>\n      <td>0.995717</td>\n      <td>0.996791</td>\n      <td>0.996791</td>\n      <td>0.996004</td>\n      <td>0.001381</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.129451</td>\n      <td>0.009933</td>\n      <td>0.000400</td>\n      <td>0.000491</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'newto...</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.978632</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.957082</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.966598</td>\n      <td>0.011553</td>\n      <td>7</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.046576</td>\n      <td>0.008974</td>\n      <td>0.000534</td>\n      <td>0.000619</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.957265</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.948498</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.922747</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.939914</td>\n      <td>0.955465</td>\n      <td>0.013029</td>\n      <td>13</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000534</td>\n      <td>0.000499</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'libli...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>17</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.679550</td>\n      <td>0.091812</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.969176</td>\n      <td>0.010046</td>\n      <td>5</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.961961</td>\n      <td>0.982320</td>\n      <td>0.000534</td>\n      <td>0.000619</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.969462</td>\n      <td>0.010003</td>\n      <td>3</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.104629</td>\n      <td>0.005542</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'liblinear'}</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.978632</td>\n      <td>0.927350</td>\n      <td>0.948498</td>\n      <td>0.969957</td>\n      <td>0.935897</td>\n      <td>0.940171</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.957481</td>\n      <td>0.014251</td>\n      <td>11</td>\n      <td>0.998929</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.998929</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.998930</td>\n      <td>0.999786</td>\n      <td>0.000428</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.122645</td>\n      <td>0.004001</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>5</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'newton-...</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.978632</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.957082</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.966598</td>\n      <td>0.011553</td>\n      <td>7</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.039836</td>\n      <td>0.001425</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>5</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.957265</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.948498</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.922747</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.939914</td>\n      <td>0.955465</td>\n      <td>0.013029</td>\n      <td>13</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'libline...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>16</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.678349</td>\n      <td>0.063620</td>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.965665</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.969177</td>\n      <td>0.010284</td>\n      <td>4</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.982335</td>\n      <td>0.724591</td>\n      <td>0.000391</td>\n      <td>0.000480</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.944206</td>\n      <td>0.968606</td>\n      <td>0.010692</td>\n      <td>6</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.110967</td>\n      <td>0.004459</td>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.944444</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.974359</td>\n      <td>0.935897</td>\n      <td>0.944206</td>\n      <td>0.965665</td>\n      <td>0.931624</td>\n      <td>0.940171</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.956339</td>\n      <td>0.013886</td>\n      <td>12</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.122046</td>\n      <td>0.003583</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>10</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'newton...</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.978632</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.957082</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.966598</td>\n      <td>0.011553</td>\n      <td>7</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.041373</td>\n      <td>0.002017</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>10</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.957265</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.948498</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.922747</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.939914</td>\n      <td>0.955465</td>\n      <td>0.013029</td>\n      <td>13</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>10</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'liblin...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création du modèle de régression\n",
    "LR_model = LogisticRegression(verbose=False, max_iter=10000)\n",
    "\n",
    "# Dictionnaire contenant les différents paramètres à essayer\n",
    "LR_params = dict()\n",
    "LR_params['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "LR_params['penalty'] = ['l2', 'none']\n",
    "LR_params['C'] = [0.1, 5, 10]\n",
    "\n",
    "# Création de nos itérations\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "# Création de notre modèle de validation croisée\n",
    "model_cv = GridSearchCV(estimator=LR_model,\n",
    "                        param_grid=LR_params,\n",
    "                        scoring='accuracy',\n",
    "                        cv=kFold,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True)\n",
    "\n",
    "# Entrainement du modèle\n",
    "model_cv.fit(x_train, y_train)\n",
    "# cv results\n",
    "pd.set_option('display.max_columns', None)\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[0]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31144b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "La validation croisée nous permet de determiner les paramètres optimaux pour notre modèle d'apprentissage.\n",
    "Les résultats de nos tests avec de nombreux paramètres différents (dont la plupart ne sont pas présent dans l'exemple ci-dessus).\n",
    "* Le **paramètre C** correspond à l'inverse de la force de régularisation des données. Plus ce paramètre est grand, plus le risque d'overfitting est grand. Une valeur de 0.1 semble être optimale.\n",
    "* Le **paramètre de pénalité** permet de réduire les coefficients θ. On remarque une perte de précision si l'on n'applique pas de pénalité. La meilleure pénalité semble être la norme L2.\n",
    "* Le **solveur** correspond à l'algorithme d'optimisation utilisé pour l'entrainement. Dans notre cas, lbfgs permet d'obtenir la precision la plus élevée malgré un temps d'entrainement significativement plus long que ses concurrents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b3aae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Classification naïve bayésienne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9833fcac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n0       0.003621      0.000868         0.000858        0.000957           0   \n1       0.002269      0.000574         0.000734        0.000443      0.0001   \n2       0.002002      0.000633         0.000600        0.000490       0.001   \n3       0.002135      0.000499         0.000801        0.000400        0.01   \n4       0.002336      0.000472         0.000601        0.000490         0.1   \n5       0.002269      0.000443         0.000334        0.000472         0.5   \n6       0.002135      0.000340         0.000267        0.000443         1.0   \n\n              params  split0_test_score  split1_test_score  split2_test_score  \\\n0       {'alpha': 0}           0.837607           0.871795           0.854701   \n1  {'alpha': 0.0001}           0.841880           0.884615           0.854701   \n2   {'alpha': 0.001}           0.846154           0.884615           0.854701   \n3    {'alpha': 0.01}           0.850427           0.884615           0.854701   \n4     {'alpha': 0.1}           0.854701           0.880342           0.854701   \n5     {'alpha': 0.5}           0.850427           0.876068           0.850427   \n6     {'alpha': 1.0}           0.850427           0.871795           0.850427   \n\n   split3_test_score  split4_test_score  split5_test_score  split6_test_score  \\\n0           0.836910           0.845494           0.871795           0.850427   \n1           0.849785           0.854077           0.876068           0.880342   \n2           0.849785           0.858369           0.876068           0.884615   \n3           0.849785           0.858369           0.867521           0.876068   \n4           0.849785           0.858369           0.863248           0.871795   \n5           0.845494           0.845494           0.858974           0.867521   \n6           0.841202           0.845494           0.858974           0.867521   \n\n   split7_test_score  split8_test_score  split9_test_score  \\\n0           0.854701           0.819742           0.875536   \n1           0.863248           0.828326           0.879828   \n2           0.867521           0.836910           0.875536   \n3           0.867521           0.841202           0.875536   \n4           0.867521           0.841202           0.884120   \n5           0.867521           0.836910           0.884120   \n6           0.867521           0.832618           0.879828   \n\n   split10_test_score  split11_test_score  split12_test_score  \\\n0            0.833333            0.854701            0.858974   \n1            0.854701            0.858974            0.867521   \n2            0.854701            0.858974            0.867521   \n3            0.858974            0.858974            0.863248   \n4            0.867521            0.858974            0.854701   \n5            0.871795            0.850427            0.854701   \n6            0.871795            0.841880            0.854701   \n\n   split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0            0.866953            0.871245         0.853594        0.015985   \n1            0.871245            0.884120         0.863296        0.015941   \n2            0.871245            0.884120         0.864722        0.014346   \n3            0.871245            0.884120         0.864154        0.012277   \n4            0.871245            0.888412         0.864442        0.012759   \n5            0.854077            0.875536         0.859300        0.013215   \n6            0.854077            0.875536         0.857587        0.013731   \n\n   rank_test_score  \n0                7  \n1                4  \n2                1  \n3                3  \n4                2  \n5                5  \n6                6  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_alpha</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.003621</td>\n      <td>0.000868</td>\n      <td>0.000858</td>\n      <td>0.000957</td>\n      <td>0</td>\n      <td>{'alpha': 0}</td>\n      <td>0.837607</td>\n      <td>0.871795</td>\n      <td>0.854701</td>\n      <td>0.836910</td>\n      <td>0.845494</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.854701</td>\n      <td>0.819742</td>\n      <td>0.875536</td>\n      <td>0.833333</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.866953</td>\n      <td>0.871245</td>\n      <td>0.853594</td>\n      <td>0.015985</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.002269</td>\n      <td>0.000574</td>\n      <td>0.000734</td>\n      <td>0.000443</td>\n      <td>0.0001</td>\n      <td>{'alpha': 0.0001}</td>\n      <td>0.841880</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.854077</td>\n      <td>0.876068</td>\n      <td>0.880342</td>\n      <td>0.863248</td>\n      <td>0.828326</td>\n      <td>0.879828</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.863296</td>\n      <td>0.015941</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.002002</td>\n      <td>0.000633</td>\n      <td>0.000600</td>\n      <td>0.000490</td>\n      <td>0.001</td>\n      <td>{'alpha': 0.001}</td>\n      <td>0.846154</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.876068</td>\n      <td>0.884615</td>\n      <td>0.867521</td>\n      <td>0.836910</td>\n      <td>0.875536</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.864722</td>\n      <td>0.014346</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.002135</td>\n      <td>0.000499</td>\n      <td>0.000801</td>\n      <td>0.000400</td>\n      <td>0.01</td>\n      <td>{'alpha': 0.01}</td>\n      <td>0.850427</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.867521</td>\n      <td>0.876068</td>\n      <td>0.867521</td>\n      <td>0.841202</td>\n      <td>0.875536</td>\n      <td>0.858974</td>\n      <td>0.858974</td>\n      <td>0.863248</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.864154</td>\n      <td>0.012277</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.002336</td>\n      <td>0.000472</td>\n      <td>0.000601</td>\n      <td>0.000490</td>\n      <td>0.1</td>\n      <td>{'alpha': 0.1}</td>\n      <td>0.854701</td>\n      <td>0.880342</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.863248</td>\n      <td>0.871795</td>\n      <td>0.867521</td>\n      <td>0.841202</td>\n      <td>0.884120</td>\n      <td>0.867521</td>\n      <td>0.858974</td>\n      <td>0.854701</td>\n      <td>0.871245</td>\n      <td>0.888412</td>\n      <td>0.864442</td>\n      <td>0.012759</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.002269</td>\n      <td>0.000443</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>0.5</td>\n      <td>{'alpha': 0.5}</td>\n      <td>0.850427</td>\n      <td>0.876068</td>\n      <td>0.850427</td>\n      <td>0.845494</td>\n      <td>0.845494</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.867521</td>\n      <td>0.836910</td>\n      <td>0.884120</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.854701</td>\n      <td>0.854077</td>\n      <td>0.875536</td>\n      <td>0.859300</td>\n      <td>0.013215</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.002135</td>\n      <td>0.000340</td>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>1.0</td>\n      <td>{'alpha': 1.0}</td>\n      <td>0.850427</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.841202</td>\n      <td>0.845494</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.867521</td>\n      <td>0.832618</td>\n      <td>0.879828</td>\n      <td>0.871795</td>\n      <td>0.841880</td>\n      <td>0.854701</td>\n      <td>0.854077</td>\n      <td>0.875536</td>\n      <td>0.857587</td>\n      <td>0.013731</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_model = BernoulliNB()\n",
    "\n",
    "NB_params = {'alpha': [0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0]}\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(NB_model, NB_params, n_iter=7, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(RCV.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[1]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le seul paramètre testé dans ce modèle, alpha, correspond à la force du lissage de Laplace a appliqué au modèle.\n",
    "En d'autres termes, ce paramètre permet de palier la présence d'une caractéristique dans le jeu de test qui n'existe pas dans le jeu d'entrainement.\n",
    "> https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece\n",
    "\n",
    "Au vu des résultats, le paramètre alpha semble être optimal autour de 0.001\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification par arbre de décision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        0.004600  6.231581e-04         0.000601        0.000490   \n1        0.002202  4.001142e-04         0.000133        0.000340   \n2        0.004562  7.966420e-04         0.000133        0.000339   \n3        0.001863  3.386618e-04         0.000200        0.000400   \n4        0.004066  2.510008e-04         0.000466        0.000498   \n5        0.001656  4.680112e-04         0.000397        0.000486   \n6        0.004003  5.169178e-04         0.000267        0.000443   \n7        0.001868  3.401657e-04         0.000334        0.000472   \n8        0.010476  9.576567e-04         0.000400        0.000490   \n9        0.004404  8.006812e-04         0.000334        0.000472   \n10       0.009609  1.144033e-03         0.000467        0.000499   \n11       0.003870  7.186739e-04         0.000600        0.000611   \n12       0.009875  1.455898e-03         0.000400        0.000490   \n13       0.004004  6.329627e-04         0.000400        0.000490   \n14       0.009008  1.033675e-03         0.000400        0.000490   \n15       0.004537  3.899957e-03         0.000334        0.000472   \n16       0.006005  5.309834e-07         0.000200        0.000400   \n17       0.002402  4.903172e-04         0.000334        0.000472   \n18       0.006272  1.124460e-03         0.000334        0.000472   \n19       0.002002  5.169145e-04         0.000334        0.000472   \n20       0.005872  7.185986e-04         0.000334        0.000472   \n21       0.002335  4.717638e-04         0.000334        0.000472   \n22       0.006339  1.851655e-03         0.000400        0.000490   \n23       0.001935  4.426735e-04         0.000334        0.000472   \n24       0.016415  5.455403e-03         0.000200        0.000400   \n25       0.005405  6.116578e-04         0.000334        0.000472   \n26       0.013345  9.438198e-04         0.000334        0.000472   \n27       0.004938  7.725001e-04         0.000400        0.000490   \n28       0.013145  7.186813e-04         0.000467        0.000499   \n29       0.004404  4.901870e-04         0.000400        0.000490   \n30       0.012152  6.140171e-04         0.000405        0.000456   \n31       0.003740  6.040129e-04         0.000200        0.000400   \n\n   param_splitter param_min_samples_leaf param_max_depth param_criterion  \\\n0            best                      1               3            gini   \n1          random                      1               3            gini   \n2            best                      2               3            gini   \n3          random                      2               3            gini   \n4            best                      3               3            gini   \n5          random                      3               3            gini   \n6            best                      4               3            gini   \n7          random                      4               3            gini   \n8            best                      1            None            gini   \n9          random                      1            None            gini   \n10           best                      2            None            gini   \n11         random                      2            None            gini   \n12           best                      3            None            gini   \n13         random                      3            None            gini   \n14           best                      4            None            gini   \n15         random                      4            None            gini   \n16           best                      1               3         entropy   \n17         random                      1               3         entropy   \n18           best                      2               3         entropy   \n19         random                      2               3         entropy   \n20           best                      3               3         entropy   \n21         random                      3               3         entropy   \n22           best                      4               3         entropy   \n23         random                      4               3         entropy   \n24           best                      1            None         entropy   \n25         random                      1            None         entropy   \n26           best                      2            None         entropy   \n27         random                      2            None         entropy   \n28           best                      3            None         entropy   \n29         random                      3            None         entropy   \n30           best                      4            None         entropy   \n31         random                      4            None         entropy   \n\n                                               params  split0_test_score  \\\n0   {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.410256   \n1   {'splitter': 'random', 'min_samples_leaf': 1, ...           0.534188   \n2   {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.410256   \n3   {'splitter': 'random', 'min_samples_leaf': 2, ...           0.465812   \n4   {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.410256   \n5   {'splitter': 'random', 'min_samples_leaf': 3, ...           0.465812   \n6   {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.410256   \n7   {'splitter': 'random', 'min_samples_leaf': 4, ...           0.431624   \n8   {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.799145   \n9   {'splitter': 'random', 'min_samples_leaf': 1, ...           0.786325   \n10  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.752137   \n11  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.837607   \n12  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.756410   \n13  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.803419   \n14  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.735043   \n15  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.717949   \n16  {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.495726   \n17  {'splitter': 'random', 'min_samples_leaf': 1, ...           0.500000   \n18  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.495726   \n19  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.525641   \n20  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.495726   \n21  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.521368   \n22  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.495726   \n23  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.465812   \n24  {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.799145   \n25  {'splitter': 'random', 'min_samples_leaf': 1, ...           0.820513   \n26  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.773504   \n27  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.803419   \n28  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.777778   \n29  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.760684   \n30  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.773504   \n31  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.794872   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.470085           0.410256           0.484979   \n1            0.440171           0.602564           0.532189   \n2            0.470085           0.410256           0.484979   \n3            0.512821           0.431624           0.489270   \n4            0.470085           0.410256           0.484979   \n5            0.521368           0.423077           0.412017   \n6            0.470085           0.410256           0.484979   \n7            0.512821           0.452991           0.502146   \n8            0.807692           0.863248           0.806867   \n9            0.876068           0.863248           0.832618   \n10           0.829060           0.858974           0.798283   \n11           0.833333           0.824786           0.849785   \n12           0.833333           0.837607           0.789700   \n13           0.841880           0.837607           0.845494   \n14           0.846154           0.841880           0.798283   \n15           0.820513           0.820513           0.776824   \n16           0.529915           0.555556           0.575107   \n17           0.500000           0.504274           0.519313   \n18           0.529915           0.555556           0.575107   \n19           0.623932           0.525641           0.570815   \n20           0.529915           0.555556           0.575107   \n21           0.602564           0.495726           0.510730   \n22           0.529915           0.555556           0.575107   \n23           0.615385           0.512821           0.489270   \n24           0.871795           0.833333           0.871245   \n25           0.850427           0.888889           0.858369   \n26           0.837607           0.833333           0.845494   \n27           0.854701           0.854701           0.849785   \n28           0.867521           0.820513           0.836910   \n29           0.850427           0.803419           0.828326   \n30           0.867521           0.816239           0.824034   \n31           0.816239           0.824786           0.798283   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.463519           0.423077           0.457265   \n1            0.437768           0.521368           0.482906   \n2            0.463519           0.423077           0.457265   \n3            0.523605           0.482906           0.581197   \n4            0.463519           0.423077           0.457265   \n5            0.472103           0.589744           0.478632   \n6            0.463519           0.423077           0.457265   \n7            0.489270           0.534188           0.495726   \n8            0.785408           0.846154           0.846154   \n9            0.832618           0.871795           0.867521   \n10           0.781116           0.829060           0.854701   \n11           0.751073           0.811966           0.807692   \n12           0.772532           0.833333           0.854701   \n13           0.776824           0.841880           0.747863   \n14           0.755365           0.833333           0.837607   \n15           0.781116           0.782051           0.820513   \n16           0.553648           0.576923           0.538462   \n17           0.557940           0.534188           0.465812   \n18           0.553648           0.576923           0.538462   \n19           0.515021           0.512821           0.517094   \n20           0.553648           0.576923           0.538462   \n21           0.476395           0.551282           0.576923   \n22           0.553648           0.576923           0.538462   \n23           0.515021           0.534188           0.598291   \n24           0.811159           0.863248           0.811966   \n25           0.862661           0.867521           0.863248   \n26           0.815451           0.850427           0.816239   \n27           0.858369           0.820513           0.833333   \n28           0.811159           0.858974           0.811966   \n29           0.793991           0.846154           0.846154   \n30           0.815451           0.850427           0.816239   \n31           0.785408           0.829060           0.811966   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.457265           0.472103           0.467811   \n1            0.431624           0.484979           0.442060   \n2            0.457265           0.472103           0.467811   \n3            0.427350           0.424893           0.519313   \n4            0.457265           0.472103           0.467811   \n5            0.405983           0.489270           0.557940   \n6            0.457265           0.472103           0.467811   \n7            0.521368           0.536481           0.424893   \n8            0.816239           0.772532           0.772532   \n9            0.888889           0.768240           0.819742   \n10           0.807692           0.768240           0.763948   \n11           0.811966           0.763948           0.789700   \n12           0.799145           0.746781           0.759657   \n13           0.803419           0.815451           0.802575   \n14           0.811966           0.759657           0.746781   \n15           0.799145           0.759657           0.789700   \n16           0.581197           0.532189           0.566524   \n17           0.589744           0.527897           0.515021   \n18           0.581197           0.532189           0.566524   \n19           0.474359           0.480687           0.536481   \n20           0.581197           0.532189           0.566524   \n21           0.551282           0.527897           0.527897   \n22           0.581197           0.532189           0.566524   \n23           0.598291           0.519313           0.549356   \n24           0.871795           0.776824           0.828326   \n25           0.833333           0.772532           0.866953   \n26           0.850427           0.742489           0.811159   \n27           0.846154           0.798283           0.828326   \n28           0.854701           0.751073           0.824034   \n29           0.846154           0.763948           0.845494   \n30           0.841880           0.733906           0.815451   \n31           0.824786           0.802575           0.871245   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.423077            0.452991            0.465812   \n1             0.478632            0.482906            0.495726   \n2             0.423077            0.452991            0.465812   \n3             0.461538            0.427350            0.568376   \n4             0.423077            0.452991            0.465812   \n5             0.405983            0.602564            0.435897   \n6             0.423077            0.452991            0.465812   \n7             0.491453            0.401709            0.521368   \n8             0.773504            0.824786            0.837607   \n9             0.829060            0.829060            0.837607   \n10            0.752137            0.829060            0.829060   \n11            0.756410            0.782051            0.833333   \n12            0.782051            0.829060            0.829060   \n13            0.799145            0.833333            0.820513   \n14            0.777778            0.811966            0.829060   \n15            0.807692            0.807692            0.790598   \n16            0.559829            0.576923            0.581197   \n17            0.529915            0.517094            0.542735   \n18            0.559829            0.576923            0.581197   \n19            0.521368            0.585470            0.572650   \n20            0.559829            0.576923            0.581197   \n21            0.521368            0.564103            0.517094   \n22            0.559829            0.576923            0.581197   \n23            0.482906            0.547009            0.547009   \n24            0.790598            0.901709            0.820513   \n25            0.841880            0.807692            0.829060   \n26            0.786325            0.888889            0.786325   \n27            0.790598            0.799145            0.841880   \n28            0.760684            0.876068            0.799145   \n29            0.794872            0.807692            0.799145   \n30            0.764957            0.893162            0.773504   \n31            0.833333            0.811966            0.790598   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.493562            0.446352         0.453227        0.024958   \n1             0.416309            0.510730         0.486275        0.047780   \n2             0.493562            0.442060         0.452941        0.025060   \n3             0.454936            0.566524         0.489168        0.052273   \n4             0.493562            0.446352         0.453227        0.024958   \n5             0.446352            0.502146         0.480593        0.061959   \n6             0.493562            0.442060         0.452941        0.025060   \n7             0.433476            0.450644         0.480011        0.042303   \n8             0.849785            0.785408         0.812471        0.029936   \n9             0.836910            0.824034         0.837582        0.031593   \n10            0.811159            0.793991         0.803908        0.033753   \n11            0.824034            0.806867         0.805637        0.029715   \n12            0.828326            0.789700         0.802760        0.033322   \n13            0.798283            0.824034         0.812781        0.026050   \n14            0.841202            0.772532         0.799907        0.037559   \n15            0.763948            0.789700         0.788507        0.026554   \n16            0.549356            0.545064         0.554508        0.023036   \n17            0.480687            0.596567         0.525412        0.034803   \n18            0.549356            0.545064         0.554508        0.023036   \n19            0.523605            0.532189         0.534518        0.037853   \n20            0.549356            0.545064         0.554508        0.023036   \n21            0.484979            0.480687         0.527353        0.035246   \n22            0.549356            0.545064         0.554508        0.023036   \n23            0.549356            0.609442         0.542231        0.045211   \n24            0.832618            0.815451         0.833315        0.034266   \n25            0.854077            0.824034         0.842746        0.028115   \n26            0.836910            0.819742         0.819621        0.035185   \n27            0.875536            0.798283         0.830202        0.026100   \n28            0.854077            0.806867         0.820765        0.036862   \n29            0.815451            0.815451         0.814491        0.028417   \n30            0.858369            0.789700         0.815623        0.041609   \n31            0.858369            0.785408         0.815926        0.024444   \n\n    rank_test_score  \n0                29  \n1                26  \n2                31  \n3                25  \n4                29  \n5                27  \n6                31  \n7                28  \n8                11  \n9                 2  \n10               13  \n11               12  \n12               14  \n13               10  \n14               15  \n15               16  \n16               17  \n17               24  \n18               17  \n19               22  \n20               17  \n21               23  \n22               17  \n23               21  \n24                3  \n25                1  \n26                6  \n27                4  \n28                5  \n29                9  \n30                8  \n31                7  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_splitter</th>\n      <th>param_min_samples_leaf</th>\n      <th>param_max_depth</th>\n      <th>param_criterion</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.004600</td>\n      <td>6.231581e-04</td>\n      <td>0.000601</td>\n      <td>0.000490</td>\n      <td>best</td>\n      <td>1</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453227</td>\n      <td>0.024958</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.002202</td>\n      <td>4.001142e-04</td>\n      <td>0.000133</td>\n      <td>0.000340</td>\n      <td>random</td>\n      <td>1</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.534188</td>\n      <td>0.440171</td>\n      <td>0.602564</td>\n      <td>0.532189</td>\n      <td>0.437768</td>\n      <td>0.521368</td>\n      <td>0.482906</td>\n      <td>0.431624</td>\n      <td>0.484979</td>\n      <td>0.442060</td>\n      <td>0.478632</td>\n      <td>0.482906</td>\n      <td>0.495726</td>\n      <td>0.416309</td>\n      <td>0.510730</td>\n      <td>0.486275</td>\n      <td>0.047780</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.004562</td>\n      <td>7.966420e-04</td>\n      <td>0.000133</td>\n      <td>0.000339</td>\n      <td>best</td>\n      <td>2</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.442060</td>\n      <td>0.452941</td>\n      <td>0.025060</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.001863</td>\n      <td>3.386618e-04</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>random</td>\n      <td>2</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.465812</td>\n      <td>0.512821</td>\n      <td>0.431624</td>\n      <td>0.489270</td>\n      <td>0.523605</td>\n      <td>0.482906</td>\n      <td>0.581197</td>\n      <td>0.427350</td>\n      <td>0.424893</td>\n      <td>0.519313</td>\n      <td>0.461538</td>\n      <td>0.427350</td>\n      <td>0.568376</td>\n      <td>0.454936</td>\n      <td>0.566524</td>\n      <td>0.489168</td>\n      <td>0.052273</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.004066</td>\n      <td>2.510008e-04</td>\n      <td>0.000466</td>\n      <td>0.000498</td>\n      <td>best</td>\n      <td>3</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453227</td>\n      <td>0.024958</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.001656</td>\n      <td>4.680112e-04</td>\n      <td>0.000397</td>\n      <td>0.000486</td>\n      <td>random</td>\n      <td>3</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.465812</td>\n      <td>0.521368</td>\n      <td>0.423077</td>\n      <td>0.412017</td>\n      <td>0.472103</td>\n      <td>0.589744</td>\n      <td>0.478632</td>\n      <td>0.405983</td>\n      <td>0.489270</td>\n      <td>0.557940</td>\n      <td>0.405983</td>\n      <td>0.602564</td>\n      <td>0.435897</td>\n      <td>0.446352</td>\n      <td>0.502146</td>\n      <td>0.480593</td>\n      <td>0.061959</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.004003</td>\n      <td>5.169178e-04</td>\n      <td>0.000267</td>\n      <td>0.000443</td>\n      <td>best</td>\n      <td>4</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.442060</td>\n      <td>0.452941</td>\n      <td>0.025060</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.001868</td>\n      <td>3.401657e-04</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>random</td>\n      <td>4</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.431624</td>\n      <td>0.512821</td>\n      <td>0.452991</td>\n      <td>0.502146</td>\n      <td>0.489270</td>\n      <td>0.534188</td>\n      <td>0.495726</td>\n      <td>0.521368</td>\n      <td>0.536481</td>\n      <td>0.424893</td>\n      <td>0.491453</td>\n      <td>0.401709</td>\n      <td>0.521368</td>\n      <td>0.433476</td>\n      <td>0.450644</td>\n      <td>0.480011</td>\n      <td>0.042303</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.010476</td>\n      <td>9.576567e-04</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>best</td>\n      <td>1</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.799145</td>\n      <td>0.807692</td>\n      <td>0.863248</td>\n      <td>0.806867</td>\n      <td>0.785408</td>\n      <td>0.846154</td>\n      <td>0.846154</td>\n      <td>0.816239</td>\n      <td>0.772532</td>\n      <td>0.772532</td>\n      <td>0.773504</td>\n      <td>0.824786</td>\n      <td>0.837607</td>\n      <td>0.849785</td>\n      <td>0.785408</td>\n      <td>0.812471</td>\n      <td>0.029936</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.004404</td>\n      <td>8.006812e-04</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>random</td>\n      <td>1</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.786325</td>\n      <td>0.876068</td>\n      <td>0.863248</td>\n      <td>0.832618</td>\n      <td>0.832618</td>\n      <td>0.871795</td>\n      <td>0.867521</td>\n      <td>0.888889</td>\n      <td>0.768240</td>\n      <td>0.819742</td>\n      <td>0.829060</td>\n      <td>0.829060</td>\n      <td>0.837607</td>\n      <td>0.836910</td>\n      <td>0.824034</td>\n      <td>0.837582</td>\n      <td>0.031593</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.009609</td>\n      <td>1.144033e-03</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>2</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.752137</td>\n      <td>0.829060</td>\n      <td>0.858974</td>\n      <td>0.798283</td>\n      <td>0.781116</td>\n      <td>0.829060</td>\n      <td>0.854701</td>\n      <td>0.807692</td>\n      <td>0.768240</td>\n      <td>0.763948</td>\n      <td>0.752137</td>\n      <td>0.829060</td>\n      <td>0.829060</td>\n      <td>0.811159</td>\n      <td>0.793991</td>\n      <td>0.803908</td>\n      <td>0.033753</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.003870</td>\n      <td>7.186739e-04</td>\n      <td>0.000600</td>\n      <td>0.000611</td>\n      <td>random</td>\n      <td>2</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.837607</td>\n      <td>0.833333</td>\n      <td>0.824786</td>\n      <td>0.849785</td>\n      <td>0.751073</td>\n      <td>0.811966</td>\n      <td>0.807692</td>\n      <td>0.811966</td>\n      <td>0.763948</td>\n      <td>0.789700</td>\n      <td>0.756410</td>\n      <td>0.782051</td>\n      <td>0.833333</td>\n      <td>0.824034</td>\n      <td>0.806867</td>\n      <td>0.805637</td>\n      <td>0.029715</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.009875</td>\n      <td>1.455898e-03</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>best</td>\n      <td>3</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.756410</td>\n      <td>0.833333</td>\n      <td>0.837607</td>\n      <td>0.789700</td>\n      <td>0.772532</td>\n      <td>0.833333</td>\n      <td>0.854701</td>\n      <td>0.799145</td>\n      <td>0.746781</td>\n      <td>0.759657</td>\n      <td>0.782051</td>\n      <td>0.829060</td>\n      <td>0.829060</td>\n      <td>0.828326</td>\n      <td>0.789700</td>\n      <td>0.802760</td>\n      <td>0.033322</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.004004</td>\n      <td>6.329627e-04</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>random</td>\n      <td>3</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.803419</td>\n      <td>0.841880</td>\n      <td>0.837607</td>\n      <td>0.845494</td>\n      <td>0.776824</td>\n      <td>0.841880</td>\n      <td>0.747863</td>\n      <td>0.803419</td>\n      <td>0.815451</td>\n      <td>0.802575</td>\n      <td>0.799145</td>\n      <td>0.833333</td>\n      <td>0.820513</td>\n      <td>0.798283</td>\n      <td>0.824034</td>\n      <td>0.812781</td>\n      <td>0.026050</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.009008</td>\n      <td>1.033675e-03</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>best</td>\n      <td>4</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.735043</td>\n      <td>0.846154</td>\n      <td>0.841880</td>\n      <td>0.798283</td>\n      <td>0.755365</td>\n      <td>0.833333</td>\n      <td>0.837607</td>\n      <td>0.811966</td>\n      <td>0.759657</td>\n      <td>0.746781</td>\n      <td>0.777778</td>\n      <td>0.811966</td>\n      <td>0.829060</td>\n      <td>0.841202</td>\n      <td>0.772532</td>\n      <td>0.799907</td>\n      <td>0.037559</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.004537</td>\n      <td>3.899957e-03</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>random</td>\n      <td>4</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.717949</td>\n      <td>0.820513</td>\n      <td>0.820513</td>\n      <td>0.776824</td>\n      <td>0.781116</td>\n      <td>0.782051</td>\n      <td>0.820513</td>\n      <td>0.799145</td>\n      <td>0.759657</td>\n      <td>0.789700</td>\n      <td>0.807692</td>\n      <td>0.807692</td>\n      <td>0.790598</td>\n      <td>0.763948</td>\n      <td>0.789700</td>\n      <td>0.788507</td>\n      <td>0.026554</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.006005</td>\n      <td>5.309834e-07</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>best</td>\n      <td>1</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.002402</td>\n      <td>4.903172e-04</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>random</td>\n      <td>1</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.504274</td>\n      <td>0.519313</td>\n      <td>0.557940</td>\n      <td>0.534188</td>\n      <td>0.465812</td>\n      <td>0.589744</td>\n      <td>0.527897</td>\n      <td>0.515021</td>\n      <td>0.529915</td>\n      <td>0.517094</td>\n      <td>0.542735</td>\n      <td>0.480687</td>\n      <td>0.596567</td>\n      <td>0.525412</td>\n      <td>0.034803</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.006272</td>\n      <td>1.124460e-03</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>best</td>\n      <td>2</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.002002</td>\n      <td>5.169145e-04</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>random</td>\n      <td>2</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.525641</td>\n      <td>0.623932</td>\n      <td>0.525641</td>\n      <td>0.570815</td>\n      <td>0.515021</td>\n      <td>0.512821</td>\n      <td>0.517094</td>\n      <td>0.474359</td>\n      <td>0.480687</td>\n      <td>0.536481</td>\n      <td>0.521368</td>\n      <td>0.585470</td>\n      <td>0.572650</td>\n      <td>0.523605</td>\n      <td>0.532189</td>\n      <td>0.534518</td>\n      <td>0.037853</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.005872</td>\n      <td>7.185986e-04</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>best</td>\n      <td>3</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.002335</td>\n      <td>4.717638e-04</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>random</td>\n      <td>3</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.521368</td>\n      <td>0.602564</td>\n      <td>0.495726</td>\n      <td>0.510730</td>\n      <td>0.476395</td>\n      <td>0.551282</td>\n      <td>0.576923</td>\n      <td>0.551282</td>\n      <td>0.527897</td>\n      <td>0.527897</td>\n      <td>0.521368</td>\n      <td>0.564103</td>\n      <td>0.517094</td>\n      <td>0.484979</td>\n      <td>0.480687</td>\n      <td>0.527353</td>\n      <td>0.035246</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.006339</td>\n      <td>1.851655e-03</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>best</td>\n      <td>4</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.001935</td>\n      <td>4.426735e-04</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>random</td>\n      <td>4</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.465812</td>\n      <td>0.615385</td>\n      <td>0.512821</td>\n      <td>0.489270</td>\n      <td>0.515021</td>\n      <td>0.534188</td>\n      <td>0.598291</td>\n      <td>0.598291</td>\n      <td>0.519313</td>\n      <td>0.549356</td>\n      <td>0.482906</td>\n      <td>0.547009</td>\n      <td>0.547009</td>\n      <td>0.549356</td>\n      <td>0.609442</td>\n      <td>0.542231</td>\n      <td>0.045211</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.016415</td>\n      <td>5.455403e-03</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>best</td>\n      <td>1</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.799145</td>\n      <td>0.871795</td>\n      <td>0.833333</td>\n      <td>0.871245</td>\n      <td>0.811159</td>\n      <td>0.863248</td>\n      <td>0.811966</td>\n      <td>0.871795</td>\n      <td>0.776824</td>\n      <td>0.828326</td>\n      <td>0.790598</td>\n      <td>0.901709</td>\n      <td>0.820513</td>\n      <td>0.832618</td>\n      <td>0.815451</td>\n      <td>0.833315</td>\n      <td>0.034266</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.005405</td>\n      <td>6.116578e-04</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>random</td>\n      <td>1</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.820513</td>\n      <td>0.850427</td>\n      <td>0.888889</td>\n      <td>0.858369</td>\n      <td>0.862661</td>\n      <td>0.867521</td>\n      <td>0.863248</td>\n      <td>0.833333</td>\n      <td>0.772532</td>\n      <td>0.866953</td>\n      <td>0.841880</td>\n      <td>0.807692</td>\n      <td>0.829060</td>\n      <td>0.854077</td>\n      <td>0.824034</td>\n      <td>0.842746</td>\n      <td>0.028115</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.013345</td>\n      <td>9.438198e-04</td>\n      <td>0.000334</td>\n      <td>0.000472</td>\n      <td>best</td>\n      <td>2</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.773504</td>\n      <td>0.837607</td>\n      <td>0.833333</td>\n      <td>0.845494</td>\n      <td>0.815451</td>\n      <td>0.850427</td>\n      <td>0.816239</td>\n      <td>0.850427</td>\n      <td>0.742489</td>\n      <td>0.811159</td>\n      <td>0.786325</td>\n      <td>0.888889</td>\n      <td>0.786325</td>\n      <td>0.836910</td>\n      <td>0.819742</td>\n      <td>0.819621</td>\n      <td>0.035185</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.004938</td>\n      <td>7.725001e-04</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>random</td>\n      <td>2</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.803419</td>\n      <td>0.854701</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.820513</td>\n      <td>0.833333</td>\n      <td>0.846154</td>\n      <td>0.798283</td>\n      <td>0.828326</td>\n      <td>0.790598</td>\n      <td>0.799145</td>\n      <td>0.841880</td>\n      <td>0.875536</td>\n      <td>0.798283</td>\n      <td>0.830202</td>\n      <td>0.026100</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.013145</td>\n      <td>7.186813e-04</td>\n      <td>0.000467</td>\n      <td>0.000499</td>\n      <td>best</td>\n      <td>3</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.777778</td>\n      <td>0.867521</td>\n      <td>0.820513</td>\n      <td>0.836910</td>\n      <td>0.811159</td>\n      <td>0.858974</td>\n      <td>0.811966</td>\n      <td>0.854701</td>\n      <td>0.751073</td>\n      <td>0.824034</td>\n      <td>0.760684</td>\n      <td>0.876068</td>\n      <td>0.799145</td>\n      <td>0.854077</td>\n      <td>0.806867</td>\n      <td>0.820765</td>\n      <td>0.036862</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.004404</td>\n      <td>4.901870e-04</td>\n      <td>0.000400</td>\n      <td>0.000490</td>\n      <td>random</td>\n      <td>3</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.760684</td>\n      <td>0.850427</td>\n      <td>0.803419</td>\n      <td>0.828326</td>\n      <td>0.793991</td>\n      <td>0.846154</td>\n      <td>0.846154</td>\n      <td>0.846154</td>\n      <td>0.763948</td>\n      <td>0.845494</td>\n      <td>0.794872</td>\n      <td>0.807692</td>\n      <td>0.799145</td>\n      <td>0.815451</td>\n      <td>0.815451</td>\n      <td>0.814491</td>\n      <td>0.028417</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.012152</td>\n      <td>6.140171e-04</td>\n      <td>0.000405</td>\n      <td>0.000456</td>\n      <td>best</td>\n      <td>4</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.773504</td>\n      <td>0.867521</td>\n      <td>0.816239</td>\n      <td>0.824034</td>\n      <td>0.815451</td>\n      <td>0.850427</td>\n      <td>0.816239</td>\n      <td>0.841880</td>\n      <td>0.733906</td>\n      <td>0.815451</td>\n      <td>0.764957</td>\n      <td>0.893162</td>\n      <td>0.773504</td>\n      <td>0.858369</td>\n      <td>0.789700</td>\n      <td>0.815623</td>\n      <td>0.041609</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.003740</td>\n      <td>6.040129e-04</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>random</td>\n      <td>4</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.794872</td>\n      <td>0.816239</td>\n      <td>0.824786</td>\n      <td>0.798283</td>\n      <td>0.785408</td>\n      <td>0.829060</td>\n      <td>0.811966</td>\n      <td>0.824786</td>\n      <td>0.802575</td>\n      <td>0.871245</td>\n      <td>0.833333</td>\n      <td>0.811966</td>\n      <td>0.790598</td>\n      <td>0.858369</td>\n      <td>0.785408</td>\n      <td>0.815926</td>\n      <td>0.024444</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT_model = DecisionTreeClassifier()\n",
    "\n",
    "DT_params = dict()\n",
    "DT_params['max_depth'] = [3, None]\n",
    "DT_params['min_samples_leaf'] = [1, 2, 3, 4]\n",
    "DT_params['criterion'] = [\"gini\", \"entropy\"]\n",
    "DT_params['splitter'] = [\"best\", \"random\"]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "RCV = RandomizedSearchCV(DT_model, DT_params, n_iter=32, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "DT = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(DT.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[2]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les paramètres testés pour ce modèle sont :\n",
    "-le max_depth, correspondant à la profondeur de l'arbre qui doit être illimité pour garantir une précision maximale du modèle.\n",
    "-le min_samples_leaf, correspondant au nombre d'échantillons minimal par feuille qui doit être de 1.\n",
    "-le criterion est une fonction permettant de mesurer la qualité d'un split, nous obtenons des meilleurs résultats avec 'entropy'.\n",
    "-le splitter est la méthode choisie pour selectionner le split à chaque noeud. Modifier ce paramètre ne semble pas impacter significativement les résultats."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification par boosting de gradient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "642551df",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        1.860865      0.091764         0.002135    3.402124e-04   \n1        4.180197      0.338002         0.005005    5.371375e-03   \n2        9.787858      0.422623         0.008007    3.655138e-04   \n3        3.112160      0.104037         0.003470    4.992149e-04   \n4        6.218692      0.117364         0.006439    4.788414e-04   \n5       15.267482      0.346281         0.014880    1.025138e-03   \n6        4.665943      0.190157         0.005405    2.896335e-03   \n7        8.813086      0.209115         0.009742    2.842043e-03   \n8       12.400834      0.284296         0.012545    1.709015e-03   \n9        7.304439      0.087300         0.007473    4.992636e-04   \n10       8.825154      0.179763         0.008875    3.402000e-04   \n11      10.683914      0.199892         0.010876    4.992487e-04   \n12       1.852483      0.051736         0.002669    5.968192e-04   \n13       3.727525      0.055634         0.003737    4.425920e-04   \n14       9.172070      0.102069         0.008575    8.737316e-04   \n15       3.075772      0.039347         0.003603    6.115175e-04   \n16       6.085162      0.067989         0.006606    4.903751e-04   \n17       9.257386      0.246669         0.009809    1.276587e-03   \n18       4.586366      0.335250         0.005005    3.349194e-07   \n19       6.004254      0.245416         0.006139    3.402842e-04   \n20       7.958798      0.352321         0.008408    8.007646e-04   \n21       4.595140      0.181814         0.005205    9.100483e-04   \n22       5.095971      0.160094         0.007206    4.796041e-03   \n23       6.975237      0.165923         0.007340    4.718650e-04   \n24       1.861291      0.059465         0.002469    4.993826e-04   \n25       3.912555      0.130692         0.004471    1.310992e-03   \n26       9.678761      0.295835         0.009008    1.714337e-03   \n27       3.384874      0.244443         0.003603    4.904465e-04   \n28       5.493424      0.206414         0.005805    5.420857e-04   \n29       7.407395      0.227284         0.007874    4.993911e-04   \n30       3.789311      0.093337         0.004003    3.970463e-07   \n31       4.285960      0.092459         0.004871    3.402718e-04   \n32       6.078656      0.101019         0.006672    4.717301e-04   \n33       3.167279      0.087267         0.003603    4.902389e-04   \n34       3.752246      0.074987         0.004804    1.974987e-03   \n35       5.169856      0.486367         0.005672    1.248382e-03   \n\n   param_n_estimators param_max_depth param_learning_rate  \\\n0                 100               1                 0.1   \n1                 200               1                 0.1   \n2                 500               1                 0.1   \n3                 100               2                 0.1   \n4                 200               2                 0.1   \n5                 500               2                 0.1   \n6                 100               3                 0.1   \n7                 200               3                 0.1   \n8                 500               3                 0.1   \n9                 100               5                 0.1   \n10                200               5                 0.1   \n11                500               5                 0.1   \n12                100               1                 0.2   \n13                200               1                 0.2   \n14                500               1                 0.2   \n15                100               2                 0.2   \n16                200               2                 0.2   \n17                500               2                 0.2   \n18                100               3                 0.2   \n19                200               3                 0.2   \n20                500               3                 0.2   \n21                100               5                 0.2   \n22                200               5                 0.2   \n23                500               5                 0.2   \n24                100               1                 0.3   \n25                200               1                 0.3   \n26                500               1                 0.3   \n27                100               2                 0.3   \n28                200               2                 0.3   \n29                500               2                 0.3   \n30                100               3                 0.3   \n31                200               3                 0.3   \n32                500               3                 0.3   \n33                100               5                 0.3   \n34                200               5                 0.3   \n35                500               5                 0.3   \n\n                                               params  split0_test_score  \\\n0   {'n_estimators': 100, 'max_depth': 1, 'learnin...           0.901709   \n1   {'n_estimators': 200, 'max_depth': 1, 'learnin...           0.923077   \n2   {'n_estimators': 500, 'max_depth': 1, 'learnin...           0.952991   \n3   {'n_estimators': 100, 'max_depth': 2, 'learnin...           0.961538   \n4   {'n_estimators': 200, 'max_depth': 2, 'learnin...           0.974359   \n5   {'n_estimators': 500, 'max_depth': 2, 'learnin...           0.970085   \n6   {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.957265   \n7   {'n_estimators': 200, 'max_depth': 3, 'learnin...           0.965812   \n8   {'n_estimators': 500, 'max_depth': 3, 'learnin...           0.965812   \n9   {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.905983   \n10  {'n_estimators': 200, 'max_depth': 5, 'learnin...           0.901709   \n11  {'n_estimators': 500, 'max_depth': 5, 'learnin...           0.905983   \n12  {'n_estimators': 100, 'max_depth': 1, 'learnin...           0.935897   \n13  {'n_estimators': 200, 'max_depth': 1, 'learnin...           0.957265   \n14  {'n_estimators': 500, 'max_depth': 1, 'learnin...           0.961538   \n15  {'n_estimators': 100, 'max_depth': 2, 'learnin...           0.974359   \n16  {'n_estimators': 200, 'max_depth': 2, 'learnin...           0.974359   \n17  {'n_estimators': 500, 'max_depth': 2, 'learnin...           0.974359   \n18  {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.978632   \n19  {'n_estimators': 200, 'max_depth': 3, 'learnin...           0.974359   \n20  {'n_estimators': 500, 'max_depth': 3, 'learnin...           0.974359   \n21  {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.918803   \n22  {'n_estimators': 200, 'max_depth': 5, 'learnin...           0.914530   \n23  {'n_estimators': 500, 'max_depth': 5, 'learnin...           0.914530   \n24  {'n_estimators': 100, 'max_depth': 1, 'learnin...           0.952991   \n25  {'n_estimators': 200, 'max_depth': 1, 'learnin...           0.948718   \n26  {'n_estimators': 500, 'max_depth': 1, 'learnin...           0.948718   \n27  {'n_estimators': 100, 'max_depth': 2, 'learnin...           0.978632   \n28  {'n_estimators': 200, 'max_depth': 2, 'learnin...           0.974359   \n29  {'n_estimators': 500, 'max_depth': 2, 'learnin...           0.974359   \n30  {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.948718   \n31  {'n_estimators': 200, 'max_depth': 3, 'learnin...           0.965812   \n32  {'n_estimators': 500, 'max_depth': 3, 'learnin...           0.965812   \n33  {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.918803   \n34  {'n_estimators': 200, 'max_depth': 5, 'learnin...           0.935897   \n35  {'n_estimators': 500, 'max_depth': 5, 'learnin...           0.910256   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.952991           0.940171           0.935622   \n1            0.961538           0.952991           0.957082   \n2            0.961538           0.965812           0.961373   \n3            0.970085           0.935897           0.961373   \n4            0.965812           0.948718           0.961373   \n5            0.965812           0.952991           0.961373   \n6            0.952991           0.940171           0.961373   \n7            0.957265           0.940171           0.969957   \n8            0.961538           0.944444           0.965665   \n9            0.948718           0.944444           0.944206   \n10           0.944444           0.952991           0.939914   \n11           0.944444           0.952991           0.939914   \n12           0.961538           0.952991           0.957082   \n13           0.965812           0.957265           0.948498   \n14           0.957265           0.957265           0.965665   \n15           0.974359           0.948718           0.969957   \n16           0.970085           0.957265           0.969957   \n17           0.965812           0.952991           0.969957   \n18           0.961538           0.944444           0.965665   \n19           0.961538           0.944444           0.965665   \n20           0.961538           0.944444           0.965665   \n21           0.948718           0.944444           0.944206   \n22           0.935897           0.944444           0.939914   \n23           0.948718           0.944444           0.948498   \n24           0.961538           0.952991           0.957082   \n25           0.961538           0.961538           0.961373   \n26           0.957265           0.961538           0.965665   \n27           0.974359           0.952991           0.969957   \n28           0.965812           0.952991           0.969957   \n29           0.965812           0.952991           0.969957   \n30           0.961538           0.952991           0.974249   \n31           0.961538           0.952991           0.969957   \n32           0.961538           0.944444           0.965665   \n33           0.948718           0.940171           0.948498   \n34           0.948718           0.944444           0.952790   \n35           0.948718           0.940171           0.948498   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.922747           0.927350           0.961538   \n1            0.931330           0.957265           0.974359   \n2            0.935622           0.961538           0.952991   \n3            0.939914           0.952991           0.965812   \n4            0.957082           0.965812           0.965812   \n5            0.961373           0.961538           0.970085   \n6            0.957082           0.957265           0.978632   \n7            0.952790           0.952991           0.982906   \n8            0.957082           0.957265           0.982906   \n9            0.948498           0.948718           0.948718   \n10           0.948498           0.940171           0.948718   \n11           0.948498           0.931624           0.948718   \n12           0.931330           0.961538           0.974359   \n13           0.939914           0.965812           0.965812   \n14           0.948498           0.961538           0.961538   \n15           0.957082           0.965812           0.970085   \n16           0.965665           0.957265           0.982906   \n17           0.965665           0.961538           0.974359   \n18           0.957082           0.948718           0.982906   \n19           0.957082           0.952991           0.982906   \n20           0.952790           0.957265           0.982906   \n21           0.952790           0.957265           0.948718   \n22           0.952790           0.944444           0.957265   \n23           0.948498           0.948718           0.952991   \n24           0.935622           0.965812           0.961538   \n25           0.944206           0.957265           0.965812   \n26           0.948498           0.961538           0.965812   \n27           0.957082           0.961538           0.974359   \n28           0.961373           0.965812           0.978632   \n29           0.961373           0.965812           0.978632   \n30           0.965665           0.952991           0.978632   \n31           0.952790           0.952991           0.978632   \n32           0.961373           0.957265           0.982906   \n33           0.944206           0.948718           0.961538   \n34           0.952790           0.944444           0.965812   \n35           0.948498           0.948718           0.952991   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.940171           0.931330           0.927039   \n1            0.948718           0.939914           0.944206   \n2            0.957265           0.948498           0.957082   \n3            0.970085           0.935622           0.957082   \n4            0.974359           0.939914           0.957082   \n5            0.978632           0.961373           0.965665   \n6            0.961538           0.939914           0.957082   \n7            0.970085           0.944206           0.965665   \n8            0.974359           0.939914           0.965665   \n9            0.944444           0.901288           0.948498   \n10           0.944444           0.901288           0.948498   \n11           0.935897           0.901288           0.948498   \n12           0.952991           0.935622           0.944206   \n13           0.961538           0.948498           0.952790   \n14           0.961538           0.952790           0.961373   \n15           0.978632           0.939914           0.969957   \n16           0.978632           0.957082           0.965665   \n17           0.978632           0.961373           0.969957   \n18           0.970085           0.957082           0.965665   \n19           0.974359           0.961373           0.969957   \n20           0.970085           0.961373           0.974249   \n21           0.952991           0.927039           0.944206   \n22           0.944444           0.918455           0.948498   \n23           0.948718           0.927039           0.957082   \n24           0.961538           0.939914           0.948498   \n25           0.961538           0.948498           0.961373   \n26           0.965812           0.944206           0.969957   \n27           0.974359           0.952790           0.965665   \n28           0.974359           0.961373           0.974249   \n29           0.974359           0.961373           0.974249   \n30           0.974359           0.961373           0.978541   \n31           0.970085           0.952790           0.965665   \n32           0.965812           0.952790           0.965665   \n33           0.952991           0.914163           0.944206   \n34           0.957265           0.922747           0.944206   \n35           0.961538           0.931330           0.961373   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.931624            0.897436            0.957265   \n1             0.948718            0.905983            0.974359   \n2             0.961538            0.923077            0.970085   \n3             0.935897            0.952991            0.970085   \n4             0.948718            0.957265            0.974359   \n5             0.957265            0.952991            0.978632   \n6             0.935897            0.952991            0.965812   \n7             0.944444            0.952991            0.965812   \n8             0.948718            0.952991            0.974359   \n9             0.918803            0.952991            0.965812   \n10            0.931624            0.944444            0.970085   \n11            0.935897            0.948718            0.961538   \n12            0.952991            0.910256            0.978632   \n13            0.957265            0.923077            0.970085   \n14            0.957265            0.931624            0.970085   \n15            0.948718            0.952991            0.974359   \n16            0.952991            0.952991            0.978632   \n17            0.957265            0.952991            0.974359   \n18            0.952991            0.952991            0.965812   \n19            0.957265            0.948718            0.970085   \n20            0.957265            0.952991            0.970085   \n21            0.910256            0.952991            0.961538   \n22            0.923077            0.940171            0.952991   \n23            0.914530            0.944444            0.965812   \n24            0.957265            0.914530            0.970085   \n25            0.961538            0.923077            0.970085   \n26            0.952991            0.931624            0.961538   \n27            0.952991            0.961538            0.970085   \n28            0.952991            0.948718            0.974359   \n29            0.952991            0.948718            0.978632   \n30            0.952991            0.952991            0.974359   \n31            0.952991            0.952991            0.970085   \n32            0.952991            0.957265            0.970085   \n33            0.918803            0.970085            0.970085   \n34            0.931624            0.965812            0.961538   \n35            0.923077            0.965812            0.957265   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.948498            0.914163         0.932644        0.018123   \n1             0.969957            0.939914         0.948627        0.018354   \n2             0.969957            0.957082         0.955763        0.012075   \n3             0.978541            0.961373         0.956619        0.013623   \n4             0.978541            0.969957         0.962611        0.010786   \n5             0.974249            0.961373         0.964896        0.007843   \n6             0.957082            0.948498         0.954906        0.010471   \n7             0.961373            0.948498         0.958331        0.011431   \n8             0.969957            0.952790         0.960898        0.011588   \n9             0.944206            0.935622         0.940063        0.017140   \n10            0.944206            0.944206         0.940349        0.017170   \n11            0.957082            0.952790         0.940925        0.016635   \n12            0.974249            0.944206         0.951193        0.017746   \n13            0.969957            0.957082         0.956045        0.012067   \n14            0.974249            0.957082         0.958621        0.009452   \n15            0.969957            0.957082         0.963466        0.011380   \n16            0.969957            0.965665         0.966608        0.009267   \n17            0.969957            0.965665         0.966326        0.007620   \n18            0.961373            0.961373         0.961757        0.010053   \n19            0.978541            0.961373         0.964044        0.010674   \n20            0.978541            0.961373         0.964329        0.010316   \n21            0.952790            0.944206         0.944064        0.013918   \n22            0.965665            0.939914         0.941500        0.013670   \n23            0.952790            0.957082         0.944926        0.014366   \n24            0.974249            0.948498         0.953477        0.014495   \n25            0.969957            0.965665         0.957479        0.011732   \n26            0.974249            0.952790         0.957480        0.010760   \n27            0.969957            0.965665         0.965465        0.008348   \n28            0.969957            0.965665         0.966041        0.008751   \n29            0.969957            0.965665         0.966326        0.009080   \n30            0.974249            0.957082         0.964049        0.010431   \n31            0.987124            0.952790         0.962616        0.010657   \n32            0.978541            0.957082         0.962616        0.009557   \n33            0.969957            0.944206         0.946343        0.017474   \n34            0.952790            0.944206         0.948339        0.011769   \n35            0.948498            0.939914         0.945777        0.014533   \n\n    rank_test_score  \n0                36  \n1                26  \n2                22  \n3                20  \n4                13  \n5                 6  \n6                23  \n7                17  \n8                15  \n9                35  \n10               34  \n11               33  \n12               25  \n13               21  \n14               16  \n15               10  \n16                1  \n17                2  \n18               14  \n19                9  \n20                7  \n21               31  \n22               32  \n23               30  \n24               24  \n25               19  \n26               18  \n27                5  \n28                4  \n29                2  \n30                8  \n31               11  \n32               11  \n33               28  \n34               27  \n35               29  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_n_estimators</th>\n      <th>param_max_depth</th>\n      <th>param_learning_rate</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.860865</td>\n      <td>0.091764</td>\n      <td>0.002135</td>\n      <td>3.402124e-04</td>\n      <td>100</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 1, 'learnin...</td>\n      <td>0.901709</td>\n      <td>0.952991</td>\n      <td>0.940171</td>\n      <td>0.935622</td>\n      <td>0.922747</td>\n      <td>0.927350</td>\n      <td>0.961538</td>\n      <td>0.940171</td>\n      <td>0.931330</td>\n      <td>0.927039</td>\n      <td>0.931624</td>\n      <td>0.897436</td>\n      <td>0.957265</td>\n      <td>0.948498</td>\n      <td>0.914163</td>\n      <td>0.932644</td>\n      <td>0.018123</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.180197</td>\n      <td>0.338002</td>\n      <td>0.005005</td>\n      <td>5.371375e-03</td>\n      <td>200</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 200, 'max_depth': 1, 'learnin...</td>\n      <td>0.923077</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.931330</td>\n      <td>0.957265</td>\n      <td>0.974359</td>\n      <td>0.948718</td>\n      <td>0.939914</td>\n      <td>0.944206</td>\n      <td>0.948718</td>\n      <td>0.905983</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.939914</td>\n      <td>0.948627</td>\n      <td>0.018354</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9.787858</td>\n      <td>0.422623</td>\n      <td>0.008007</td>\n      <td>3.655138e-04</td>\n      <td>500</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 500, 'max_depth': 1, 'learnin...</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.961373</td>\n      <td>0.935622</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.957265</td>\n      <td>0.948498</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.923077</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.955763</td>\n      <td>0.012075</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.112160</td>\n      <td>0.104037</td>\n      <td>0.003470</td>\n      <td>4.992149e-04</td>\n      <td>100</td>\n      <td>2</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 2, 'learnin...</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.935897</td>\n      <td>0.961373</td>\n      <td>0.939914</td>\n      <td>0.952991</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.935622</td>\n      <td>0.957082</td>\n      <td>0.935897</td>\n      <td>0.952991</td>\n      <td>0.970085</td>\n      <td>0.978541</td>\n      <td>0.961373</td>\n      <td>0.956619</td>\n      <td>0.013623</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6.218692</td>\n      <td>0.117364</td>\n      <td>0.006439</td>\n      <td>4.788414e-04</td>\n      <td>200</td>\n      <td>2</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 200, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.948718</td>\n      <td>0.961373</td>\n      <td>0.957082</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.939914</td>\n      <td>0.957082</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.962611</td>\n      <td>0.010786</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>15.267482</td>\n      <td>0.346281</td>\n      <td>0.014880</td>\n      <td>1.025138e-03</td>\n      <td>500</td>\n      <td>2</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 500, 'max_depth': 2, 'learnin...</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.961373</td>\n      <td>0.961373</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.978632</td>\n      <td>0.961373</td>\n      <td>0.965665</td>\n      <td>0.957265</td>\n      <td>0.952991</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964896</td>\n      <td>0.007843</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4.665943</td>\n      <td>0.190157</td>\n      <td>0.005405</td>\n      <td>2.896335e-03</td>\n      <td>100</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.957265</td>\n      <td>0.952991</td>\n      <td>0.940171</td>\n      <td>0.961373</td>\n      <td>0.957082</td>\n      <td>0.957265</td>\n      <td>0.978632</td>\n      <td>0.961538</td>\n      <td>0.939914</td>\n      <td>0.957082</td>\n      <td>0.935897</td>\n      <td>0.952991</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.948498</td>\n      <td>0.954906</td>\n      <td>0.010471</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8.813086</td>\n      <td>0.209115</td>\n      <td>0.009742</td>\n      <td>2.842043e-03</td>\n      <td>200</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 200, 'max_depth': 3, 'learnin...</td>\n      <td>0.965812</td>\n      <td>0.957265</td>\n      <td>0.940171</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.952991</td>\n      <td>0.982906</td>\n      <td>0.970085</td>\n      <td>0.944206</td>\n      <td>0.965665</td>\n      <td>0.944444</td>\n      <td>0.952991</td>\n      <td>0.965812</td>\n      <td>0.961373</td>\n      <td>0.948498</td>\n      <td>0.958331</td>\n      <td>0.011431</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>12.400834</td>\n      <td>0.284296</td>\n      <td>0.012545</td>\n      <td>1.709015e-03</td>\n      <td>500</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 500, 'max_depth': 3, 'learnin...</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.944444</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.939914</td>\n      <td>0.965665</td>\n      <td>0.948718</td>\n      <td>0.952991</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.960898</td>\n      <td>0.011588</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>7.304439</td>\n      <td>0.087300</td>\n      <td>0.007473</td>\n      <td>4.992636e-04</td>\n      <td>100</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.905983</td>\n      <td>0.948718</td>\n      <td>0.944444</td>\n      <td>0.944206</td>\n      <td>0.948498</td>\n      <td>0.948718</td>\n      <td>0.948718</td>\n      <td>0.944444</td>\n      <td>0.901288</td>\n      <td>0.948498</td>\n      <td>0.918803</td>\n      <td>0.952991</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.935622</td>\n      <td>0.940063</td>\n      <td>0.017140</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>8.825154</td>\n      <td>0.179763</td>\n      <td>0.008875</td>\n      <td>3.402000e-04</td>\n      <td>200</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 200, 'max_depth': 5, 'learnin...</td>\n      <td>0.901709</td>\n      <td>0.944444</td>\n      <td>0.952991</td>\n      <td>0.939914</td>\n      <td>0.948498</td>\n      <td>0.940171</td>\n      <td>0.948718</td>\n      <td>0.944444</td>\n      <td>0.901288</td>\n      <td>0.948498</td>\n      <td>0.931624</td>\n      <td>0.944444</td>\n      <td>0.970085</td>\n      <td>0.944206</td>\n      <td>0.944206</td>\n      <td>0.940349</td>\n      <td>0.017170</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>10.683914</td>\n      <td>0.199892</td>\n      <td>0.010876</td>\n      <td>4.992487e-04</td>\n      <td>500</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 500, 'max_depth': 5, 'learnin...</td>\n      <td>0.905983</td>\n      <td>0.944444</td>\n      <td>0.952991</td>\n      <td>0.939914</td>\n      <td>0.948498</td>\n      <td>0.931624</td>\n      <td>0.948718</td>\n      <td>0.935897</td>\n      <td>0.901288</td>\n      <td>0.948498</td>\n      <td>0.935897</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.957082</td>\n      <td>0.952790</td>\n      <td>0.940925</td>\n      <td>0.016635</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.852483</td>\n      <td>0.051736</td>\n      <td>0.002669</td>\n      <td>5.968192e-04</td>\n      <td>100</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 100, 'max_depth': 1, 'learnin...</td>\n      <td>0.935897</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.931330</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.935622</td>\n      <td>0.944206</td>\n      <td>0.952991</td>\n      <td>0.910256</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.944206</td>\n      <td>0.951193</td>\n      <td>0.017746</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3.727525</td>\n      <td>0.055634</td>\n      <td>0.003737</td>\n      <td>4.425920e-04</td>\n      <td>200</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 200, 'max_depth': 1, 'learnin...</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957265</td>\n      <td>0.948498</td>\n      <td>0.939914</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.948498</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.923077</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.956045</td>\n      <td>0.012067</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>9.172070</td>\n      <td>0.102069</td>\n      <td>0.008575</td>\n      <td>8.737316e-04</td>\n      <td>500</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 500, 'max_depth': 1, 'learnin...</td>\n      <td>0.961538</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.952790</td>\n      <td>0.961373</td>\n      <td>0.957265</td>\n      <td>0.931624</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.958621</td>\n      <td>0.009452</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>3.075772</td>\n      <td>0.039347</td>\n      <td>0.003603</td>\n      <td>6.115175e-04</td>\n      <td>100</td>\n      <td>2</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 100, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.948718</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.978632</td>\n      <td>0.939914</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.952991</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.963466</td>\n      <td>0.011380</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>6.085162</td>\n      <td>0.067989</td>\n      <td>0.006606</td>\n      <td>4.903751e-04</td>\n      <td>200</td>\n      <td>2</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 200, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.952991</td>\n      <td>0.952991</td>\n      <td>0.978632</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.966608</td>\n      <td>0.009267</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>9.257386</td>\n      <td>0.246669</td>\n      <td>0.009809</td>\n      <td>1.276587e-03</td>\n      <td>500</td>\n      <td>2</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 500, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.957265</td>\n      <td>0.952991</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.966326</td>\n      <td>0.007620</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>4.586366</td>\n      <td>0.335250</td>\n      <td>0.005005</td>\n      <td>3.349194e-07</td>\n      <td>100</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.978632</td>\n      <td>0.961538</td>\n      <td>0.944444</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.948718</td>\n      <td>0.982906</td>\n      <td>0.970085</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.952991</td>\n      <td>0.952991</td>\n      <td>0.965812</td>\n      <td>0.961373</td>\n      <td>0.961373</td>\n      <td>0.961757</td>\n      <td>0.010053</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>6.004254</td>\n      <td>0.245416</td>\n      <td>0.006139</td>\n      <td>3.402842e-04</td>\n      <td>200</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 200, 'max_depth': 3, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.961538</td>\n      <td>0.944444</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.952991</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.957265</td>\n      <td>0.948718</td>\n      <td>0.970085</td>\n      <td>0.978541</td>\n      <td>0.961373</td>\n      <td>0.964044</td>\n      <td>0.010674</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>7.958798</td>\n      <td>0.352321</td>\n      <td>0.008408</td>\n      <td>8.007646e-04</td>\n      <td>500</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 500, 'max_depth': 3, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.961538</td>\n      <td>0.944444</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.970085</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.957265</td>\n      <td>0.952991</td>\n      <td>0.970085</td>\n      <td>0.978541</td>\n      <td>0.961373</td>\n      <td>0.964329</td>\n      <td>0.010316</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>4.595140</td>\n      <td>0.181814</td>\n      <td>0.005205</td>\n      <td>9.100483e-04</td>\n      <td>100</td>\n      <td>5</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.918803</td>\n      <td>0.948718</td>\n      <td>0.944444</td>\n      <td>0.944206</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.948718</td>\n      <td>0.952991</td>\n      <td>0.927039</td>\n      <td>0.944206</td>\n      <td>0.910256</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.952790</td>\n      <td>0.944206</td>\n      <td>0.944064</td>\n      <td>0.013918</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>5.095971</td>\n      <td>0.160094</td>\n      <td>0.007206</td>\n      <td>4.796041e-03</td>\n      <td>200</td>\n      <td>5</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 200, 'max_depth': 5, 'learnin...</td>\n      <td>0.914530</td>\n      <td>0.935897</td>\n      <td>0.944444</td>\n      <td>0.939914</td>\n      <td>0.952790</td>\n      <td>0.944444</td>\n      <td>0.957265</td>\n      <td>0.944444</td>\n      <td>0.918455</td>\n      <td>0.948498</td>\n      <td>0.923077</td>\n      <td>0.940171</td>\n      <td>0.952991</td>\n      <td>0.965665</td>\n      <td>0.939914</td>\n      <td>0.941500</td>\n      <td>0.013670</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>6.975237</td>\n      <td>0.165923</td>\n      <td>0.007340</td>\n      <td>4.718650e-04</td>\n      <td>500</td>\n      <td>5</td>\n      <td>0.2</td>\n      <td>{'n_estimators': 500, 'max_depth': 5, 'learnin...</td>\n      <td>0.914530</td>\n      <td>0.948718</td>\n      <td>0.944444</td>\n      <td>0.948498</td>\n      <td>0.948498</td>\n      <td>0.948718</td>\n      <td>0.952991</td>\n      <td>0.948718</td>\n      <td>0.927039</td>\n      <td>0.957082</td>\n      <td>0.914530</td>\n      <td>0.944444</td>\n      <td>0.965812</td>\n      <td>0.952790</td>\n      <td>0.957082</td>\n      <td>0.944926</td>\n      <td>0.014366</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1.861291</td>\n      <td>0.059465</td>\n      <td>0.002469</td>\n      <td>4.993826e-04</td>\n      <td>100</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 100, 'max_depth': 1, 'learnin...</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.939914</td>\n      <td>0.948498</td>\n      <td>0.957265</td>\n      <td>0.914530</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.948498</td>\n      <td>0.953477</td>\n      <td>0.014495</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>3.912555</td>\n      <td>0.130692</td>\n      <td>0.004471</td>\n      <td>1.310992e-03</td>\n      <td>200</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 200, 'max_depth': 1, 'learnin...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.944206</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.948498</td>\n      <td>0.961373</td>\n      <td>0.961538</td>\n      <td>0.923077</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.957479</td>\n      <td>0.011732</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>9.678761</td>\n      <td>0.295835</td>\n      <td>0.009008</td>\n      <td>1.714337e-03</td>\n      <td>500</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 500, 'max_depth': 1, 'learnin...</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.961538</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.969957</td>\n      <td>0.952991</td>\n      <td>0.931624</td>\n      <td>0.961538</td>\n      <td>0.974249</td>\n      <td>0.952790</td>\n      <td>0.957480</td>\n      <td>0.010760</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>3.384874</td>\n      <td>0.244443</td>\n      <td>0.003603</td>\n      <td>4.904465e-04</td>\n      <td>100</td>\n      <td>2</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 100, 'max_depth': 2, 'learnin...</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.952790</td>\n      <td>0.965665</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.965465</td>\n      <td>0.008348</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>5.493424</td>\n      <td>0.206414</td>\n      <td>0.005805</td>\n      <td>5.420857e-04</td>\n      <td>200</td>\n      <td>2</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 200, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.969957</td>\n      <td>0.961373</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.952991</td>\n      <td>0.948718</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.966041</td>\n      <td>0.008751</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>7.407395</td>\n      <td>0.227284</td>\n      <td>0.007874</td>\n      <td>4.993911e-04</td>\n      <td>500</td>\n      <td>2</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 500, 'max_depth': 2, 'learnin...</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.969957</td>\n      <td>0.961373</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.952991</td>\n      <td>0.948718</td>\n      <td>0.978632</td>\n      <td>0.969957</td>\n      <td>0.965665</td>\n      <td>0.966326</td>\n      <td>0.009080</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>3.789311</td>\n      <td>0.093337</td>\n      <td>0.004003</td>\n      <td>3.970463e-07</td>\n      <td>100</td>\n      <td>3</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.952991</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.978541</td>\n      <td>0.952991</td>\n      <td>0.952991</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.964049</td>\n      <td>0.010431</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>4.285960</td>\n      <td>0.092459</td>\n      <td>0.004871</td>\n      <td>3.402718e-04</td>\n      <td>200</td>\n      <td>3</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 200, 'max_depth': 3, 'learnin...</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.952991</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.952790</td>\n      <td>0.965665</td>\n      <td>0.952991</td>\n      <td>0.952991</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.952790</td>\n      <td>0.962616</td>\n      <td>0.010657</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>6.078656</td>\n      <td>0.101019</td>\n      <td>0.006672</td>\n      <td>4.717301e-04</td>\n      <td>500</td>\n      <td>3</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 500, 'max_depth': 3, 'learnin...</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.944444</td>\n      <td>0.965665</td>\n      <td>0.961373</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.952790</td>\n      <td>0.965665</td>\n      <td>0.952991</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.978541</td>\n      <td>0.957082</td>\n      <td>0.962616</td>\n      <td>0.009557</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>3.167279</td>\n      <td>0.087267</td>\n      <td>0.003603</td>\n      <td>4.902389e-04</td>\n      <td>100</td>\n      <td>5</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.918803</td>\n      <td>0.948718</td>\n      <td>0.940171</td>\n      <td>0.948498</td>\n      <td>0.944206</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.914163</td>\n      <td>0.944206</td>\n      <td>0.918803</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.944206</td>\n      <td>0.946343</td>\n      <td>0.017474</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>3.752246</td>\n      <td>0.074987</td>\n      <td>0.004804</td>\n      <td>1.974987e-03</td>\n      <td>200</td>\n      <td>5</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 200, 'max_depth': 5, 'learnin...</td>\n      <td>0.935897</td>\n      <td>0.948718</td>\n      <td>0.944444</td>\n      <td>0.952790</td>\n      <td>0.952790</td>\n      <td>0.944444</td>\n      <td>0.965812</td>\n      <td>0.957265</td>\n      <td>0.922747</td>\n      <td>0.944206</td>\n      <td>0.931624</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.952790</td>\n      <td>0.944206</td>\n      <td>0.948339</td>\n      <td>0.011769</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>5.169856</td>\n      <td>0.486367</td>\n      <td>0.005672</td>\n      <td>1.248382e-03</td>\n      <td>500</td>\n      <td>5</td>\n      <td>0.3</td>\n      <td>{'n_estimators': 500, 'max_depth': 5, 'learnin...</td>\n      <td>0.910256</td>\n      <td>0.948718</td>\n      <td>0.940171</td>\n      <td>0.948498</td>\n      <td>0.948498</td>\n      <td>0.948718</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.931330</td>\n      <td>0.961373</td>\n      <td>0.923077</td>\n      <td>0.965812</td>\n      <td>0.957265</td>\n      <td>0.948498</td>\n      <td>0.939914</td>\n      <td>0.945777</td>\n      <td>0.014533</td>\n      <td>29</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB_model = GradientBoostingClassifier()\n",
    "GB_params = dict()\n",
    "GB_params['n_estimators'] = [100, 200, 500]\n",
    "GB_params['max_depth'] = [1, 2, 3, 5]\n",
    "GB_params['learning_rate'] = [0.1, 0.2, 0.3]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(GB_model, GB_params, n_iter=36, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "GB = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(GB.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[3]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour améliorer la précision de notre modèle, nous pouvons faire varrier les trois paramètes suivants :\n",
    "* Le nombre d'estimation\n",
    "* La profondeur maximale\n",
    "* Le taux d'apprentissage\n",
    "\n",
    "#### Le nombre d'estimation\n",
    "\n",
    "Ce paramètre permet de jouer sur le nombre d'arbre du modèle.\n",
    "Plus ce paramètre est élevé, plus le modèle est précis.\n",
    "\n",
    "Néanmoins, une valeur trop élevé peut causer des problèmes de performance lors de l'apprentissage.\n",
    "\n",
    "#### La profondeur maximale\n",
    "\n",
    "Cette valeur définit la profondeur des arbres utilisé.\n",
    "\n",
    "Il est important de choisir une profondeur adaptée à notre jeu d'entrainement au risque de faire de l'`over fitting`.\n",
    "\n",
    "#### Le taux d'apprentissage\n",
    "\n",
    "Il permet d'éviter l'`under fitting` et l'`over fitting`.\n",
    "Un taux d'apprentissage trop élevé pourrait amener à de l'`over fitting` et donc un mauvais résultat avec le jeu de test.\n",
    "A l'inverse, un taux d'apprentissage trop faible conduirait à de l'`under fitting`.\n",
    "\n",
    "---\n",
    "\n",
    "> Source : [https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae](https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae)\n",
    "\n",
    "---\n",
    "\n",
    "### Analyse des résultats\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "   param_n_estimators param_max_depth param_learning_rate  mean_test_score  \\\n16                200               2                 0.2         0.966608   \n17                500               2                 0.2         0.966326   \n29                500               2                 0.3         0.966326   \n28                200               2                 0.3         0.966041   \n27                100               2                 0.3         0.965465   \n5                 500               2                 0.1         0.964896   \n20                500               3                 0.2         0.964329   \n30                100               3                 0.3         0.964049   \n19                200               3                 0.2         0.964044   \n15                100               2                 0.2         0.963466   \n32                500               3                 0.3         0.962616   \n31                200               3                 0.3         0.962616   \n4                 200               2                 0.1         0.962611   \n18                100               3                 0.2         0.961757   \n8                 500               3                 0.1         0.960898   \n14                500               1                 0.2         0.958621   \n7                 200               3                 0.1         0.958331   \n26                500               1                 0.3         0.957480   \n25                200               1                 0.3         0.957479   \n3                 100               2                 0.1         0.956619   \n13                200               1                 0.2         0.956045   \n2                 500               1                 0.1         0.955763   \n6                 100               3                 0.1         0.954906   \n24                100               1                 0.3         0.953477   \n12                100               1                 0.2         0.951193   \n1                 200               1                 0.1         0.948627   \n34                200               5                 0.3         0.948339   \n33                100               5                 0.3         0.946343   \n35                500               5                 0.3         0.945777   \n23                500               5                 0.2         0.944926   \n21                100               5                 0.2         0.944064   \n22                200               5                 0.2         0.941500   \n11                500               5                 0.1         0.940925   \n10                200               5                 0.1         0.940349   \n9                 100               5                 0.1         0.940063   \n0                 100               1                 0.1         0.932644   \n\n    rank_test_score  \n16                1  \n17                2  \n29                2  \n28                4  \n27                5  \n5                 6  \n20                7  \n30                8  \n19                9  \n15               10  \n32               11  \n31               11  \n4                13  \n18               14  \n8                15  \n14               16  \n7                17  \n26               18  \n25               19  \n3                20  \n13               21  \n2                22  \n6                23  \n24               24  \n12               25  \n1                26  \n34               27  \n33               28  \n35               29  \n23               30  \n21               31  \n22               32  \n11               33  \n10               34  \n9                35  \n0                36  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>param_n_estimators</th>\n      <th>param_max_depth</th>\n      <th>param_learning_rate</th>\n      <th>mean_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>16</th>\n      <td>200</td>\n      <td>2</td>\n      <td>0.2</td>\n      <td>0.966608</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>500</td>\n      <td>2</td>\n      <td>0.2</td>\n      <td>0.966326</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>500</td>\n      <td>2</td>\n      <td>0.3</td>\n      <td>0.966326</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>200</td>\n      <td>2</td>\n      <td>0.3</td>\n      <td>0.966041</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>100</td>\n      <td>2</td>\n      <td>0.3</td>\n      <td>0.965465</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>500</td>\n      <td>2</td>\n      <td>0.1</td>\n      <td>0.964896</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>500</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>0.964329</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>100</td>\n      <td>3</td>\n      <td>0.3</td>\n      <td>0.964049</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>200</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>0.964044</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>100</td>\n      <td>2</td>\n      <td>0.2</td>\n      <td>0.963466</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>500</td>\n      <td>3</td>\n      <td>0.3</td>\n      <td>0.962616</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>200</td>\n      <td>3</td>\n      <td>0.3</td>\n      <td>0.962616</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>200</td>\n      <td>2</td>\n      <td>0.1</td>\n      <td>0.962611</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>100</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>0.961757</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>500</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>0.960898</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>500</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>0.958621</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>200</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>0.958331</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>500</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>0.957480</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>200</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>0.957479</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100</td>\n      <td>2</td>\n      <td>0.1</td>\n      <td>0.956619</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>200</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>0.956045</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>500</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>0.955763</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>100</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>0.954906</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>100</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>0.953477</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>100</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>0.951193</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>0.948627</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>200</td>\n      <td>5</td>\n      <td>0.3</td>\n      <td>0.948339</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>100</td>\n      <td>5</td>\n      <td>0.3</td>\n      <td>0.946343</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>500</td>\n      <td>5</td>\n      <td>0.3</td>\n      <td>0.945777</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>500</td>\n      <td>5</td>\n      <td>0.2</td>\n      <td>0.944926</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>100</td>\n      <td>5</td>\n      <td>0.2</td>\n      <td>0.944064</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>200</td>\n      <td>5</td>\n      <td>0.2</td>\n      <td>0.941500</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>500</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>0.940925</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>200</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>0.940349</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>100</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>0.940063</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>0.932644</td>\n      <td>36</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results[[\"param_n_estimators\",\"param_max_depth\",\"param_learning_rate\",\"mean_test_score\",\"rank_test_score\"]].sort_values(\"rank_test_score\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le meilleur résultat semble être avec la configuration suivante :\n",
    "\n",
    "|  Paramètre  | Valeur  |\n",
    "|---|---|\n",
    "| n_estimators |  100 |\n",
    "| max_depth  | 3  |\n",
    "|  learning_rate | 0.1  |\n",
    "\n",
    "> Nous avons essayé d'augmenté *le nombre d'itération* mais cela a un impacte négligeable sur le résultat."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification par machine à vecteurs de support"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dev\\machine_learning\\tp\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 12 is smaller than n_iter=50. Running 12 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_gamma  \\\n0        0.100157      0.007800         0.031963        0.003500        0.01   \n1        0.066628      0.006429         0.035933        0.007941       0.001   \n2        0.086846      0.005563         0.032296        0.002017      0.0001   \n3        0.102226      0.004307         0.032630        0.002527        0.01   \n4        0.041704      0.002024         0.019217        0.000833       0.001   \n5        0.042972      0.001770         0.025690        0.001076      0.0001   \n6        0.113369      0.003196         0.032096        0.000999        0.01   \n7        0.039502      0.001587         0.017883        0.000719       0.001   \n8        0.032563      0.001629         0.021886        0.001409      0.0001   \n9        0.113303      0.002641         0.030961        0.002382        0.01   \n10       0.038902      0.001670         0.018217        0.000749       0.001   \n11       0.025954      0.001237         0.016484        0.002532      0.0001   \n\n   param_C                       params  split0_test_score  split1_test_score  \\\n0      0.1    {'gamma': 0.01, 'C': 0.1}           0.106838           0.111111   \n1      0.1   {'gamma': 0.001, 'C': 0.1}           0.944444           0.952991   \n2      0.1  {'gamma': 0.0001, 'C': 0.1}           0.773504           0.816239   \n3      0.5    {'gamma': 0.01, 'C': 0.5}           0.209402           0.200855   \n4      0.5   {'gamma': 0.001, 'C': 0.5}           0.982906           0.991453   \n5      0.5  {'gamma': 0.0001, 'C': 0.5}           0.944444           0.944444   \n6        1      {'gamma': 0.01, 'C': 1}           0.696581           0.739316   \n7        1     {'gamma': 0.001, 'C': 1}           0.991453           0.991453   \n8        1    {'gamma': 0.0001, 'C': 1}           0.961538           0.957265   \n9        2      {'gamma': 0.01, 'C': 2}           0.730769           0.782051   \n10       2     {'gamma': 0.001, 'C': 2}           0.987179           0.991453   \n11       2    {'gamma': 0.0001, 'C': 2}           0.982906           0.970085   \n\n    split2_test_score  split3_test_score  split4_test_score  \\\n0            0.111111           0.111588           0.111588   \n1            0.948718           0.961373           0.935622   \n2            0.799145           0.763948           0.793991   \n3            0.226496           0.193133           0.206009   \n4            0.978632           0.987124           0.987124   \n5            0.940171           0.961373           0.944206   \n6            0.782051           0.768240           0.712446   \n7            0.987179           0.991416           0.991416   \n8            0.970085           0.974249           0.952790   \n9            0.811966           0.781116           0.733906   \n10           0.987179           0.991416           0.991416   \n11           0.978632           0.982833           0.982833   \n\n    split5_test_score  split6_test_score  split7_test_score  \\\n0            0.106838           0.111111           0.111111   \n1            0.948718           0.961538           0.957265   \n2            0.773504           0.803419           0.786325   \n3            0.213675           0.226496           0.196581   \n4            0.982906           1.000000           0.982906   \n5            0.935897           0.970085           0.944444   \n6            0.786325           0.760684           0.735043   \n7            0.982906           1.000000           0.995726   \n8            0.952991           0.978632           0.965812   \n9            0.807692           0.786325           0.760684   \n10           0.982906           0.995726           0.991453   \n11           0.970085           0.995726           0.974359   \n\n    split8_test_score  split9_test_score  split10_test_score  \\\n0            0.111588           0.111588            0.106838   \n1            0.944206           0.965665            0.940171   \n2            0.785408           0.798283            0.799145   \n3            0.171674           0.197425            0.179487   \n4            0.969957           0.987124            0.978632   \n5            0.939914           0.961373            0.948718   \n6            0.721030           0.798283            0.705128   \n7            0.978541           0.995708            0.987179   \n8            0.961373           0.965665            0.952991   \n9            0.729614           0.824034            0.709402   \n10           0.978541           0.995708            0.987179   \n11           0.969957           0.969957            0.974359   \n\n    split11_test_score  split12_test_score  split13_test_score  \\\n0             0.111111            0.111111            0.111588   \n1             0.961538            0.957265            0.961373   \n2             0.807692            0.790598            0.763948   \n3             0.205128            0.213675            0.206009   \n4             0.982906            0.982906            0.982833   \n5             0.957265            0.965812            0.948498   \n6             0.726496            0.752137            0.763948   \n7             0.987179            0.987179            0.991416   \n8             0.974359            0.974359            0.974249   \n9             0.756410            0.769231            0.789700   \n10            0.987179            0.995726            0.991416   \n11            0.974359            0.974359            0.978541   \n\n    split14_test_score  mean_test_score  std_test_score  rank_test_score  \n0             0.111588         0.110447        0.001817               12  \n1             0.944206         0.952340        0.008972                6  \n2             0.776824         0.788798        0.015323                8  \n3             0.197425         0.202898        0.014457               11  \n4             0.995708         0.984875        0.006970                3  \n5             0.935622         0.949485        0.010630                7  \n6             0.755365         0.746872        0.029679               10  \n7             0.995708         0.990297        0.005294                1  \n8             0.957082         0.964896        0.008733                5  \n9             0.772532         0.769695        0.032091                9  \n10            0.995708         0.990013        0.004861                2  \n11            0.974249         0.976883        0.006776                4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_gamma</th>\n      <th>param_C</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.100157</td>\n      <td>0.007800</td>\n      <td>0.031963</td>\n      <td>0.003500</td>\n      <td>0.01</td>\n      <td>0.1</td>\n      <td>{'gamma': 0.01, 'C': 0.1}</td>\n      <td>0.106838</td>\n      <td>0.111111</td>\n      <td>0.111111</td>\n      <td>0.111588</td>\n      <td>0.111588</td>\n      <td>0.106838</td>\n      <td>0.111111</td>\n      <td>0.111111</td>\n      <td>0.111588</td>\n      <td>0.111588</td>\n      <td>0.106838</td>\n      <td>0.111111</td>\n      <td>0.111111</td>\n      <td>0.111588</td>\n      <td>0.111588</td>\n      <td>0.110447</td>\n      <td>0.001817</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.066628</td>\n      <td>0.006429</td>\n      <td>0.035933</td>\n      <td>0.007941</td>\n      <td>0.001</td>\n      <td>0.1</td>\n      <td>{'gamma': 0.001, 'C': 0.1}</td>\n      <td>0.944444</td>\n      <td>0.952991</td>\n      <td>0.948718</td>\n      <td>0.961373</td>\n      <td>0.935622</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.957265</td>\n      <td>0.944206</td>\n      <td>0.965665</td>\n      <td>0.940171</td>\n      <td>0.961538</td>\n      <td>0.957265</td>\n      <td>0.961373</td>\n      <td>0.944206</td>\n      <td>0.952340</td>\n      <td>0.008972</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.086846</td>\n      <td>0.005563</td>\n      <td>0.032296</td>\n      <td>0.002017</td>\n      <td>0.0001</td>\n      <td>0.1</td>\n      <td>{'gamma': 0.0001, 'C': 0.1}</td>\n      <td>0.773504</td>\n      <td>0.816239</td>\n      <td>0.799145</td>\n      <td>0.763948</td>\n      <td>0.793991</td>\n      <td>0.773504</td>\n      <td>0.803419</td>\n      <td>0.786325</td>\n      <td>0.785408</td>\n      <td>0.798283</td>\n      <td>0.799145</td>\n      <td>0.807692</td>\n      <td>0.790598</td>\n      <td>0.763948</td>\n      <td>0.776824</td>\n      <td>0.788798</td>\n      <td>0.015323</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.102226</td>\n      <td>0.004307</td>\n      <td>0.032630</td>\n      <td>0.002527</td>\n      <td>0.01</td>\n      <td>0.5</td>\n      <td>{'gamma': 0.01, 'C': 0.5}</td>\n      <td>0.209402</td>\n      <td>0.200855</td>\n      <td>0.226496</td>\n      <td>0.193133</td>\n      <td>0.206009</td>\n      <td>0.213675</td>\n      <td>0.226496</td>\n      <td>0.196581</td>\n      <td>0.171674</td>\n      <td>0.197425</td>\n      <td>0.179487</td>\n      <td>0.205128</td>\n      <td>0.213675</td>\n      <td>0.206009</td>\n      <td>0.197425</td>\n      <td>0.202898</td>\n      <td>0.014457</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.041704</td>\n      <td>0.002024</td>\n      <td>0.019217</td>\n      <td>0.000833</td>\n      <td>0.001</td>\n      <td>0.5</td>\n      <td>{'gamma': 0.001, 'C': 0.5}</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.978632</td>\n      <td>0.987124</td>\n      <td>0.987124</td>\n      <td>0.982906</td>\n      <td>1.000000</td>\n      <td>0.982906</td>\n      <td>0.969957</td>\n      <td>0.987124</td>\n      <td>0.978632</td>\n      <td>0.982906</td>\n      <td>0.982906</td>\n      <td>0.982833</td>\n      <td>0.995708</td>\n      <td>0.984875</td>\n      <td>0.006970</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.042972</td>\n      <td>0.001770</td>\n      <td>0.025690</td>\n      <td>0.001076</td>\n      <td>0.0001</td>\n      <td>0.5</td>\n      <td>{'gamma': 0.0001, 'C': 0.5}</td>\n      <td>0.944444</td>\n      <td>0.944444</td>\n      <td>0.940171</td>\n      <td>0.961373</td>\n      <td>0.944206</td>\n      <td>0.935897</td>\n      <td>0.970085</td>\n      <td>0.944444</td>\n      <td>0.939914</td>\n      <td>0.961373</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.948498</td>\n      <td>0.935622</td>\n      <td>0.949485</td>\n      <td>0.010630</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.113369</td>\n      <td>0.003196</td>\n      <td>0.032096</td>\n      <td>0.000999</td>\n      <td>0.01</td>\n      <td>1</td>\n      <td>{'gamma': 0.01, 'C': 1}</td>\n      <td>0.696581</td>\n      <td>0.739316</td>\n      <td>0.782051</td>\n      <td>0.768240</td>\n      <td>0.712446</td>\n      <td>0.786325</td>\n      <td>0.760684</td>\n      <td>0.735043</td>\n      <td>0.721030</td>\n      <td>0.798283</td>\n      <td>0.705128</td>\n      <td>0.726496</td>\n      <td>0.752137</td>\n      <td>0.763948</td>\n      <td>0.755365</td>\n      <td>0.746872</td>\n      <td>0.029679</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.039502</td>\n      <td>0.001587</td>\n      <td>0.017883</td>\n      <td>0.000719</td>\n      <td>0.001</td>\n      <td>1</td>\n      <td>{'gamma': 0.001, 'C': 1}</td>\n      <td>0.991453</td>\n      <td>0.991453</td>\n      <td>0.987179</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.982906</td>\n      <td>1.000000</td>\n      <td>0.995726</td>\n      <td>0.978541</td>\n      <td>0.995708</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.991416</td>\n      <td>0.995708</td>\n      <td>0.990297</td>\n      <td>0.005294</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.032563</td>\n      <td>0.001629</td>\n      <td>0.021886</td>\n      <td>0.001409</td>\n      <td>0.0001</td>\n      <td>1</td>\n      <td>{'gamma': 0.0001, 'C': 1}</td>\n      <td>0.961538</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.952790</td>\n      <td>0.952991</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.961373</td>\n      <td>0.965665</td>\n      <td>0.952991</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.964896</td>\n      <td>0.008733</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.113303</td>\n      <td>0.002641</td>\n      <td>0.030961</td>\n      <td>0.002382</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>{'gamma': 0.01, 'C': 2}</td>\n      <td>0.730769</td>\n      <td>0.782051</td>\n      <td>0.811966</td>\n      <td>0.781116</td>\n      <td>0.733906</td>\n      <td>0.807692</td>\n      <td>0.786325</td>\n      <td>0.760684</td>\n      <td>0.729614</td>\n      <td>0.824034</td>\n      <td>0.709402</td>\n      <td>0.756410</td>\n      <td>0.769231</td>\n      <td>0.789700</td>\n      <td>0.772532</td>\n      <td>0.769695</td>\n      <td>0.032091</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.038902</td>\n      <td>0.001670</td>\n      <td>0.018217</td>\n      <td>0.000749</td>\n      <td>0.001</td>\n      <td>2</td>\n      <td>{'gamma': 0.001, 'C': 2}</td>\n      <td>0.987179</td>\n      <td>0.991453</td>\n      <td>0.987179</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.991453</td>\n      <td>0.978541</td>\n      <td>0.995708</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.995726</td>\n      <td>0.991416</td>\n      <td>0.995708</td>\n      <td>0.990013</td>\n      <td>0.004861</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.025954</td>\n      <td>0.001237</td>\n      <td>0.016484</td>\n      <td>0.002532</td>\n      <td>0.0001</td>\n      <td>2</td>\n      <td>{'gamma': 0.0001, 'C': 2}</td>\n      <td>0.982906</td>\n      <td>0.970085</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.970085</td>\n      <td>0.995726</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.974249</td>\n      <td>0.976883</td>\n      <td>0.006776</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "SVC_model = svm.SVC()\n",
    "\n",
    "SVC_params = dict()\n",
    "SVC_params['gamma'] = [0.01, 0.001, 0.0001]\n",
    "SVC_params['C'] = [0.1, 0.5, 1, 2]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(SVC_model, SVC_params, n_iter=50, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "SVC = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(SVC.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[4]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification par voisin le plus proche"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        0.000855      0.000338         0.018689        0.002496   \n1        0.009342      0.000870         0.020485        0.004749   \n2        0.000601      0.000491         0.013214        0.001228   \n3        0.000937      0.000449         0.004468        0.000497   \n4        0.001337      0.000809         0.011316        0.000931   \n5        0.000601      0.000490         0.006005        0.000817   \n6        0.017616      0.001144         0.041838        0.002430   \n7        0.006406      0.000612         0.015881        0.000619   \n8        0.026357      0.000944         0.179096        0.006044   \n9        0.000801      0.000400         0.019551        0.003387   \n10       0.000667      0.000472         0.019017        0.002759   \n11       0.017416      0.000801         0.037167        0.001588   \n12       0.000734      0.000443         0.013479        0.000619   \n13       0.000601      0.000490         0.016415        0.000801   \n14       0.027692      0.007352         0.100691        0.002391   \n15       0.009742      0.002622         0.016815        0.001601   \n16       0.000600      0.000490         0.018617        0.000953   \n17       0.013746      0.001063         0.021219        0.001223   \n18       0.000801      0.000400         0.005739        0.000443   \n19       0.000734      0.000443         0.005605        0.000801   \n20       0.000667      0.000472         0.005538        0.000499   \n21       0.000667      0.000472         0.017749        0.000680   \n22       0.028359      0.006854         0.068996        0.009783   \n23       0.000867      0.000340         0.005472        0.000499   \n24       0.013746      0.000681         0.029894        0.001587   \n25       0.027892      0.005812         0.169839        0.029515   \n26       0.000734      0.000443         0.018483        0.001088   \n27       0.006539      0.000619         0.019618        0.000801   \n28       0.013412      0.000491         0.022754        0.002296   \n29       0.014813      0.006052         0.028559        0.010779   \n30       0.011210      0.004122         0.024356        0.006057   \n31       0.021019      0.007236         0.035032        0.001714   \n32       0.015347      0.003757         0.038635        0.010828   \n33       0.006806      0.000833         0.022621        0.001308   \n34       0.000734      0.000443         0.011143        0.001259   \n35       0.000801      0.000400         0.011077        0.001290   \n36       0.000867      0.000619         0.022687        0.004718   \n37       0.001668      0.003072         0.005071        0.002382   \n38       0.000667      0.000472         0.021419        0.004503   \n39       0.001201      0.001377         0.013746        0.007259   \n40       0.021887      0.005218         0.032896        0.006567   \n41       0.000801      0.000400         0.011944        0.003590   \n42       0.000934      0.000250         0.007874        0.004763   \n43       0.000734      0.000443         0.012011        0.002584   \n44       0.001268      0.002146         0.013679        0.004015   \n45       0.000801      0.000542         0.011610        0.000953   \n46       0.000667      0.000472         0.017349        0.001012   \n47       0.012878      0.001409         0.026691        0.003383   \n48       0.000801      0.000400         0.014146        0.001088   \n49       0.016282      0.003533         0.029011        0.006670   \n\n   param_weights param_p param_n_neighbors param_leaf_size param_algorithm  \\\n0        uniform       1                16              30            auto   \n1       distance       1                 6              10       ball_tree   \n2       distance       1                 1              10           brute   \n3       distance       2                 1              30            auto   \n4        uniform       2                 6              30           brute   \n5       distance       2                11               1            auto   \n6        uniform       2                11              10         kd_tree   \n7       distance       1                 1              30       ball_tree   \n8        uniform       2                16               1         kd_tree   \n9        uniform       1                 6              30           brute   \n10       uniform       1                16              10           brute   \n11      distance       2                16              10         kd_tree   \n12      distance       1                16              10            auto   \n13       uniform       1                 1              10            auto   \n14       uniform       1                16               1         kd_tree   \n15      distance       2                 1              10       ball_tree   \n16       uniform       1                16              10            auto   \n17      distance       2                 1               1       ball_tree   \n18      distance       2                16              10            auto   \n19      distance       2                11               1           brute   \n20      distance       2                16              30            auto   \n21       uniform       1                 6               1            auto   \n22      distance       1                 1               1         kd_tree   \n23      distance       2                11              30           brute   \n24       uniform       1                11               1       ball_tree   \n25       uniform       2                11               1         kd_tree   \n26       uniform       1                11               1           brute   \n27       uniform       2                 1              30       ball_tree   \n28      distance       1                 6               1       ball_tree   \n29       uniform       2                 1              30         kd_tree   \n30       uniform       2                 1              10       ball_tree   \n31      distance       2                 6              10         kd_tree   \n32       uniform       2                11               1       ball_tree   \n33       uniform       1                 6              30       ball_tree   \n34       uniform       2                11              30           brute   \n35       uniform       2                16               1            auto   \n36       uniform       1                11              30            auto   \n37      distance       2                 1              10            auto   \n38       uniform       1                 6              10           brute   \n39       uniform       2                16              30           brute   \n40      distance       1                 1              10         kd_tree   \n41       uniform       2                16              10           brute   \n42      distance       2                 6              10           brute   \n43       uniform       2                 6               1           brute   \n44      distance       1                 1              30            auto   \n45       uniform       2                11              10           brute   \n46       uniform       1                 1              10           brute   \n47      distance       2                11               1       ball_tree   \n48      distance       1                 6              10           brute   \n49       uniform       1                 1              10         kd_tree   \n\n                                               params  split0_test_score  \\\n0   {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.948718   \n1   {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.987179   \n2   {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n3   {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n4   {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.982906   \n5   {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n6   {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n7   {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n8   {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.944444   \n9   {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n10  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.948718   \n11  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.957265   \n12  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.961538   \n13  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n14  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.944444   \n15  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n16  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.948718   \n17  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n18  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.957265   \n19  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n20  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.957265   \n21  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n22  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n23  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n24  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.965812   \n25  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n26  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.965812   \n27  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.987179   \n28  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.987179   \n29  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.987179   \n30  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.987179   \n31  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.982906   \n32  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n33  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.982906   \n34  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n35  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.940171   \n36  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.965812   \n37  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n38  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n39  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.940171   \n40  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n41  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.940171   \n42  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.982906   \n43  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.982906   \n44  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n45  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n46  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n47  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n48  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.982906   \n49  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.961538           0.970085           0.969957   \n1            0.970085           0.974359           0.982833   \n2            0.978632           0.970085           0.987124   \n3            0.987179           0.978632           0.978541   \n4            0.991453           0.970085           0.965665   \n5            0.982906           0.965812           0.978541   \n6            0.978632           0.965812           0.974249   \n7            0.978632           0.970085           0.987124   \n8            0.970085           0.965812           0.969957   \n9            0.974359           0.965812           0.969957   \n10           0.961538           0.970085           0.969957   \n11           0.970085           0.970085           0.974249   \n12           0.961538           0.970085           0.974249   \n13           0.978632           0.970085           0.987124   \n14           0.961538           0.970085           0.974249   \n15           0.987179           0.978632           0.978541   \n16           0.961538           0.970085           0.969957   \n17           0.987179           0.978632           0.978541   \n18           0.970085           0.970085           0.974249   \n19           0.982906           0.965812           0.978541   \n20           0.970085           0.970085           0.974249   \n21           0.974359           0.965812           0.969957   \n22           0.978632           0.970085           0.987124   \n23           0.982906           0.965812           0.978541   \n24           0.974359           0.957265           0.965665   \n25           0.978632           0.965812           0.974249   \n26           0.970085           0.957265           0.965665   \n27           0.987179           0.978632           0.978541   \n28           0.970085           0.974359           0.982833   \n29           0.987179           0.978632           0.978541   \n30           0.987179           0.978632           0.978541   \n31           0.991453           0.970085           0.974249   \n32           0.978632           0.965812           0.974249   \n33           0.974359           0.965812           0.969957   \n34           0.978632           0.965812           0.974249   \n35           0.970085           0.965812           0.969957   \n36           0.970085           0.957265           0.965665   \n37           0.987179           0.978632           0.978541   \n38           0.974359           0.965812           0.969957   \n39           0.970085           0.965812           0.969957   \n40           0.978632           0.970085           0.987124   \n41           0.970085           0.965812           0.969957   \n42           0.991453           0.970085           0.974249   \n43           0.991453           0.970085           0.965665   \n44           0.978632           0.970085           0.987124   \n45           0.978632           0.965812           0.974249   \n46           0.978632           0.970085           0.987124   \n47           0.982906           0.965812           0.978541   \n48           0.970085           0.974359           0.982833   \n49           0.978632           0.970085           0.987124   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.957082           0.961538           0.970085   \n1            0.982833           0.982906           0.995726   \n2            0.982833           0.974359           0.982906   \n3            0.991416           0.978632           1.000000   \n4            0.995708           0.978632           0.995726   \n5            0.969957           0.974359           0.982906   \n6            0.965665           0.965812           0.978632   \n7            0.982833           0.974359           0.982906   \n8            0.957082           0.970085           0.974359   \n9            0.978541           0.965812           0.982906   \n10           0.957082           0.961538           0.970085   \n11           0.957082           0.978632           0.978632   \n12           0.965665           0.974359           0.978632   \n13           0.982833           0.974359           0.982906   \n14           0.957082           0.961538           0.970085   \n15           0.991416           0.978632           1.000000   \n16           0.957082           0.961538           0.970085   \n17           0.991416           0.978632           1.000000   \n18           0.957082           0.978632           0.978632   \n19           0.969957           0.974359           0.982906   \n20           0.957082           0.978632           0.978632   \n21           0.978541           0.965812           0.982906   \n22           0.982833           0.974359           0.982906   \n23           0.969957           0.974359           0.982906   \n24           0.957082           0.957265           0.974359   \n25           0.965665           0.965812           0.978632   \n26           0.957082           0.961538           0.974359   \n27           0.991416           0.978632           1.000000   \n28           0.982833           0.982906           0.991453   \n29           0.991416           0.978632           1.000000   \n30           0.991416           0.978632           1.000000   \n31           0.995708           0.982906           1.000000   \n32           0.965665           0.965812           0.978632   \n33           0.982833           0.965812           0.982906   \n34           0.965665           0.965812           0.978632   \n35           0.957082           0.970085           0.974359   \n36           0.957082           0.961538           0.974359   \n37           0.991416           0.978632           1.000000   \n38           0.978541           0.965812           0.982906   \n39           0.957082           0.970085           0.974359   \n40           0.982833           0.974359           0.982906   \n41           0.957082           0.970085           0.974359   \n42           0.995708           0.982906           1.000000   \n43           0.995708           0.978632           0.995726   \n44           0.982833           0.974359           0.982906   \n45           0.965665           0.965812           0.978632   \n46           0.982833           0.974359           0.982906   \n47           0.969957           0.974359           0.982906   \n48           0.982833           0.982906           0.991453   \n49           0.982833           0.974359           0.982906   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.944444           0.952790           0.969957   \n1            0.991453           0.957082           0.991416   \n2            0.995726           0.965665           0.982833   \n3            1.000000           0.978541           0.987124   \n4            0.974359           0.969957           0.978541   \n5            0.974359           0.969957           0.982833   \n6            0.974359           0.961373           0.974249   \n7            0.991453           0.965665           0.982833   \n8            0.952991           0.957082           0.965665   \n9            0.974359           0.948498           0.982833   \n10           0.944444           0.952790           0.969957   \n11           0.974359           0.961373           0.969957   \n12           0.961538           0.961373           0.978541   \n13           0.995726           0.965665           0.982833   \n14           0.948718           0.952790           0.969957   \n15           1.000000           0.978541           0.987124   \n16           0.944444           0.952790           0.969957   \n17           1.000000           0.978541           0.987124   \n18           0.974359           0.961373           0.969957   \n19           0.974359           0.969957           0.982833   \n20           0.974359           0.961373           0.969957   \n21           0.974359           0.948498           0.982833   \n22           0.991453           0.965665           0.982833   \n23           0.974359           0.969957           0.982833   \n24           0.961538           0.957082           0.978541   \n25           0.974359           0.961373           0.974249   \n26           0.961538           0.961373           0.978541   \n27           1.000000           0.978541           0.987124   \n28           0.991453           0.957082           0.991416   \n29           1.000000           0.978541           0.987124   \n30           1.000000           0.978541           0.987124   \n31           0.995726           0.969957           0.978541   \n32           0.974359           0.965665           0.974249   \n33           0.974359           0.948498           0.982833   \n34           0.974359           0.965665           0.974249   \n35           0.952991           0.957082           0.965665   \n36           0.961538           0.961373           0.978541   \n37           1.000000           0.978541           0.987124   \n38           0.974359           0.948498           0.982833   \n39           0.952991           0.957082           0.965665   \n40           0.991453           0.965665           0.982833   \n41           0.952991           0.957082           0.965665   \n42           0.995726           0.969957           0.978541   \n43           0.974359           0.969957           0.978541   \n44           0.995726           0.965665           0.982833   \n45           0.974359           0.965665           0.974249   \n46           0.995726           0.965665           0.982833   \n47           0.974359           0.969957           0.982833   \n48           0.991453           0.957082           0.991416   \n49           0.991453           0.965665           0.982833   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.948718            0.965812            0.961538   \n1             0.965812            0.974359            0.978632   \n2             0.974359            0.974359            0.974359   \n3             0.987179            0.987179            1.000000   \n4             0.978632            0.978632            0.978632   \n5             0.982906            0.974359            0.987179   \n6             0.965812            0.974359            0.978632   \n7             0.974359            0.974359            0.974359   \n8             0.965812            0.970085            0.965812   \n9             0.965812            0.961538            0.974359   \n10            0.948718            0.965812            0.961538   \n11            0.974359            0.974359            0.974359   \n12            0.961538            0.965812            0.974359   \n13            0.974359            0.974359            0.974359   \n14            0.948718            0.965812            0.961538   \n15            0.987179            0.987179            1.000000   \n16            0.948718            0.965812            0.961538   \n17            0.987179            0.987179            1.000000   \n18            0.974359            0.974359            0.974359   \n19            0.982906            0.974359            0.987179   \n20            0.974359            0.974359            0.974359   \n21            0.965812            0.961538            0.974359   \n22            0.974359            0.974359            0.974359   \n23            0.982906            0.974359            0.987179   \n24            0.961538            0.961538            0.965812   \n25            0.965812            0.974359            0.978632   \n26            0.957265            0.961538            0.965812   \n27            0.987179            0.987179            1.000000   \n28            0.965812            0.974359            0.978632   \n29            0.987179            0.987179            1.000000   \n30            0.987179            0.987179            1.000000   \n31            0.991453            0.982906            0.991453   \n32            0.965812            0.974359            0.978632   \n33            0.965812            0.961538            0.974359   \n34            0.965812            0.974359            0.982906   \n35            0.965812            0.970085            0.965812   \n36            0.957265            0.961538            0.965812   \n37            0.987179            0.987179            1.000000   \n38            0.965812            0.961538            0.974359   \n39            0.965812            0.970085            0.965812   \n40            0.974359            0.974359            0.974359   \n41            0.965812            0.970085            0.965812   \n42            0.991453            0.982906            0.991453   \n43            0.978632            0.978632            0.978632   \n44            0.974359            0.974359            0.974359   \n45            0.965812            0.974359            0.982906   \n46            0.974359            0.974359            0.974359   \n47            0.982906            0.974359            0.982906   \n48            0.965812            0.974359            0.978632   \n49            0.974359            0.974359            0.974359   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.969957            0.957082         0.960620        0.008569   \n1             0.982833            0.987124         0.980309        0.010108   \n2             0.978541            0.987124         0.979169        0.007303   \n3             0.991416            0.991416         0.988296        0.007410   \n4             0.974249            0.978541         0.979448        0.008604   \n5             0.978541            0.978541         0.976883        0.006013   \n6             0.974249            0.969957         0.970889        0.005895   \n7             0.978541            0.987124         0.978884        0.006710   \n8             0.974249            0.961373         0.964326        0.008067   \n9             0.982833            0.974249         0.972033        0.009103   \n10            0.969957            0.957082         0.960620        0.008569   \n11            0.978541            0.969957         0.970886        0.006869   \n12            0.974249            0.961373         0.968323        0.006588   \n13            0.978541            0.987124         0.979169        0.007303   \n14            0.969957            0.957082         0.960906        0.008940   \n15            0.991416            0.991416         0.988296        0.007410   \n16            0.969957            0.957082         0.960620        0.008569   \n17            0.991416            0.991416         0.988296        0.007410   \n18            0.978541            0.969957         0.970886        0.006869   \n19            0.978541            0.978541         0.976883        0.006013   \n20            0.978541            0.969957         0.970886        0.006869   \n21            0.982833            0.974249         0.972033        0.009103   \n22            0.978541            0.987124         0.978884        0.006710   \n23            0.978541            0.978541         0.976883        0.006013   \n24            0.974249            0.965665         0.965185        0.006961   \n25            0.974249            0.969957         0.970889        0.005895   \n26            0.974249            0.969957         0.965472        0.006516   \n27            0.991416            0.991416         0.988296        0.007410   \n28            0.982833            0.987124         0.980024        0.009722   \n29            0.991416            0.991416         0.988296        0.007410   \n30            0.991416            0.991416         0.988296        0.007410   \n31            0.982833            0.991416         0.985440        0.009099   \n32            0.974249            0.969957         0.971175        0.005519   \n33            0.974249            0.974249         0.972032        0.009238   \n34            0.974249            0.969957         0.971460        0.005987   \n35            0.974249            0.961373         0.964041        0.008806   \n36            0.974249            0.969957         0.965472        0.006516   \n37            0.991416            0.991416         0.988296        0.007410   \n38            0.982833            0.974249         0.972033        0.009103   \n39            0.974249            0.961373         0.964041        0.008806   \n40            0.978541            0.987124         0.978884        0.006710   \n41            0.974249            0.961373         0.964041        0.008806   \n42            0.982833            0.991416         0.985440        0.009099   \n43            0.974249            0.978541         0.979448        0.008604   \n44            0.978541            0.987124         0.979169        0.007303   \n45            0.974249            0.969957         0.971460        0.005987   \n46            0.978541            0.987124         0.979169        0.007303   \n47            0.978541            0.978541         0.976598        0.005606   \n48            0.982833            0.987124         0.979739        0.009570   \n49            0.978541            0.987124         0.978884        0.006710   \n\n    rank_test_score  \n0                48  \n1                10  \n2                15  \n3                 1  \n4                13  \n5                23  \n6                34  \n7                19  \n8                43  \n9                27  \n10               48  \n11               36  \n12               39  \n13               15  \n14               47  \n15                1  \n16               48  \n17                1  \n18               36  \n19               23  \n20               36  \n21               27  \n22               19  \n23               23  \n24               42  \n25               34  \n26               40  \n27                1  \n28               11  \n29                1  \n30                1  \n31                8  \n32               33  \n33               30  \n34               31  \n35               44  \n36               40  \n37                1  \n38               27  \n39               44  \n40               19  \n41               44  \n42                8  \n43               13  \n44               15  \n45               31  \n46               15  \n47               26  \n48               12  \n49               19  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_weights</th>\n      <th>param_p</th>\n      <th>param_n_neighbors</th>\n      <th>param_leaf_size</th>\n      <th>param_algorithm</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000855</td>\n      <td>0.000338</td>\n      <td>0.018689</td>\n      <td>0.002496</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.944444</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960620</td>\n      <td>0.008569</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.009342</td>\n      <td>0.000870</td>\n      <td>0.020485</td>\n      <td>0.004749</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>6</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.991453</td>\n      <td>0.957082</td>\n      <td>0.991416</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.987124</td>\n      <td>0.980309</td>\n      <td>0.010108</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000601</td>\n      <td>0.000491</td>\n      <td>0.013214</td>\n      <td>0.001228</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.979169</td>\n      <td>0.007303</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000937</td>\n      <td>0.000449</td>\n      <td>0.004468</td>\n      <td>0.000497</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.001337</td>\n      <td>0.000809</td>\n      <td>0.011316</td>\n      <td>0.000931</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>6</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.965665</td>\n      <td>0.995708</td>\n      <td>0.978632</td>\n      <td>0.995726</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.978541</td>\n      <td>0.979448</td>\n      <td>0.008604</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000601</td>\n      <td>0.000490</td>\n      <td>0.006005</td>\n      <td>0.000817</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976883</td>\n      <td>0.006013</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.017616</td>\n      <td>0.001144</td>\n      <td>0.041838</td>\n      <td>0.002430</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.970889</td>\n      <td>0.005895</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.006406</td>\n      <td>0.000612</td>\n      <td>0.015881</td>\n      <td>0.000619</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.026357</td>\n      <td>0.000944</td>\n      <td>0.179096</td>\n      <td>0.006044</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>16</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.944444</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964326</td>\n      <td>0.008067</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.000801</td>\n      <td>0.000400</td>\n      <td>0.019551</td>\n      <td>0.003387</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.974249</td>\n      <td>0.972033</td>\n      <td>0.009103</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.000667</td>\n      <td>0.000472</td>\n      <td>0.019017</td>\n      <td>0.002759</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.944444</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960620</td>\n      <td>0.008569</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.017416</td>\n      <td>0.000801</td>\n      <td>0.037167</td>\n      <td>0.001588</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>16</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.970886</td>\n      <td>0.006869</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.000734</td>\n      <td>0.000443</td>\n      <td>0.013479</td>\n      <td>0.000619</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>16</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.978541</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.968323</td>\n      <td>0.006588</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.000601</td>\n      <td>0.000490</td>\n      <td>0.016415</td>\n      <td>0.000801</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.979169</td>\n      <td>0.007303</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.027692</td>\n      <td>0.007352</td>\n      <td>0.100691</td>\n      <td>0.002391</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.944444</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.948718</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960906</td>\n      <td>0.008940</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.009742</td>\n      <td>0.002622</td>\n      <td>0.016815</td>\n      <td>0.001601</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.000600</td>\n      <td>0.000490</td>\n      <td>0.018617</td>\n      <td>0.000953</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.944444</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960620</td>\n      <td>0.008569</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.013746</td>\n      <td>0.001063</td>\n      <td>0.021219</td>\n      <td>0.001223</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.000801</td>\n      <td>0.000400</td>\n      <td>0.005739</td>\n      <td>0.000443</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>16</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.970886</td>\n      <td>0.006869</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.000734</td>\n      <td>0.000443</td>\n      <td>0.005605</td>\n      <td>0.000801</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976883</td>\n      <td>0.006013</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.000667</td>\n      <td>0.000472</td>\n      <td>0.005538</td>\n      <td>0.000499</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>16</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.970886</td>\n      <td>0.006869</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.000667</td>\n      <td>0.000472</td>\n      <td>0.017749</td>\n      <td>0.000680</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.974249</td>\n      <td>0.972033</td>\n      <td>0.009103</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.028359</td>\n      <td>0.006854</td>\n      <td>0.068996</td>\n      <td>0.009783</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.000867</td>\n      <td>0.000340</td>\n      <td>0.005472</td>\n      <td>0.000499</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976883</td>\n      <td>0.006013</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.013746</td>\n      <td>0.000681</td>\n      <td>0.029894</td>\n      <td>0.001587</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>11</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.957265</td>\n      <td>0.974359</td>\n      <td>0.961538</td>\n      <td>0.957082</td>\n      <td>0.978541</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965185</td>\n      <td>0.006961</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.027892</td>\n      <td>0.005812</td>\n      <td>0.169839</td>\n      <td>0.029515</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.970889</td>\n      <td>0.005895</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.000734</td>\n      <td>0.000443</td>\n      <td>0.018483</td>\n      <td>0.001088</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>11</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.978541</td>\n      <td>0.957265</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.965472</td>\n      <td>0.006516</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.006539</td>\n      <td>0.000619</td>\n      <td>0.019618</td>\n      <td>0.000801</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>1</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.013412</td>\n      <td>0.000491</td>\n      <td>0.022754</td>\n      <td>0.002296</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.991453</td>\n      <td>0.957082</td>\n      <td>0.991416</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.987124</td>\n      <td>0.980024</td>\n      <td>0.009722</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.014813</td>\n      <td>0.006052</td>\n      <td>0.028559</td>\n      <td>0.010779</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>1</td>\n      <td>30</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.011210</td>\n      <td>0.004122</td>\n      <td>0.024356</td>\n      <td>0.006057</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>1</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.021019</td>\n      <td>0.007236</td>\n      <td>0.035032</td>\n      <td>0.001714</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>6</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.995708</td>\n      <td>0.982906</td>\n      <td>1.000000</td>\n      <td>0.995726</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.991453</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.982833</td>\n      <td>0.991416</td>\n      <td>0.985440</td>\n      <td>0.009099</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.015347</td>\n      <td>0.003757</td>\n      <td>0.038635</td>\n      <td>0.010828</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.971175</td>\n      <td>0.005519</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.006806</td>\n      <td>0.000833</td>\n      <td>0.022621</td>\n      <td>0.001308</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.974249</td>\n      <td>0.972032</td>\n      <td>0.009238</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.000734</td>\n      <td>0.000443</td>\n      <td>0.011143</td>\n      <td>0.001259</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.971460</td>\n      <td>0.005987</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.000801</td>\n      <td>0.000400</td>\n      <td>0.011077</td>\n      <td>0.001290</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>16</td>\n      <td>1</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.940171</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964041</td>\n      <td>0.008806</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.000867</td>\n      <td>0.000619</td>\n      <td>0.022687</td>\n      <td>0.004718</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>11</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.978541</td>\n      <td>0.957265</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.965472</td>\n      <td>0.006516</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.001668</td>\n      <td>0.003072</td>\n      <td>0.005071</td>\n      <td>0.002382</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>10</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.000667</td>\n      <td>0.000472</td>\n      <td>0.021419</td>\n      <td>0.004503</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.974249</td>\n      <td>0.972033</td>\n      <td>0.009103</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.001201</td>\n      <td>0.001377</td>\n      <td>0.013746</td>\n      <td>0.007259</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>16</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.940171</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964041</td>\n      <td>0.008806</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.021887</td>\n      <td>0.005218</td>\n      <td>0.032896</td>\n      <td>0.006567</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>0.000801</td>\n      <td>0.000400</td>\n      <td>0.011944</td>\n      <td>0.003590</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>16</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.940171</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964041</td>\n      <td>0.008806</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>0.000934</td>\n      <td>0.000250</td>\n      <td>0.007874</td>\n      <td>0.004763</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>6</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.995708</td>\n      <td>0.982906</td>\n      <td>1.000000</td>\n      <td>0.995726</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.991453</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.982833</td>\n      <td>0.991416</td>\n      <td>0.985440</td>\n      <td>0.009099</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>0.000734</td>\n      <td>0.000443</td>\n      <td>0.012011</td>\n      <td>0.002584</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>6</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.965665</td>\n      <td>0.995708</td>\n      <td>0.978632</td>\n      <td>0.995726</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.978541</td>\n      <td>0.979448</td>\n      <td>0.008604</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>0.001268</td>\n      <td>0.002146</td>\n      <td>0.013679</td>\n      <td>0.004015</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>30</td>\n      <td>auto</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.979169</td>\n      <td>0.007303</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>0.000801</td>\n      <td>0.000542</td>\n      <td>0.011610</td>\n      <td>0.000953</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.971460</td>\n      <td>0.005987</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>0.000667</td>\n      <td>0.000472</td>\n      <td>0.017349</td>\n      <td>0.001012</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.979169</td>\n      <td>0.007303</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>0.012878</td>\n      <td>0.001409</td>\n      <td>0.026691</td>\n      <td>0.003383</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976598</td>\n      <td>0.005606</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>0.000801</td>\n      <td>0.000400</td>\n      <td>0.014146</td>\n      <td>0.001088</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>6</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.982906</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.991453</td>\n      <td>0.957082</td>\n      <td>0.991416</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.987124</td>\n      <td>0.979739</td>\n      <td>0.009570</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>0.016282</td>\n      <td>0.003533</td>\n      <td>0.029011</td>\n      <td>0.006670</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_model = KNeighborsClassifier()\n",
    "\n",
    "KNN_params = dict()\n",
    "KNN_params['n_neighbors'] = [i for i in range(1, 20, 5)]\n",
    "KNN_params['weights'] = [\"uniform\", \"distance\"]\n",
    "KNN_params['algorithm'] = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "KNN_params['leaf_size'] = [1, 10, 30]\n",
    "KNN_params['p'] = [1, 2]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(KNN_model, KNN_params, n_iter=50, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "KNN = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(KNN.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[5]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparaison des résultats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                               Best Mean Test Score\nLogistic Regression (LR)                   0.972313\nNaïve Bayes Classifier (NB)                0.864722\nDecision Tree Classifier (DT)              0.842746\nGradient Boosting (GB)                     0.966608\nSupport Vector Machines (SVM)              0.990297\nK Nearest Neighbours (KNN)                 0.988296",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Best Mean Test Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression (LR)</th>\n      <td>0.972313</td>\n    </tr>\n    <tr>\n      <th>Naïve Bayes Classifier (NB)</th>\n      <td>0.864722</td>\n    </tr>\n    <tr>\n      <th>Decision Tree Classifier (DT)</th>\n      <td>0.842746</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting (GB)</th>\n      <td>0.966608</td>\n    </tr>\n    <tr>\n      <th>Support Vector Machines (SVM)</th>\n      <td>0.990297</td>\n    </tr>\n    <tr>\n      <th>K Nearest Neighbours (KNN)</th>\n      <td>0.988296</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(Evaluation_Results)\n",
    "df.plot(kind = 'bar')\n",
    "plt.show()\n",
    "\n",
    "Evaluation_Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Au vu des résultats, le modèle ayant la meilleure exactitude semble être la classification machine à vecteurs de support avec pour paramètre 'gamma' = 0.001 et 'C'= 1.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Création du prédicteur"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "SVC_definitive_model = svm.SVC(gamma=0.001, C=1)\n",
    "\n",
    "SVC_fitted = SVC_definitive_model.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Matrice de confusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "predicted = SVC_fitted.predict(x_test)\n",
    "disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predicted, cmap=plt.cm.gray_r)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La matrice de confusion nous permet de constater que les rares erreurs sont souvent sur les labels 2, 5."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test avec des données externes\n",
    "\n",
    "Essayons à présent de tester notre prédicteur sur des images que nous avons créées grâce au logiciel paint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x216 with 9 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image, ImageChops\n",
    "\n",
    "# Import de nos images\n",
    "image_test_1 = np.array(ImageChops.invert(Image.open('test_images/1.bmp'))) - 240\n",
    "image_test_2 = np.array(ImageChops.invert(Image.open('test_images/2.bmp'))) - 240\n",
    "image_test_3 = np.array(ImageChops.invert(Image.open('test_images/3.bmp'))) - 240\n",
    "image_test_4 = np.array(ImageChops.invert(Image.open('test_images/4.bmp'))) - 240\n",
    "image_test_5 = np.array(ImageChops.invert(Image.open('test_images/5.bmp'))) - 240\n",
    "image_test_6 = np.array(ImageChops.invert(Image.open('test_images/6.bmp'))) - 240\n",
    "image_test_7 = np.array(ImageChops.invert(Image.open('test_images/7.bmp'))) - 240\n",
    "image_test_8 = np.array(ImageChops.invert(Image.open('test_images/8.bmp'))) - 240\n",
    "image_test_9 = np.array(ImageChops.invert(Image.open('test_images/9.bmp'))) - 240\n",
    "\n",
    "\n",
    "images_test = np.array([image_test_1, image_test_2, image_test_3, image_test_4, image_test_5, image_test_6, image_test_7, image_test_8, image_test_9])\n",
    "\n",
    "# Création de 10 figures de 10 par 3 pixels\n",
    "_, axes = plt.subplots(nrows=1, ncols=9, figsize=(10, 3))\n",
    "\n",
    "# prédiction du prédicteur\n",
    "y_pred = SVC_fitted.predict(images_test.reshape(len(images_test), -1))\n",
    "\n",
    "# Pour chaque figure, on affiche l'image du chiffre et son label en titre\n",
    "for ax, image, label in zip(axes, images_test, y_pred):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "    ax.set_title(\"Pred: %i\" % label)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Citation :\n",
    "Stephanie Glen. \"Regularization: Simple Definition, L1 & L2 Penalties\" From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/regularization/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}