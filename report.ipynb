{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0b8e1d",
   "metadata": {},
   "source": [
    "# Rapport du projet de résolution de problème\n",
    "\n",
    "- Paul Achard\n",
    "- Julien Faure\n",
    "\n",
    "    \n",
    "- *Date : 20/01/2022*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1070770",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42622bf2",
   "metadata": {},
   "source": [
    "## Problématique\n",
    "\n",
    "Notre problématique est d'identifier un chiffre à partir d'une image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3a0c6",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "- Trouver un modèle permettant d'identifier un chiffre à partir d'une image\n",
    "- Comparer différentes stratégies de solveur pour le modèle trouvé\n",
    "\n",
    "## Analyse du dataset\n",
    "\n",
    "Identifier un chiffre à partir d'une image est une tache qui peut s'avérer très complexe. Afin d'avoir une difficulté raisonnable et adapté au contexte de ce projet, nous avons fixé certains paramètres dans notre dataset.\n",
    "\n",
    "- La résolution de nos images est identiques pour tout le dataset. Cette résolution est **8 pixels par 8 pixels**.\n",
    "- Chaque pixel est codé sur **4 bits**.\n",
    "- Les images contiennent uniquement un chiffre sans élément parasite, sans effet et sans traitement.\n",
    "\n",
    "Nous utilisons le dataset `digits` de `sklearn`.\n",
    "\n",
    "### Forme du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "894968c2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data Shape (1797, 64)\n",
      "Label Data Shape (1797,)\n"
     ]
    }
   ],
   "source": [
    "# Import du dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Affiche le nombre d'images et leur format\n",
    "print(\"Image Data Shape\" , digits.data.shape)\n",
    "\n",
    "# Affiche le nombre de labels\n",
    "print(\"Label Data Shape\", digits.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311b24e7",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir ci-dessus, le dataset est composé de **1797** images labellisées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb1ba9",
   "metadata": {},
   "source": [
    "### Répartition des images du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1826bdc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Text(0.5, 0, 'n° labellisé'), Text(0, 0.5, 'Occurrence')]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphique = sns.countplot(x=digits.target)\n",
    "graphique.set(xlabel=\"n° labellisé\", ylabel = \"Occurrence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee617b1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Les données labellisées sont équitablement distribuées (environ 175 par label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc387e8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Représentation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17d087b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x216 with 10 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Création de 10 figures de 10 par 3 pixels\n",
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 3))\n",
    "\n",
    "# Pour chaque figure, on affiche l'image du chiffre et son label en titre\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "    ax.set_title(\"Label: %i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e6cc1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Représentation des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb283e09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "\n",
      " Type de chaque valeur : <class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "# Affiche le tableau représentant la première image\n",
    "print(digits.images[0])\n",
    "print(\"\\n Type de chaque valeur :\", type(digits.images[0][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa72f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Comme nous l'avons vu précédemment, chaque image possède une résolution de 8x8 pixels en niveaux de gris codés sur 4bits par pixel.\n",
    "\n",
    "Dans notre programme, une image est représentée par une matrice de dimension 8x8. Chaque élément représente un pixel avec un niveau de gris codé sur 4bits (de 0 à 15). Plus la valeur est élevée, plus la couleur est foncée.\n",
    "\n",
    "> Exemple :\n",
    "* 0 : Blanc\n",
    "* 15 : Noir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3675a",
   "metadata": {},
   "source": [
    "\n",
    "## Modèles\n",
    "\n",
    "Pour résoudre notre problème, nous allons essayer les modèles suivants :\n",
    "* [Modèle de regréssion logistique multiclasse](#Mod%C3%A8le-de-regr%C3%A9ssion-logistique-multiclasse)\n",
    "* [Modèle de Bayes](#Mod%C3%A8le-de-Bayes)\n",
    "\n",
    "### Mise en forme des données d'entrées\n",
    "\n",
    "Pour commencer, nous devons redimenssionner nos données d'entrées.\n",
    "\n",
    "Actuellement, nous avons des données sous la forme d'un tableau de matrice.\n",
    "\n",
    "Il nous faut les mettre sous forme d'une matrice où chaque vecteur correspond aux pixels de l'image.\n",
    "\n",
    "Donc si nous avons 1797 images, la matrice d'entrée est composée de 1797 vecteurs.\n",
    "\n",
    "Avec une résolution de 8x8, la taille d'un vecteur est de 64 valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74b2574d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Redimensionne la matrice en vecteur\n",
    "print(digits.images[0].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e10f7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
    "\n",
    "# Redimenssionne le tableau de matrice en matrice\n",
    "x = digits.images.reshape((len(digits.images), -1))\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9331fa9",
   "metadata": {},
   "source": [
    "### Jeu de test et jeu d'entraînement\n",
    "\n",
    "Il nous faut maintenant séparer notre jeu de test et notre jeu d'entraînement du dataset.\n",
    "\n",
    "Pour choisir la taille de notre jeu de test, il est nécessaire de faire attention à plusieurs points :\n",
    "* Le temps d'entrainement de notre modèle\n",
    "* La taille de notre dataset\n",
    "\n",
    "Plus la proportion du jeu d'entraînement est faible, plus notre modèle à un niveau de variance élevé. Ainsi, on augmente les chances d'avoir de l'*over fitting*.\n",
    "\n",
    "A contrario, plus la proportion du jeu de test est faible, plus notre modèle à un niveau de variance faible. Ainsi, on augmente les chances d'avoir de l'*under fitting*.\n",
    "\n",
    "Le but est donc de trouver la valeur qui nous permet d'avoir le taux de variance optimal.\n",
    "\n",
    "\n",
    "Pour trouver cette valeur, nous avons appliqué la procédure suivante, nous avons essayé avec plusieurs valeurs en partant de 20% jusqu'à 80% avec un pas de 5%. Nous avons déterminé que la meilleure valeur est **35%**.\n",
    "\n",
    "\n",
    "> [https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio](https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de94e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# séparation du dataset en données \"d'apprentissage\" et de test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.35, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Création du tableau de résultat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "                               Best Mean Test Score\nLogistic Regression (LR)                        0.0\nNaïve Bayes Classifier (NB)                     0.0\nDecision Tree Classifier (DT)                   0.0\nGradient Boosting (GB)                          0.0\nK Nearest Neighbours (KNN)                      0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Best Mean Test Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression (LR)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Naïve Bayes Classifier (NB)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Decision Tree Classifier (DT)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting (GB)</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>K Nearest Neighbours (KNN)</th>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluation_Results = pd.DataFrame(np.zeros((5, 1)), columns=['Best Mean Test Score'])\n",
    "Evaluation_Results.index = ['Logistic Regression (LR)', 'Naïve Bayes Classifier (NB)',\n",
    "                            'Decision Tree Classifier (DT)', 'Gradient Boosting (GB)',\n",
    "                            'K Nearest Neighbours (KNN)']\n",
    "Evaluation_Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "334ad595",
   "metadata": {},
   "source": [
    "### Modèle de regréssion logistique multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc863948",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 18 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.97231332 0.97231332 0.96489613 0.96659819 0.95546507        nan\n",
      " 0.96917575 0.96946187 0.95748138 0.96659819 0.95546507        nan\n",
      " 0.96917697 0.96860595 0.95633934 0.96659819 0.95546507        nan]\n",
      "  warnings.warn(\n",
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [1.         1.         0.99600438 1.         1.                nan\n",
      " 1.         1.         0.99978594 1.         1.                nan\n",
      " 1.         1.         1.         1.         1.                nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n0        0.760930      0.186828         0.000673        0.002030     0.1   \n1        2.063472      0.719442         0.000000        0.000000     0.1   \n2        0.051133      0.005570         0.000548        0.000909     0.1   \n3        0.150434      0.061752         0.000309        0.000914     0.1   \n4        0.106588      0.162071         0.000140        0.000525     0.1   \n5        0.000884      0.002117         0.000000        0.000000     0.1   \n6        0.824467      0.156376         0.000000        0.000000       5   \n7        2.811047      1.313212         0.001211        0.002731       5   \n8        0.120490      0.024473         0.000677        0.002533       5   \n9        0.161477      0.010363         0.001221        0.003124       5   \n10       0.055977      0.004590         0.000388        0.001038       5   \n11       0.000073      0.000272         0.000000        0.000000       5   \n12       0.832793      0.078511         0.000139        0.000519      10   \n13       3.154757      0.987532         0.000946        0.002200      10   \n14       0.111418      0.007896         0.001755        0.003204      10   \n15       0.157436      0.010092         0.000405        0.001091      10   \n16       0.056531      0.002473         0.000800        0.001600      10   \n17       0.000800      0.001600         0.000000        0.000000      10   \n\n   param_penalty param_solver  \\\n0             l2    newton-cg   \n1             l2        lbfgs   \n2             l2    liblinear   \n3           none    newton-cg   \n4           none        lbfgs   \n5           none    liblinear   \n6             l2    newton-cg   \n7             l2        lbfgs   \n8             l2    liblinear   \n9           none    newton-cg   \n10          none        lbfgs   \n11          none    liblinear   \n12            l2    newton-cg   \n13            l2        lbfgs   \n14            l2    liblinear   \n15          none    newton-cg   \n16          none        lbfgs   \n17          none    liblinear   \n\n                                               params  split0_test_score  \\\n0   {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-...           0.978632   \n1      {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}           0.978632   \n2   {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...           0.978632   \n3   {'C': 0.1, 'penalty': 'none', 'solver': 'newto...           0.970085   \n4    {'C': 0.1, 'penalty': 'none', 'solver': 'lbfgs'}           0.957265   \n5   {'C': 0.1, 'penalty': 'none', 'solver': 'libli...                NaN   \n6    {'C': 5, 'penalty': 'l2', 'solver': 'newton-cg'}           0.978632   \n7        {'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}           0.978632   \n8    {'C': 5, 'penalty': 'l2', 'solver': 'liblinear'}           0.970085   \n9   {'C': 5, 'penalty': 'none', 'solver': 'newton-...           0.970085   \n10     {'C': 5, 'penalty': 'none', 'solver': 'lbfgs'}           0.957265   \n11  {'C': 5, 'penalty': 'none', 'solver': 'libline...                NaN   \n12  {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}           0.974359   \n13      {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}           0.974359   \n14  {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}           0.970085   \n15  {'C': 10, 'penalty': 'none', 'solver': 'newton...           0.970085   \n16    {'C': 10, 'penalty': 'none', 'solver': 'lbfgs'}           0.957265   \n17  {'C': 10, 'penalty': 'none', 'solver': 'liblin...                NaN   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.965812           0.974359           0.982833   \n1            0.965812           0.974359           0.982833   \n2            0.965812           0.961538           0.969957   \n3            0.957265           0.965812           0.974249   \n4            0.948718           0.965812           0.944206   \n5                 NaN                NaN                NaN   \n6            0.970085           0.965812           0.974249   \n7            0.970085           0.965812           0.974249   \n8            0.965812           0.952991           0.965665   \n9            0.957265           0.965812           0.974249   \n10           0.948718           0.965812           0.944206   \n11                NaN                NaN                NaN   \n12           0.965812           0.965812           0.978541   \n13           0.965812           0.965812           0.978541   \n14           0.965812           0.944444           0.969957   \n15           0.957265           0.965812           0.974249   \n16           0.948718           0.965812           0.944206   \n17                NaN                NaN                NaN   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.969957           0.974359           0.987179   \n1            0.969957           0.974359           0.987179   \n2            0.952790           0.957265           0.982906   \n3            0.961373           0.978632           0.982906   \n4            0.948498           0.970085           0.970085   \n5                 NaN                NaN                NaN   \n6            0.965665           0.974359           0.978632   \n7            0.969957           0.974359           0.978632   \n8            0.952790           0.957265           0.978632   \n9            0.961373           0.978632           0.982906   \n10           0.948498           0.970085           0.970085   \n11                NaN                NaN                NaN   \n12           0.965665           0.978632           0.978632   \n13           0.965665           0.974359           0.978632   \n14           0.952790           0.957265           0.974359   \n15           0.961373           0.978632           0.982906   \n16           0.948498           0.970085           0.970085   \n17                NaN                NaN                NaN   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.970085           0.957082           0.982833   \n1            0.970085           0.957082           0.982833   \n2            0.952991           0.948498           0.982833   \n3            0.974359           0.957082           0.974249   \n4            0.965812           0.922747           0.969957   \n5                 NaN                NaN                NaN   \n6            0.965812           0.965665           0.987124   \n7            0.965812           0.965665           0.987124   \n8            0.927350           0.948498           0.969957   \n9            0.974359           0.957082           0.974249   \n10           0.965812           0.922747           0.969957   \n11                NaN                NaN                NaN   \n12           0.965812           0.965665           0.987124   \n13           0.965812           0.965665           0.987124   \n14           0.935897           0.944206           0.965665   \n15           0.974359           0.957082           0.974249   \n16           0.965812           0.922747           0.969957   \n17                NaN                NaN                NaN   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.970085            0.961538            0.982906   \n1             0.970085            0.961538            0.982906   \n2             0.952991            0.961538            0.974359   \n3             0.965812            0.965812            0.978632   \n4             0.948718            0.957265            0.965812   \n5                  NaN                 NaN                 NaN   \n6             0.957265            0.957265            0.982906   \n7             0.957265            0.957265            0.982906   \n8             0.935897            0.940171            0.974359   \n9             0.965812            0.965812            0.978632   \n10            0.948718            0.957265            0.965812   \n11                 NaN                 NaN                 NaN   \n12            0.957265            0.957265            0.982906   \n13            0.957265            0.957265            0.982906   \n14            0.931624            0.940171            0.974359   \n15            0.965812            0.965812            0.978632   \n16            0.948718            0.957265            0.965812   \n17                 NaN                 NaN                 NaN   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.969957            0.957082         0.972313        0.009115   \n1             0.969957            0.957082         0.972313        0.009115   \n2             0.961373            0.969957         0.964896        0.010858   \n3             0.957082            0.935622         0.966598        0.011553   \n4             0.957082            0.939914         0.955465        0.013029   \n5                  NaN                 NaN              NaN             NaN   \n6             0.965665            0.948498         0.969176        0.010046   \n7             0.965665            0.948498         0.969462        0.010003   \n8             0.965665            0.957082         0.957481        0.014251   \n9             0.957082            0.935622         0.966598        0.011553   \n10            0.957082            0.939914         0.955465        0.013029   \n11                 NaN                 NaN              NaN             NaN   \n12            0.965665            0.948498         0.969177        0.010284   \n13            0.965665            0.944206         0.968606        0.010692   \n14            0.965665            0.952790         0.956339        0.013886   \n15            0.957082            0.935622         0.966598        0.011553   \n16            0.957082            0.939914         0.955465        0.013029   \n17                 NaN                 NaN              NaN             NaN   \n\n    rank_test_score  split0_train_score  split1_train_score  \\\n0                 1            1.000000            1.000000   \n1                 1            1.000000            1.000000   \n2                10            0.994647            0.998929   \n3                 7            1.000000            1.000000   \n4                13            1.000000            1.000000   \n5                17                 NaN                 NaN   \n6                 5            1.000000            1.000000   \n7                 3            1.000000            1.000000   \n8                11            0.998929            1.000000   \n9                 7            1.000000            1.000000   \n10               13            1.000000            1.000000   \n11               16                 NaN                 NaN   \n12                4            1.000000            1.000000   \n13                6            1.000000            1.000000   \n14               12            1.000000            1.000000   \n15                7            1.000000            1.000000   \n16               13            1.000000            1.000000   \n17               18                 NaN                 NaN   \n\n    split2_train_score  split3_train_score  split4_train_score  \\\n0             1.000000            1.000000            1.000000   \n1             1.000000            1.000000            1.000000   \n2             0.994647            0.995722            0.996791   \n3             1.000000            1.000000            1.000000   \n4             1.000000            1.000000            1.000000   \n5                  NaN                 NaN                 NaN   \n6             1.000000            1.000000            1.000000   \n7             1.000000            1.000000            1.000000   \n8             1.000000            1.000000            1.000000   \n9             1.000000            1.000000            1.000000   \n10            1.000000            1.000000            1.000000   \n11                 NaN                 NaN                 NaN   \n12            1.000000            1.000000            1.000000   \n13            1.000000            1.000000            1.000000   \n14            1.000000            1.000000            1.000000   \n15            1.000000            1.000000            1.000000   \n16            1.000000            1.000000            1.000000   \n17                 NaN                 NaN                 NaN   \n\n    split5_train_score  split6_train_score  split7_train_score  \\\n0             1.000000            1.000000            1.000000   \n1             1.000000            1.000000            1.000000   \n2             0.992505            0.996788            0.995717   \n3             1.000000            1.000000            1.000000   \n4             1.000000            1.000000            1.000000   \n5                  NaN                 NaN                 NaN   \n6             1.000000            1.000000            1.000000   \n7             1.000000            1.000000            1.000000   \n8             1.000000            0.998929            1.000000   \n9             1.000000            1.000000            1.000000   \n10            1.000000            1.000000            1.000000   \n11                 NaN                 NaN                 NaN   \n12            1.000000            1.000000            1.000000   \n13            1.000000            1.000000            1.000000   \n14            1.000000            1.000000            1.000000   \n15            1.000000            1.000000            1.000000   \n16            1.000000            1.000000            1.000000   \n17                 NaN                 NaN                 NaN   \n\n    split8_train_score  split9_train_score  split10_train_score  \\\n0             1.000000            1.000000             1.000000   \n1             1.000000            1.000000             1.000000   \n2             0.995722            0.996791             0.995717   \n3             1.000000            1.000000             1.000000   \n4             1.000000            1.000000             1.000000   \n5                  NaN                 NaN                  NaN   \n6             1.000000            1.000000             1.000000   \n7             1.000000            1.000000             1.000000   \n8             1.000000            1.000000             1.000000   \n9             1.000000            1.000000             1.000000   \n10            1.000000            1.000000             1.000000   \n11                 NaN                 NaN                  NaN   \n12            1.000000            1.000000             1.000000   \n13            1.000000            1.000000             1.000000   \n14            1.000000            1.000000             1.000000   \n15            1.000000            1.000000             1.000000   \n16            1.000000            1.000000             1.000000   \n17                 NaN                 NaN                  NaN   \n\n    split11_train_score  split12_train_score  split13_train_score  \\\n0              1.000000             1.000000             1.000000   \n1              1.000000             1.000000             1.000000   \n2              0.996788             0.995717             0.996791   \n3              1.000000             1.000000             1.000000   \n4              1.000000             1.000000             1.000000   \n5                   NaN                  NaN                  NaN   \n6              1.000000             1.000000             1.000000   \n7              1.000000             1.000000             1.000000   \n8              1.000000             1.000000             1.000000   \n9              1.000000             1.000000             1.000000   \n10             1.000000             1.000000             1.000000   \n11                  NaN                  NaN                  NaN   \n12             1.000000             1.000000             1.000000   \n13             1.000000             1.000000             1.000000   \n14             1.000000             1.000000             1.000000   \n15             1.000000             1.000000             1.000000   \n16             1.000000             1.000000             1.000000   \n17                  NaN                  NaN                  NaN   \n\n    split14_train_score  mean_train_score  std_train_score  \n0              1.000000          1.000000         0.000000  \n1              1.000000          1.000000         0.000000  \n2              0.996791          0.996004         0.001381  \n3              1.000000          1.000000         0.000000  \n4              1.000000          1.000000         0.000000  \n5                   NaN               NaN              NaN  \n6              1.000000          1.000000         0.000000  \n7              1.000000          1.000000         0.000000  \n8              0.998930          0.999786         0.000428  \n9              1.000000          1.000000         0.000000  \n10             1.000000          1.000000         0.000000  \n11                  NaN               NaN              NaN  \n12             1.000000          1.000000         0.000000  \n13             1.000000          1.000000         0.000000  \n14             1.000000          1.000000         0.000000  \n15             1.000000          1.000000         0.000000  \n16             1.000000          1.000000         0.000000  \n17                  NaN               NaN              NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_C</th>\n      <th>param_penalty</th>\n      <th>param_solver</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n      <th>split0_train_score</th>\n      <th>split1_train_score</th>\n      <th>split2_train_score</th>\n      <th>split3_train_score</th>\n      <th>split4_train_score</th>\n      <th>split5_train_score</th>\n      <th>split6_train_score</th>\n      <th>split7_train_score</th>\n      <th>split8_train_score</th>\n      <th>split9_train_score</th>\n      <th>split10_train_score</th>\n      <th>split11_train_score</th>\n      <th>split12_train_score</th>\n      <th>split13_train_score</th>\n      <th>split14_train_score</th>\n      <th>mean_train_score</th>\n      <th>std_train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.760930</td>\n      <td>0.186828</td>\n      <td>0.000673</td>\n      <td>0.002030</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'newton-...</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.957082</td>\n      <td>0.982833</td>\n      <td>0.970085</td>\n      <td>0.961538</td>\n      <td>0.982906</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.972313</td>\n      <td>0.009115</td>\n      <td>1</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.063472</td>\n      <td>0.719442</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.957082</td>\n      <td>0.982833</td>\n      <td>0.970085</td>\n      <td>0.961538</td>\n      <td>0.982906</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.972313</td>\n      <td>0.009115</td>\n      <td>1</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.051133</td>\n      <td>0.005570</td>\n      <td>0.000548</td>\n      <td>0.000909</td>\n      <td>0.1</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'libline...</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.952991</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.952991</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.964896</td>\n      <td>0.010858</td>\n      <td>10</td>\n      <td>0.994647</td>\n      <td>0.998929</td>\n      <td>0.994647</td>\n      <td>0.995722</td>\n      <td>0.996791</td>\n      <td>0.992505</td>\n      <td>0.996788</td>\n      <td>0.995717</td>\n      <td>0.995722</td>\n      <td>0.996791</td>\n      <td>0.995717</td>\n      <td>0.996788</td>\n      <td>0.995717</td>\n      <td>0.996791</td>\n      <td>0.996791</td>\n      <td>0.996004</td>\n      <td>0.001381</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.150434</td>\n      <td>0.061752</td>\n      <td>0.000309</td>\n      <td>0.000914</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'newto...</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.978632</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.957082</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.966598</td>\n      <td>0.011553</td>\n      <td>7</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.106588</td>\n      <td>0.162071</td>\n      <td>0.000140</td>\n      <td>0.000525</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.957265</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.948498</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.922747</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.939914</td>\n      <td>0.955465</td>\n      <td>0.013029</td>\n      <td>13</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000884</td>\n      <td>0.002117</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.1</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 0.1, 'penalty': 'none', 'solver': 'libli...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>17</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.824467</td>\n      <td>0.156376</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.969176</td>\n      <td>0.010046</td>\n      <td>5</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2.811047</td>\n      <td>1.313212</td>\n      <td>0.001211</td>\n      <td>0.002731</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.969462</td>\n      <td>0.010003</td>\n      <td>3</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.120490</td>\n      <td>0.024473</td>\n      <td>0.000677</td>\n      <td>0.002533</td>\n      <td>5</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 5, 'penalty': 'l2', 'solver': 'liblinear'}</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.952991</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.978632</td>\n      <td>0.927350</td>\n      <td>0.948498</td>\n      <td>0.969957</td>\n      <td>0.935897</td>\n      <td>0.940171</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.957481</td>\n      <td>0.014251</td>\n      <td>11</td>\n      <td>0.998929</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.998929</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.998930</td>\n      <td>0.999786</td>\n      <td>0.000428</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.161477</td>\n      <td>0.010363</td>\n      <td>0.001221</td>\n      <td>0.003124</td>\n      <td>5</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'newton-...</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.978632</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.957082</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.966598</td>\n      <td>0.011553</td>\n      <td>7</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.055977</td>\n      <td>0.004590</td>\n      <td>0.000388</td>\n      <td>0.001038</td>\n      <td>5</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.957265</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.948498</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.922747</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.939914</td>\n      <td>0.955465</td>\n      <td>0.013029</td>\n      <td>13</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.000073</td>\n      <td>0.000272</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 5, 'penalty': 'none', 'solver': 'libline...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>16</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.832793</td>\n      <td>0.078511</td>\n      <td>0.000139</td>\n      <td>0.000519</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>newton-cg</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.965665</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.948498</td>\n      <td>0.969177</td>\n      <td>0.010284</td>\n      <td>4</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3.154757</td>\n      <td>0.987532</td>\n      <td>0.000946</td>\n      <td>0.002200</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>lbfgs</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.965665</td>\n      <td>0.987124</td>\n      <td>0.957265</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.965665</td>\n      <td>0.944206</td>\n      <td>0.968606</td>\n      <td>0.010692</td>\n      <td>6</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.111418</td>\n      <td>0.007896</td>\n      <td>0.001755</td>\n      <td>0.003204</td>\n      <td>10</td>\n      <td>l2</td>\n      <td>liblinear</td>\n      <td>{'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.944444</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.957265</td>\n      <td>0.974359</td>\n      <td>0.935897</td>\n      <td>0.944206</td>\n      <td>0.965665</td>\n      <td>0.931624</td>\n      <td>0.940171</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.956339</td>\n      <td>0.013886</td>\n      <td>12</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.157436</td>\n      <td>0.010092</td>\n      <td>0.000405</td>\n      <td>0.001091</td>\n      <td>10</td>\n      <td>none</td>\n      <td>newton-cg</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'newton...</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.978632</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.957082</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.957082</td>\n      <td>0.935622</td>\n      <td>0.966598</td>\n      <td>0.011553</td>\n      <td>7</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.056531</td>\n      <td>0.002473</td>\n      <td>0.000800</td>\n      <td>0.001600</td>\n      <td>10</td>\n      <td>none</td>\n      <td>lbfgs</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'lbfgs'}</td>\n      <td>0.957265</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.944206</td>\n      <td>0.948498</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.922747</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.957265</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.939914</td>\n      <td>0.955465</td>\n      <td>0.013029</td>\n      <td>13</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.000800</td>\n      <td>0.001600</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>10</td>\n      <td>none</td>\n      <td>liblinear</td>\n      <td>{'C': 10, 'penalty': 'none', 'solver': 'liblin...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création du modèle de régression\n",
    "LR_model = LogisticRegression(verbose=False, max_iter=10000)\n",
    "\n",
    "# Dictionnaire contenant les différents paramètres à essayer\n",
    "LR_params = dict()\n",
    "LR_params['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "LR_params['penalty'] = ['l2', 'none']\n",
    "LR_params['C'] = [0.1, 5, 10]\n",
    "\n",
    "# Création de nos itérations\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "# Création de notre modèle de validation croisée\n",
    "model_cv = GridSearchCV(estimator=LR_model,\n",
    "                        param_grid=LR_params,\n",
    "                        scoring='accuracy',\n",
    "                        cv=kFold,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True)\n",
    "\n",
    "# Entrainement du modèle\n",
    "model_cv.fit(x_train, y_train)\n",
    "# cv results\n",
    "pd.set_option('display.max_columns', None)\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[0]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31144b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "La validation croisée nous permet de determiner les paramètres optimaux pour notre modèle d'apprentissage.\n",
    "Les résultats de nos tests avec de nombreux paramètres différents (dont la plupart ne sont pas présent dans l'exemple ci-dessus).\n",
    "* Le **paramètre C** correspond à l'inverse de la force de régularisation des données. Plus ce paramètre est grand, plus le risque d'overfitting est grand. Une valeur de 0.1 semble être optimale.\n",
    "* Le **paramètre de pénalité** permet de réduire les coefficients θ. On remarque une perte de précision si l'on n'applique pas de pénalité. La meilleure pénalité semble être la norme L2.\n",
    "* Le **solveur** correspond à l'algorithme d'optimisation utilisé pour l'entrainement. Dans notre cas, lbfgs permet d'obtenir la precision la plus élevée malgré un temps d'entrainement significativement plus long que ses concurrents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b3aae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Classification naïve bayésienne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9833fcac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n0       0.003171      0.005522         0.001410        0.004040           0   \n1       0.002977      0.003527         0.000814        0.002068      0.0001   \n2       0.003799      0.004176         0.000821        0.002068       0.001   \n3       0.005996      0.009946         0.001219        0.002179        0.01   \n4       0.002582      0.003426         0.000278        0.000709         0.1   \n5       0.004750      0.005685         0.001497        0.003105         0.5   \n6       0.005698      0.004642         0.000539        0.002017         1.0   \n\n              params  split0_test_score  split1_test_score  split2_test_score  \\\n0       {'alpha': 0}           0.837607           0.871795           0.854701   \n1  {'alpha': 0.0001}           0.841880           0.884615           0.854701   \n2   {'alpha': 0.001}           0.846154           0.884615           0.854701   \n3    {'alpha': 0.01}           0.850427           0.884615           0.854701   \n4     {'alpha': 0.1}           0.854701           0.880342           0.854701   \n5     {'alpha': 0.5}           0.850427           0.876068           0.850427   \n6     {'alpha': 1.0}           0.850427           0.871795           0.850427   \n\n   split3_test_score  split4_test_score  split5_test_score  split6_test_score  \\\n0           0.836910           0.845494           0.871795           0.850427   \n1           0.849785           0.854077           0.876068           0.880342   \n2           0.849785           0.858369           0.876068           0.884615   \n3           0.849785           0.858369           0.867521           0.876068   \n4           0.849785           0.858369           0.863248           0.871795   \n5           0.845494           0.845494           0.858974           0.867521   \n6           0.841202           0.845494           0.858974           0.867521   \n\n   split7_test_score  split8_test_score  split9_test_score  \\\n0           0.854701           0.819742           0.875536   \n1           0.863248           0.828326           0.879828   \n2           0.867521           0.836910           0.875536   \n3           0.867521           0.841202           0.875536   \n4           0.867521           0.841202           0.884120   \n5           0.867521           0.836910           0.884120   \n6           0.867521           0.832618           0.879828   \n\n   split10_test_score  split11_test_score  split12_test_score  \\\n0            0.833333            0.854701            0.858974   \n1            0.854701            0.858974            0.867521   \n2            0.854701            0.858974            0.867521   \n3            0.858974            0.858974            0.863248   \n4            0.867521            0.858974            0.854701   \n5            0.871795            0.850427            0.854701   \n6            0.871795            0.841880            0.854701   \n\n   split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0            0.866953            0.871245         0.853594        0.015985   \n1            0.871245            0.884120         0.863296        0.015941   \n2            0.871245            0.884120         0.864722        0.014346   \n3            0.871245            0.884120         0.864154        0.012277   \n4            0.871245            0.888412         0.864442        0.012759   \n5            0.854077            0.875536         0.859300        0.013215   \n6            0.854077            0.875536         0.857587        0.013731   \n\n   rank_test_score  \n0                7  \n1                4  \n2                1  \n3                3  \n4                2  \n5                5  \n6                6  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_alpha</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.003171</td>\n      <td>0.005522</td>\n      <td>0.001410</td>\n      <td>0.004040</td>\n      <td>0</td>\n      <td>{'alpha': 0}</td>\n      <td>0.837607</td>\n      <td>0.871795</td>\n      <td>0.854701</td>\n      <td>0.836910</td>\n      <td>0.845494</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.854701</td>\n      <td>0.819742</td>\n      <td>0.875536</td>\n      <td>0.833333</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.866953</td>\n      <td>0.871245</td>\n      <td>0.853594</td>\n      <td>0.015985</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.002977</td>\n      <td>0.003527</td>\n      <td>0.000814</td>\n      <td>0.002068</td>\n      <td>0.0001</td>\n      <td>{'alpha': 0.0001}</td>\n      <td>0.841880</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.854077</td>\n      <td>0.876068</td>\n      <td>0.880342</td>\n      <td>0.863248</td>\n      <td>0.828326</td>\n      <td>0.879828</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.863296</td>\n      <td>0.015941</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.003799</td>\n      <td>0.004176</td>\n      <td>0.000821</td>\n      <td>0.002068</td>\n      <td>0.001</td>\n      <td>{'alpha': 0.001}</td>\n      <td>0.846154</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.876068</td>\n      <td>0.884615</td>\n      <td>0.867521</td>\n      <td>0.836910</td>\n      <td>0.875536</td>\n      <td>0.854701</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.864722</td>\n      <td>0.014346</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.005996</td>\n      <td>0.009946</td>\n      <td>0.001219</td>\n      <td>0.002179</td>\n      <td>0.01</td>\n      <td>{'alpha': 0.01}</td>\n      <td>0.850427</td>\n      <td>0.884615</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.867521</td>\n      <td>0.876068</td>\n      <td>0.867521</td>\n      <td>0.841202</td>\n      <td>0.875536</td>\n      <td>0.858974</td>\n      <td>0.858974</td>\n      <td>0.863248</td>\n      <td>0.871245</td>\n      <td>0.884120</td>\n      <td>0.864154</td>\n      <td>0.012277</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.002582</td>\n      <td>0.003426</td>\n      <td>0.000278</td>\n      <td>0.000709</td>\n      <td>0.1</td>\n      <td>{'alpha': 0.1}</td>\n      <td>0.854701</td>\n      <td>0.880342</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.858369</td>\n      <td>0.863248</td>\n      <td>0.871795</td>\n      <td>0.867521</td>\n      <td>0.841202</td>\n      <td>0.884120</td>\n      <td>0.867521</td>\n      <td>0.858974</td>\n      <td>0.854701</td>\n      <td>0.871245</td>\n      <td>0.888412</td>\n      <td>0.864442</td>\n      <td>0.012759</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.004750</td>\n      <td>0.005685</td>\n      <td>0.001497</td>\n      <td>0.003105</td>\n      <td>0.5</td>\n      <td>{'alpha': 0.5}</td>\n      <td>0.850427</td>\n      <td>0.876068</td>\n      <td>0.850427</td>\n      <td>0.845494</td>\n      <td>0.845494</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.867521</td>\n      <td>0.836910</td>\n      <td>0.884120</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.854701</td>\n      <td>0.854077</td>\n      <td>0.875536</td>\n      <td>0.859300</td>\n      <td>0.013215</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.005698</td>\n      <td>0.004642</td>\n      <td>0.000539</td>\n      <td>0.002017</td>\n      <td>1.0</td>\n      <td>{'alpha': 1.0}</td>\n      <td>0.850427</td>\n      <td>0.871795</td>\n      <td>0.850427</td>\n      <td>0.841202</td>\n      <td>0.845494</td>\n      <td>0.858974</td>\n      <td>0.867521</td>\n      <td>0.867521</td>\n      <td>0.832618</td>\n      <td>0.879828</td>\n      <td>0.871795</td>\n      <td>0.841880</td>\n      <td>0.854701</td>\n      <td>0.854077</td>\n      <td>0.875536</td>\n      <td>0.857587</td>\n      <td>0.013731</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "NB_model = BernoulliNB()\n",
    "\n",
    "NB_params = {'alpha': [0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0]}\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(NB_model, NB_params, n_iter=7, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "# NB = RCV.fit(x_train, y_train).best_estimator_\n",
    "RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(RCV.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[1]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le seul paramètre testé dans ce modèle, alpha, correspond à la force du lissage de Laplace a appliqué au modèle.\n",
    "En d'autres termes, ce paramètre permet de palier la présence d'une caractéristique dans le jeu de test qui n'existe pas dans le jeu d'entrainement.\n",
    "> https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece\n",
    "\n",
    "Au vu des résultats, le paramètre alpha semble être optimal autour de 0.001\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification par arbre de décision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\paula\\desktop\\mines\\machine_learning\\mines-ai-number-recognition-project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 32 is smaller than n_iter=50. Running 32 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        0.011902      0.024716         0.001043        0.003903   \n1        0.004164      0.006905         0.002082        0.005309   \n2        0.004605      0.006843         0.002085        0.005315   \n3        0.002957      0.005438         0.000436        0.001630   \n4        0.008342      0.009671         0.000000        0.000000   \n5        0.005208      0.007366         0.001042        0.003900   \n6        0.005208      0.007366         0.000000        0.000000   \n7        0.003129      0.006258         0.000000        0.000000   \n8        0.016596      0.005827         0.000137        0.000511   \n9        0.005798      0.005652         0.000136        0.000508   \n10       0.016840      0.008435         0.000831        0.002218   \n11       0.009993      0.009954         0.000676        0.002528   \n12       0.013380      0.005211         0.001077        0.002747   \n13       0.004868      0.004952         0.000000        0.000000   \n14       0.012028      0.003350         0.001079        0.002745   \n15       0.004592      0.004463         0.000673        0.002518   \n16       0.007704      0.004127         0.000540        0.002021   \n17       0.002163      0.003423         0.000676        0.002047   \n18       0.007716      0.004798         0.001214        0.002742   \n19       0.002312      0.003035         0.000539        0.002016   \n20       0.008115      0.003122         0.000000        0.000000   \n21       0.001488      0.002705         0.001082        0.002758   \n22       0.006766      0.004023         0.000539        0.002015   \n23       0.002571      0.003420         0.000671        0.002037   \n24       0.020263      0.006549         0.000000        0.000000   \n25       0.005941      0.004398         0.000672        0.002031   \n26       0.019042      0.004335         0.000136        0.000507   \n27       0.006498      0.004509         0.000539        0.002017   \n28       0.017939      0.003692         0.000135        0.000507   \n29       0.007168      0.004117         0.000541        0.002023   \n30       0.016999      0.003684         0.000275        0.000702   \n31       0.003118      0.004309         0.000538        0.002013   \n\n   param_splitter param_min_samples_leaf param_max_depth param_criterion  \\\n0            best                      1               3            gini   \n1          random                      1               3            gini   \n2            best                      2               3            gini   \n3          random                      2               3            gini   \n4            best                      3               3            gini   \n5          random                      3               3            gini   \n6            best                      4               3            gini   \n7          random                      4               3            gini   \n8            best                      1            None            gini   \n9          random                      1            None            gini   \n10           best                      2            None            gini   \n11         random                      2            None            gini   \n12           best                      3            None            gini   \n13         random                      3            None            gini   \n14           best                      4            None            gini   \n15         random                      4            None            gini   \n16           best                      1               3         entropy   \n17         random                      1               3         entropy   \n18           best                      2               3         entropy   \n19         random                      2               3         entropy   \n20           best                      3               3         entropy   \n21         random                      3               3         entropy   \n22           best                      4               3         entropy   \n23         random                      4               3         entropy   \n24           best                      1            None         entropy   \n25         random                      1            None         entropy   \n26           best                      2            None         entropy   \n27         random                      2            None         entropy   \n28           best                      3            None         entropy   \n29         random                      3            None         entropy   \n30           best                      4            None         entropy   \n31         random                      4            None         entropy   \n\n                                               params  split0_test_score  \\\n0   {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.410256   \n1   {'splitter': 'random', 'min_samples_leaf': 1, ...           0.491453   \n2   {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.410256   \n3   {'splitter': 'random', 'min_samples_leaf': 2, ...           0.465812   \n4   {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.410256   \n5   {'splitter': 'random', 'min_samples_leaf': 3, ...           0.427350   \n6   {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.410256   \n7   {'splitter': 'random', 'min_samples_leaf': 4, ...           0.410256   \n8   {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.769231   \n9   {'splitter': 'random', 'min_samples_leaf': 1, ...           0.841880   \n10  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.756410   \n11  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.786325   \n12  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.743590   \n13  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.777778   \n14  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.735043   \n15  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.739316   \n16  {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.495726   \n17  {'splitter': 'random', 'min_samples_leaf': 1, ...           0.525641   \n18  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.495726   \n19  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.491453   \n20  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.495726   \n21  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.508547   \n22  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.495726   \n23  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.547009   \n24  {'splitter': 'best', 'min_samples_leaf': 1, 'm...           0.773504   \n25  {'splitter': 'random', 'min_samples_leaf': 1, ...           0.782051   \n26  {'splitter': 'best', 'min_samples_leaf': 2, 'm...           0.769231   \n27  {'splitter': 'random', 'min_samples_leaf': 2, ...           0.743590   \n28  {'splitter': 'best', 'min_samples_leaf': 3, 'm...           0.769231   \n29  {'splitter': 'random', 'min_samples_leaf': 3, ...           0.786325   \n30  {'splitter': 'best', 'min_samples_leaf': 4, 'm...           0.760684   \n31  {'splitter': 'random', 'min_samples_leaf': 4, ...           0.769231   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.470085           0.410256           0.484979   \n1            0.491453           0.517094           0.523605   \n2            0.470085           0.410256           0.484979   \n3            0.470085           0.495726           0.557940   \n4            0.470085           0.410256           0.484979   \n5            0.525641           0.423077           0.557940   \n6            0.470085           0.410256           0.484979   \n7            0.529915           0.431624           0.545064   \n8            0.846154           0.858974           0.828326   \n9            0.841880           0.854701           0.828326   \n10           0.824786           0.846154           0.802575   \n11           0.854701           0.850427           0.789700   \n12           0.850427           0.833333           0.789700   \n13           0.807692           0.820513           0.798283   \n14           0.846154           0.824786           0.789700   \n15           0.824786           0.829060           0.772532   \n16           0.529915           0.555556           0.575107   \n17           0.542735           0.517094           0.510730   \n18           0.529915           0.555556           0.575107   \n19           0.551282           0.482906           0.519313   \n20           0.529915           0.555556           0.575107   \n21           0.465812           0.482906           0.600858   \n22           0.529915           0.555556           0.575107   \n23           0.474359           0.547009           0.506438   \n24           0.871795           0.829060           0.832618   \n25           0.846154           0.858974           0.858369   \n26           0.820513           0.807692           0.845494   \n27           0.846154           0.884615           0.768240   \n28           0.850427           0.807692           0.828326   \n29           0.794872           0.820513           0.828326   \n30           0.871795           0.820513           0.824034   \n31           0.794872           0.863248           0.781116   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.463519           0.423077           0.457265   \n1            0.545064           0.529915           0.500000   \n2            0.463519           0.423077           0.457265   \n3            0.450644           0.448718           0.410256   \n4            0.463519           0.423077           0.457265   \n5            0.416309           0.431624           0.551282   \n6            0.463519           0.423077           0.457265   \n7            0.442060           0.487179           0.474359   \n8            0.793991           0.850427           0.850427   \n9            0.789700           0.807692           0.871795   \n10           0.785408           0.841880           0.833333   \n11           0.828326           0.816239           0.854701   \n12           0.776824           0.837607           0.829060   \n13           0.828326           0.833333           0.833333   \n14           0.759657           0.837607           0.837607   \n15           0.832618           0.794872           0.833333   \n16           0.553648           0.576923           0.538462   \n17           0.540773           0.521368           0.525641   \n18           0.553648           0.576923           0.538462   \n19           0.549356           0.521368           0.512821   \n20           0.553648           0.576923           0.538462   \n21           0.540773           0.491453           0.611111   \n22           0.553648           0.576923           0.538462   \n23           0.540773           0.470085           0.508547   \n24           0.811159           0.833333           0.820513   \n25           0.793991           0.871795           0.841880   \n26           0.811159           0.846154           0.803419   \n27           0.828326           0.807692           0.846154   \n28           0.824034           0.833333           0.803419   \n29           0.815451           0.829060           0.863248   \n30           0.806867           0.841880           0.811966   \n31           0.806867           0.846154           0.820513   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.457265           0.472103           0.472103   \n1            0.474359           0.454936           0.437768   \n2            0.457265           0.472103           0.472103   \n3            0.551282           0.480687           0.549356   \n4            0.457265           0.472103           0.467811   \n5            0.376068           0.557940           0.506438   \n6            0.457265           0.472103           0.467811   \n7            0.482906           0.450644           0.450644   \n8            0.807692           0.781116           0.768240   \n9            0.794872           0.755365           0.802575   \n10           0.820513           0.742489           0.751073   \n11           0.799145           0.806867           0.785408   \n12           0.794872           0.759657           0.755365   \n13           0.811966           0.793991           0.811159   \n14           0.807692           0.768240           0.751073   \n15           0.799145           0.798283           0.781116   \n16           0.581197           0.532189           0.566524   \n17           0.482906           0.506438           0.536481   \n18           0.581197           0.532189           0.566524   \n19           0.500000           0.545064           0.549356   \n20           0.581197           0.532189           0.566524   \n21           0.581197           0.566524           0.562232   \n22           0.581197           0.532189           0.566524   \n23           0.491453           0.536481           0.467811   \n24           0.858974           0.755365           0.828326   \n25           0.833333           0.815451           0.862661   \n26           0.846154           0.738197           0.806867   \n27           0.794872           0.819742           0.802575   \n28           0.850427           0.742489           0.815451   \n29           0.850427           0.776824           0.884120   \n30           0.846154           0.729614           0.815451   \n31           0.816239           0.793991           0.849785   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.423077            0.452991            0.465812   \n1             0.478632            0.512821            0.474359   \n2             0.423077            0.452991            0.465812   \n3             0.534188            0.470085            0.405983   \n4             0.423077            0.452991            0.465812   \n5             0.521368            0.525641            0.568376   \n6             0.423077            0.452991            0.465812   \n7             0.500000            0.585470            0.491453   \n8             0.764957            0.841880            0.837607   \n9             0.816239            0.816239            0.786325   \n10            0.752137            0.850427            0.824786   \n11            0.811966            0.829060            0.794872   \n12            0.769231            0.811966            0.841880   \n13            0.735043            0.850427            0.837607   \n14            0.752137            0.807692            0.816239   \n15            0.777778            0.811966            0.794872   \n16            0.559829            0.576923            0.581197   \n17            0.559829            0.495726            0.504274   \n18            0.559829            0.576923            0.581197   \n19            0.538462            0.572650            0.487179   \n20            0.559829            0.576923            0.581197   \n21            0.529915            0.521368            0.589744   \n22            0.559829            0.576923            0.581197   \n23            0.500000            0.632479            0.538462   \n24            0.756410            0.914530            0.782051   \n25            0.807692            0.884615            0.811966   \n26            0.773504            0.897436            0.790598   \n27            0.794872            0.816239            0.837607   \n28            0.764957            0.871795            0.794872   \n29            0.794872            0.816239            0.824786   \n30            0.773504            0.880342            0.769231   \n31            0.739316            0.794872            0.794872   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.493562            0.446352         0.453514        0.025147   \n1             0.480687            0.476395         0.492569        0.028122   \n2             0.493562            0.446352         0.453514        0.025147   \n3             0.476395            0.515021         0.485479        0.046562   \n4             0.493562            0.442060         0.452941        0.025060   \n5             0.532189            0.463519         0.492317        0.060854   \n6             0.493562            0.446352         0.453227        0.024958   \n7             0.459227            0.467811         0.480574        0.044283   \n8             0.858369            0.776824         0.815614        0.035201   \n9             0.828326            0.824034         0.817330        0.028562   \n10            0.836910            0.802575         0.804764        0.036813   \n11            0.789700            0.776824         0.811617        0.025534   \n12            0.841202            0.785408         0.801341        0.034748   \n13            0.854077            0.772532         0.811071        0.030900   \n14            0.845494            0.789700         0.797921        0.036252   \n15            0.815451            0.802575         0.800514        0.024983   \n16            0.549356            0.545064         0.554508        0.023036   \n17            0.562232            0.557940         0.525987        0.023163   \n18            0.549356            0.545064         0.554508        0.023036   \n19            0.549356            0.484979         0.523703        0.028340   \n20            0.549356            0.545064         0.554508        0.023036   \n21            0.489270            0.553648         0.539690        0.044308   \n22            0.549356            0.545064         0.554508        0.023036   \n23            0.532189            0.557940         0.523402        0.041148   \n24            0.854077            0.802575         0.821619        0.042390   \n25            0.811159            0.832618         0.834181        0.028964   \n26            0.845494            0.798283         0.813346        0.037811   \n27            0.811159            0.819742         0.814772        0.032497   \n28            0.832618            0.806867         0.813063        0.033765   \n29            0.811159            0.815451         0.820778        0.027650   \n30            0.862661            0.789700         0.813626        0.041886   \n31            0.845494            0.798283         0.807657        0.032285   \n\n    rank_test_score  \n0                29  \n1                25  \n2                29  \n3                27  \n4                32  \n5                26  \n6                31  \n7                28  \n8                 5  \n9                 4  \n10               13  \n11               10  \n12               14  \n13               11  \n14               16  \n15               15  \n16               17  \n17               22  \n18               17  \n19               23  \n20               17  \n21               21  \n22               17  \n23               24  \n24                2  \n25                1  \n26                8  \n27                6  \n28                9  \n29                3  \n30                7  \n31               12  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_splitter</th>\n      <th>param_min_samples_leaf</th>\n      <th>param_max_depth</th>\n      <th>param_criterion</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.011902</td>\n      <td>0.024716</td>\n      <td>0.001043</td>\n      <td>0.003903</td>\n      <td>best</td>\n      <td>1</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.472103</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453514</td>\n      <td>0.025147</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.004164</td>\n      <td>0.006905</td>\n      <td>0.002082</td>\n      <td>0.005309</td>\n      <td>random</td>\n      <td>1</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.491453</td>\n      <td>0.491453</td>\n      <td>0.517094</td>\n      <td>0.523605</td>\n      <td>0.545064</td>\n      <td>0.529915</td>\n      <td>0.500000</td>\n      <td>0.474359</td>\n      <td>0.454936</td>\n      <td>0.437768</td>\n      <td>0.478632</td>\n      <td>0.512821</td>\n      <td>0.474359</td>\n      <td>0.480687</td>\n      <td>0.476395</td>\n      <td>0.492569</td>\n      <td>0.028122</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.004605</td>\n      <td>0.006843</td>\n      <td>0.002085</td>\n      <td>0.005315</td>\n      <td>best</td>\n      <td>2</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.472103</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453514</td>\n      <td>0.025147</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.002957</td>\n      <td>0.005438</td>\n      <td>0.000436</td>\n      <td>0.001630</td>\n      <td>random</td>\n      <td>2</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.465812</td>\n      <td>0.470085</td>\n      <td>0.495726</td>\n      <td>0.557940</td>\n      <td>0.450644</td>\n      <td>0.448718</td>\n      <td>0.410256</td>\n      <td>0.551282</td>\n      <td>0.480687</td>\n      <td>0.549356</td>\n      <td>0.534188</td>\n      <td>0.470085</td>\n      <td>0.405983</td>\n      <td>0.476395</td>\n      <td>0.515021</td>\n      <td>0.485479</td>\n      <td>0.046562</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.008342</td>\n      <td>0.009671</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>best</td>\n      <td>3</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.442060</td>\n      <td>0.452941</td>\n      <td>0.025060</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.005208</td>\n      <td>0.007366</td>\n      <td>0.001042</td>\n      <td>0.003900</td>\n      <td>random</td>\n      <td>3</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.427350</td>\n      <td>0.525641</td>\n      <td>0.423077</td>\n      <td>0.557940</td>\n      <td>0.416309</td>\n      <td>0.431624</td>\n      <td>0.551282</td>\n      <td>0.376068</td>\n      <td>0.557940</td>\n      <td>0.506438</td>\n      <td>0.521368</td>\n      <td>0.525641</td>\n      <td>0.568376</td>\n      <td>0.532189</td>\n      <td>0.463519</td>\n      <td>0.492317</td>\n      <td>0.060854</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.005208</td>\n      <td>0.007366</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>best</td>\n      <td>4</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.410256</td>\n      <td>0.470085</td>\n      <td>0.410256</td>\n      <td>0.484979</td>\n      <td>0.463519</td>\n      <td>0.423077</td>\n      <td>0.457265</td>\n      <td>0.457265</td>\n      <td>0.472103</td>\n      <td>0.467811</td>\n      <td>0.423077</td>\n      <td>0.452991</td>\n      <td>0.465812</td>\n      <td>0.493562</td>\n      <td>0.446352</td>\n      <td>0.453227</td>\n      <td>0.024958</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.003129</td>\n      <td>0.006258</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>random</td>\n      <td>4</td>\n      <td>3</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.410256</td>\n      <td>0.529915</td>\n      <td>0.431624</td>\n      <td>0.545064</td>\n      <td>0.442060</td>\n      <td>0.487179</td>\n      <td>0.474359</td>\n      <td>0.482906</td>\n      <td>0.450644</td>\n      <td>0.450644</td>\n      <td>0.500000</td>\n      <td>0.585470</td>\n      <td>0.491453</td>\n      <td>0.459227</td>\n      <td>0.467811</td>\n      <td>0.480574</td>\n      <td>0.044283</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.016596</td>\n      <td>0.005827</td>\n      <td>0.000137</td>\n      <td>0.000511</td>\n      <td>best</td>\n      <td>1</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.769231</td>\n      <td>0.846154</td>\n      <td>0.858974</td>\n      <td>0.828326</td>\n      <td>0.793991</td>\n      <td>0.850427</td>\n      <td>0.850427</td>\n      <td>0.807692</td>\n      <td>0.781116</td>\n      <td>0.768240</td>\n      <td>0.764957</td>\n      <td>0.841880</td>\n      <td>0.837607</td>\n      <td>0.858369</td>\n      <td>0.776824</td>\n      <td>0.815614</td>\n      <td>0.035201</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.005798</td>\n      <td>0.005652</td>\n      <td>0.000136</td>\n      <td>0.000508</td>\n      <td>random</td>\n      <td>1</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.841880</td>\n      <td>0.841880</td>\n      <td>0.854701</td>\n      <td>0.828326</td>\n      <td>0.789700</td>\n      <td>0.807692</td>\n      <td>0.871795</td>\n      <td>0.794872</td>\n      <td>0.755365</td>\n      <td>0.802575</td>\n      <td>0.816239</td>\n      <td>0.816239</td>\n      <td>0.786325</td>\n      <td>0.828326</td>\n      <td>0.824034</td>\n      <td>0.817330</td>\n      <td>0.028562</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.016840</td>\n      <td>0.008435</td>\n      <td>0.000831</td>\n      <td>0.002218</td>\n      <td>best</td>\n      <td>2</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.756410</td>\n      <td>0.824786</td>\n      <td>0.846154</td>\n      <td>0.802575</td>\n      <td>0.785408</td>\n      <td>0.841880</td>\n      <td>0.833333</td>\n      <td>0.820513</td>\n      <td>0.742489</td>\n      <td>0.751073</td>\n      <td>0.752137</td>\n      <td>0.850427</td>\n      <td>0.824786</td>\n      <td>0.836910</td>\n      <td>0.802575</td>\n      <td>0.804764</td>\n      <td>0.036813</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.009993</td>\n      <td>0.009954</td>\n      <td>0.000676</td>\n      <td>0.002528</td>\n      <td>random</td>\n      <td>2</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.786325</td>\n      <td>0.854701</td>\n      <td>0.850427</td>\n      <td>0.789700</td>\n      <td>0.828326</td>\n      <td>0.816239</td>\n      <td>0.854701</td>\n      <td>0.799145</td>\n      <td>0.806867</td>\n      <td>0.785408</td>\n      <td>0.811966</td>\n      <td>0.829060</td>\n      <td>0.794872</td>\n      <td>0.789700</td>\n      <td>0.776824</td>\n      <td>0.811617</td>\n      <td>0.025534</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.013380</td>\n      <td>0.005211</td>\n      <td>0.001077</td>\n      <td>0.002747</td>\n      <td>best</td>\n      <td>3</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.743590</td>\n      <td>0.850427</td>\n      <td>0.833333</td>\n      <td>0.789700</td>\n      <td>0.776824</td>\n      <td>0.837607</td>\n      <td>0.829060</td>\n      <td>0.794872</td>\n      <td>0.759657</td>\n      <td>0.755365</td>\n      <td>0.769231</td>\n      <td>0.811966</td>\n      <td>0.841880</td>\n      <td>0.841202</td>\n      <td>0.785408</td>\n      <td>0.801341</td>\n      <td>0.034748</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.004868</td>\n      <td>0.004952</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>random</td>\n      <td>3</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.777778</td>\n      <td>0.807692</td>\n      <td>0.820513</td>\n      <td>0.798283</td>\n      <td>0.828326</td>\n      <td>0.833333</td>\n      <td>0.833333</td>\n      <td>0.811966</td>\n      <td>0.793991</td>\n      <td>0.811159</td>\n      <td>0.735043</td>\n      <td>0.850427</td>\n      <td>0.837607</td>\n      <td>0.854077</td>\n      <td>0.772532</td>\n      <td>0.811071</td>\n      <td>0.030900</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.012028</td>\n      <td>0.003350</td>\n      <td>0.001079</td>\n      <td>0.002745</td>\n      <td>best</td>\n      <td>4</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.735043</td>\n      <td>0.846154</td>\n      <td>0.824786</td>\n      <td>0.789700</td>\n      <td>0.759657</td>\n      <td>0.837607</td>\n      <td>0.837607</td>\n      <td>0.807692</td>\n      <td>0.768240</td>\n      <td>0.751073</td>\n      <td>0.752137</td>\n      <td>0.807692</td>\n      <td>0.816239</td>\n      <td>0.845494</td>\n      <td>0.789700</td>\n      <td>0.797921</td>\n      <td>0.036252</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.004592</td>\n      <td>0.004463</td>\n      <td>0.000673</td>\n      <td>0.002518</td>\n      <td>random</td>\n      <td>4</td>\n      <td>None</td>\n      <td>gini</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.739316</td>\n      <td>0.824786</td>\n      <td>0.829060</td>\n      <td>0.772532</td>\n      <td>0.832618</td>\n      <td>0.794872</td>\n      <td>0.833333</td>\n      <td>0.799145</td>\n      <td>0.798283</td>\n      <td>0.781116</td>\n      <td>0.777778</td>\n      <td>0.811966</td>\n      <td>0.794872</td>\n      <td>0.815451</td>\n      <td>0.802575</td>\n      <td>0.800514</td>\n      <td>0.024983</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.007704</td>\n      <td>0.004127</td>\n      <td>0.000540</td>\n      <td>0.002021</td>\n      <td>best</td>\n      <td>1</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.002163</td>\n      <td>0.003423</td>\n      <td>0.000676</td>\n      <td>0.002047</td>\n      <td>random</td>\n      <td>1</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.525641</td>\n      <td>0.542735</td>\n      <td>0.517094</td>\n      <td>0.510730</td>\n      <td>0.540773</td>\n      <td>0.521368</td>\n      <td>0.525641</td>\n      <td>0.482906</td>\n      <td>0.506438</td>\n      <td>0.536481</td>\n      <td>0.559829</td>\n      <td>0.495726</td>\n      <td>0.504274</td>\n      <td>0.562232</td>\n      <td>0.557940</td>\n      <td>0.525987</td>\n      <td>0.023163</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.007716</td>\n      <td>0.004798</td>\n      <td>0.001214</td>\n      <td>0.002742</td>\n      <td>best</td>\n      <td>2</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.002312</td>\n      <td>0.003035</td>\n      <td>0.000539</td>\n      <td>0.002016</td>\n      <td>random</td>\n      <td>2</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.491453</td>\n      <td>0.551282</td>\n      <td>0.482906</td>\n      <td>0.519313</td>\n      <td>0.549356</td>\n      <td>0.521368</td>\n      <td>0.512821</td>\n      <td>0.500000</td>\n      <td>0.545064</td>\n      <td>0.549356</td>\n      <td>0.538462</td>\n      <td>0.572650</td>\n      <td>0.487179</td>\n      <td>0.549356</td>\n      <td>0.484979</td>\n      <td>0.523703</td>\n      <td>0.028340</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.008115</td>\n      <td>0.003122</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>best</td>\n      <td>3</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.001488</td>\n      <td>0.002705</td>\n      <td>0.001082</td>\n      <td>0.002758</td>\n      <td>random</td>\n      <td>3</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.508547</td>\n      <td>0.465812</td>\n      <td>0.482906</td>\n      <td>0.600858</td>\n      <td>0.540773</td>\n      <td>0.491453</td>\n      <td>0.611111</td>\n      <td>0.581197</td>\n      <td>0.566524</td>\n      <td>0.562232</td>\n      <td>0.529915</td>\n      <td>0.521368</td>\n      <td>0.589744</td>\n      <td>0.489270</td>\n      <td>0.553648</td>\n      <td>0.539690</td>\n      <td>0.044308</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.006766</td>\n      <td>0.004023</td>\n      <td>0.000539</td>\n      <td>0.002015</td>\n      <td>best</td>\n      <td>4</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.495726</td>\n      <td>0.529915</td>\n      <td>0.555556</td>\n      <td>0.575107</td>\n      <td>0.553648</td>\n      <td>0.576923</td>\n      <td>0.538462</td>\n      <td>0.581197</td>\n      <td>0.532189</td>\n      <td>0.566524</td>\n      <td>0.559829</td>\n      <td>0.576923</td>\n      <td>0.581197</td>\n      <td>0.549356</td>\n      <td>0.545064</td>\n      <td>0.554508</td>\n      <td>0.023036</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.002571</td>\n      <td>0.003420</td>\n      <td>0.000671</td>\n      <td>0.002037</td>\n      <td>random</td>\n      <td>4</td>\n      <td>3</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.547009</td>\n      <td>0.474359</td>\n      <td>0.547009</td>\n      <td>0.506438</td>\n      <td>0.540773</td>\n      <td>0.470085</td>\n      <td>0.508547</td>\n      <td>0.491453</td>\n      <td>0.536481</td>\n      <td>0.467811</td>\n      <td>0.500000</td>\n      <td>0.632479</td>\n      <td>0.538462</td>\n      <td>0.532189</td>\n      <td>0.557940</td>\n      <td>0.523402</td>\n      <td>0.041148</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.020263</td>\n      <td>0.006549</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>best</td>\n      <td>1</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 1, 'm...</td>\n      <td>0.773504</td>\n      <td>0.871795</td>\n      <td>0.829060</td>\n      <td>0.832618</td>\n      <td>0.811159</td>\n      <td>0.833333</td>\n      <td>0.820513</td>\n      <td>0.858974</td>\n      <td>0.755365</td>\n      <td>0.828326</td>\n      <td>0.756410</td>\n      <td>0.914530</td>\n      <td>0.782051</td>\n      <td>0.854077</td>\n      <td>0.802575</td>\n      <td>0.821619</td>\n      <td>0.042390</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.005941</td>\n      <td>0.004398</td>\n      <td>0.000672</td>\n      <td>0.002031</td>\n      <td>random</td>\n      <td>1</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 1, ...</td>\n      <td>0.782051</td>\n      <td>0.846154</td>\n      <td>0.858974</td>\n      <td>0.858369</td>\n      <td>0.793991</td>\n      <td>0.871795</td>\n      <td>0.841880</td>\n      <td>0.833333</td>\n      <td>0.815451</td>\n      <td>0.862661</td>\n      <td>0.807692</td>\n      <td>0.884615</td>\n      <td>0.811966</td>\n      <td>0.811159</td>\n      <td>0.832618</td>\n      <td>0.834181</td>\n      <td>0.028964</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.019042</td>\n      <td>0.004335</td>\n      <td>0.000136</td>\n      <td>0.000507</td>\n      <td>best</td>\n      <td>2</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 2, 'm...</td>\n      <td>0.769231</td>\n      <td>0.820513</td>\n      <td>0.807692</td>\n      <td>0.845494</td>\n      <td>0.811159</td>\n      <td>0.846154</td>\n      <td>0.803419</td>\n      <td>0.846154</td>\n      <td>0.738197</td>\n      <td>0.806867</td>\n      <td>0.773504</td>\n      <td>0.897436</td>\n      <td>0.790598</td>\n      <td>0.845494</td>\n      <td>0.798283</td>\n      <td>0.813346</td>\n      <td>0.037811</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.006498</td>\n      <td>0.004509</td>\n      <td>0.000539</td>\n      <td>0.002017</td>\n      <td>random</td>\n      <td>2</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 2, ...</td>\n      <td>0.743590</td>\n      <td>0.846154</td>\n      <td>0.884615</td>\n      <td>0.768240</td>\n      <td>0.828326</td>\n      <td>0.807692</td>\n      <td>0.846154</td>\n      <td>0.794872</td>\n      <td>0.819742</td>\n      <td>0.802575</td>\n      <td>0.794872</td>\n      <td>0.816239</td>\n      <td>0.837607</td>\n      <td>0.811159</td>\n      <td>0.819742</td>\n      <td>0.814772</td>\n      <td>0.032497</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.017939</td>\n      <td>0.003692</td>\n      <td>0.000135</td>\n      <td>0.000507</td>\n      <td>best</td>\n      <td>3</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 3, 'm...</td>\n      <td>0.769231</td>\n      <td>0.850427</td>\n      <td>0.807692</td>\n      <td>0.828326</td>\n      <td>0.824034</td>\n      <td>0.833333</td>\n      <td>0.803419</td>\n      <td>0.850427</td>\n      <td>0.742489</td>\n      <td>0.815451</td>\n      <td>0.764957</td>\n      <td>0.871795</td>\n      <td>0.794872</td>\n      <td>0.832618</td>\n      <td>0.806867</td>\n      <td>0.813063</td>\n      <td>0.033765</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.007168</td>\n      <td>0.004117</td>\n      <td>0.000541</td>\n      <td>0.002023</td>\n      <td>random</td>\n      <td>3</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 3, ...</td>\n      <td>0.786325</td>\n      <td>0.794872</td>\n      <td>0.820513</td>\n      <td>0.828326</td>\n      <td>0.815451</td>\n      <td>0.829060</td>\n      <td>0.863248</td>\n      <td>0.850427</td>\n      <td>0.776824</td>\n      <td>0.884120</td>\n      <td>0.794872</td>\n      <td>0.816239</td>\n      <td>0.824786</td>\n      <td>0.811159</td>\n      <td>0.815451</td>\n      <td>0.820778</td>\n      <td>0.027650</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.016999</td>\n      <td>0.003684</td>\n      <td>0.000275</td>\n      <td>0.000702</td>\n      <td>best</td>\n      <td>4</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'best', 'min_samples_leaf': 4, 'm...</td>\n      <td>0.760684</td>\n      <td>0.871795</td>\n      <td>0.820513</td>\n      <td>0.824034</td>\n      <td>0.806867</td>\n      <td>0.841880</td>\n      <td>0.811966</td>\n      <td>0.846154</td>\n      <td>0.729614</td>\n      <td>0.815451</td>\n      <td>0.773504</td>\n      <td>0.880342</td>\n      <td>0.769231</td>\n      <td>0.862661</td>\n      <td>0.789700</td>\n      <td>0.813626</td>\n      <td>0.041886</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.003118</td>\n      <td>0.004309</td>\n      <td>0.000538</td>\n      <td>0.002013</td>\n      <td>random</td>\n      <td>4</td>\n      <td>None</td>\n      <td>entropy</td>\n      <td>{'splitter': 'random', 'min_samples_leaf': 4, ...</td>\n      <td>0.769231</td>\n      <td>0.794872</td>\n      <td>0.863248</td>\n      <td>0.781116</td>\n      <td>0.806867</td>\n      <td>0.846154</td>\n      <td>0.820513</td>\n      <td>0.816239</td>\n      <td>0.793991</td>\n      <td>0.849785</td>\n      <td>0.739316</td>\n      <td>0.794872</td>\n      <td>0.794872</td>\n      <td>0.845494</td>\n      <td>0.798283</td>\n      <td>0.807657</td>\n      <td>0.032285</td>\n      <td>12</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT_model = DecisionTreeClassifier()\n",
    "\n",
    "DT_params = dict()\n",
    "DT_params['max_depth'] = [3, None]\n",
    "DT_params['min_samples_leaf'] = [1, 2, 3, 4]\n",
    "DT_params['criterion'] = [\"gini\", \"entropy\"]\n",
    "DT_params['splitter'] = [\"best\", \"random\"]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "RCV = RandomizedSearchCV(DT_model, DT_params, n_iter=32, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "DT = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(DT.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[2]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les paramètres testés pour ce modèle sont :\n",
    "-le max_depth, correspondant à la profondeur de l'arbre qui doit être illimité pour garantir une précision maximale du modèle.\n",
    "-le min_samples_leaf, correspondant au nombre d'échantillons minimal par feuille qui doit être de 1.\n",
    "-le criterion est une fonction permettant de mesurer la qualité d'un split, nous obtenons des meilleurs résultats avec 'entropy'.\n",
    "-le splitter est la méthode choisie pour selectionner le split à chaque noeud. Modifier ce paramètre ne semble pas impacter significativement les résultats."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification par boosting de gradient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "642551df",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        0.308470      0.040786         0.000539        0.001152   \n1        1.300610      0.240282         0.001341        0.002407   \n2        5.980673      0.652208         0.005532        0.004006   \n3        0.447546      0.004478         0.001485        0.002704   \n4        1.809340      0.021084         0.002540        0.002706   \n5        9.112545      0.125521         0.007518        0.003388   \n6        0.610575      0.009771         0.001074        0.002739   \n7        2.648563      0.128171         0.002425        0.003307   \n8       13.397420      0.283995         0.013849        0.003415   \n9        0.272154      0.002916         0.000800        0.001601   \n10       1.097921      0.009783         0.002022        0.003126   \n11       5.621758      0.248152         0.008494        0.002681   \n12       0.435682      0.005271         0.000810        0.002058   \n13       1.802385      0.015655         0.001626        0.002685   \n14       9.086724      0.095253         0.009298        0.002065   \n15       0.615474      0.013758         0.000003        0.000009   \n16       2.602337      0.037224         0.003369        0.004017   \n17      11.715912      0.530508         0.010460        0.001444   \n18       0.279137      0.007278         0.000534        0.001360   \n19       1.119136      0.040264         0.003233        0.003514   \n20       2.990290      1.345340         0.003707        0.003615   \n21       0.476734      0.066961         0.002155        0.002805   \n22       1.254965      0.166223         0.002283        0.003513   \n23       2.468417      1.223435         0.003650        0.004384   \n24       0.628991      0.047277         0.001825        0.003183   \n25       1.070145      0.051903         0.001085        0.002075   \n26       1.610570      0.057966         0.001896        0.002998   \n27       0.290363      0.008679         0.001211        0.002732   \n28       1.195566      0.029424         0.001475        0.002377   \n29       6.155422      0.297763         0.003648        0.004386   \n30       0.303961      0.058215         0.001486        0.002706   \n31       0.987141      0.465329         0.001884        0.003177   \n32       4.853859      3.138086         0.004193        0.004646   \n33       0.165441      0.019251         0.001354        0.002728   \n34       0.287946      0.042637         0.000678        0.002052   \n35       0.923908      0.128783         0.002957        0.003449   \n\n   param_n_estimators param_max_depth param_learning_rate  \\\n0                   5               3                0.01   \n1                  20               3                0.01   \n2                 100               3                0.01   \n3                   5               5                0.01   \n4                  20               5                0.01   \n5                 100               5                0.01   \n6                   5               7                0.01   \n7                  20               7                0.01   \n8                 100               7                0.01   \n9                   5               3                 0.1   \n10                 20               3                 0.1   \n11                100               3                 0.1   \n12                  5               5                 0.1   \n13                 20               5                 0.1   \n14                100               5                 0.1   \n15                  5               7                 0.1   \n16                 20               7                 0.1   \n17                100               7                 0.1   \n18                  5               3                   1   \n19                 20               3                   1   \n20                100               3                   1   \n21                  5               5                   1   \n22                 20               5                   1   \n23                100               5                   1   \n24                  5               7                   1   \n25                 20               7                   1   \n26                100               7                   1   \n27                  5               3                  10   \n28                 20               3                  10   \n29                100               3                  10   \n30                  5               5                  10   \n31                 20               5                  10   \n32                100               5                  10   \n33                  5               7                  10   \n34                 20               7                  10   \n35                100               7                  10   \n\n                                               params  split0_test_score  \\\n0   {'n_estimators': 5, 'max_depth': 3, 'learning_...           0.709402   \n1   {'n_estimators': 20, 'max_depth': 3, 'learning...           0.756410   \n2   {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.841880   \n3   {'n_estimators': 5, 'max_depth': 5, 'learning_...           0.747863   \n4   {'n_estimators': 20, 'max_depth': 5, 'learning...           0.782051   \n5   {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.820513   \n6   {'n_estimators': 5, 'max_depth': 7, 'learning_...           0.756410   \n7   {'n_estimators': 20, 'max_depth': 7, 'learning...           0.777778   \n8   {'n_estimators': 100, 'max_depth': 7, 'learnin...           0.786325   \n9   {'n_estimators': 5, 'max_depth': 3, 'learning_...           0.786325   \n10  {'n_estimators': 20, 'max_depth': 3, 'learning...           0.897436   \n11  {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.961538   \n12  {'n_estimators': 5, 'max_depth': 5, 'learning_...           0.807692   \n13  {'n_estimators': 20, 'max_depth': 5, 'learning...           0.837607   \n14  {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.905983   \n15  {'n_estimators': 5, 'max_depth': 7, 'learning_...           0.773504   \n16  {'n_estimators': 20, 'max_depth': 7, 'learning...           0.807692   \n17  {'n_estimators': 100, 'max_depth': 7, 'learnin...           0.833333   \n18  {'n_estimators': 5, 'max_depth': 3, 'learning_...           0.799145   \n19  {'n_estimators': 20, 'max_depth': 3, 'learning...           0.897436   \n20  {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.901709   \n21  {'n_estimators': 5, 'max_depth': 5, 'learning_...           0.837607   \n22  {'n_estimators': 20, 'max_depth': 5, 'learning...           0.910256   \n23  {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.884615   \n24  {'n_estimators': 5, 'max_depth': 7, 'learning_...           0.863248   \n25  {'n_estimators': 20, 'max_depth': 7, 'learning...           0.880342   \n26  {'n_estimators': 100, 'max_depth': 7, 'learnin...           0.884615   \n27  {'n_estimators': 5, 'max_depth': 3, 'learning_...           0.042735   \n28  {'n_estimators': 20, 'max_depth': 3, 'learning...           0.068376   \n29  {'n_estimators': 100, 'max_depth': 3, 'learnin...           0.111111   \n30  {'n_estimators': 5, 'max_depth': 5, 'learning_...           0.222222   \n31  {'n_estimators': 20, 'max_depth': 5, 'learning...           0.282051   \n32  {'n_estimators': 100, 'max_depth': 5, 'learnin...           0.055556   \n33  {'n_estimators': 5, 'max_depth': 7, 'learning_...           0.743590   \n34  {'n_estimators': 20, 'max_depth': 7, 'learning...           0.752137   \n35  {'n_estimators': 100, 'max_depth': 7, 'learnin...           0.760684   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.777778           0.777778           0.785408   \n1            0.837607           0.846154           0.841202   \n2            0.901709           0.901709           0.909871   \n3            0.807692           0.846154           0.836910   \n4            0.833333           0.858974           0.866953   \n5            0.867521           0.914530           0.884120   \n6            0.807692           0.863248           0.811159   \n7            0.824786           0.867521           0.862661   \n8            0.833333           0.880342           0.862661   \n9            0.854701           0.880342           0.875536   \n10           0.931624           0.918803           0.927039   \n11           0.952991           0.944444           0.961373   \n12           0.854701           0.884615           0.866953   \n13           0.897436           0.918803           0.909871   \n14           0.948718           0.940171           0.935622   \n15           0.803419           0.854701           0.849785   \n16           0.837607           0.888889           0.858369   \n17           0.897436           0.914530           0.901288   \n18           0.871795           0.858974           0.832618   \n19           0.944444           0.042735           0.931330   \n20           0.952991           0.927350           0.042918   \n21           0.850427           0.923077           0.909871   \n22           0.927350           0.935897           0.952790   \n23           0.931624           0.935897           0.935622   \n24           0.910256           0.905983           0.931330   \n25           0.927350           0.927350           0.931330   \n26           0.927350           0.927350           0.944206   \n27           0.059829           0.055556           0.090129   \n28           0.059829           0.012821           0.098712   \n29           0.059829           0.055556           0.090129   \n30           0.230769           0.811966           0.819742   \n31           0.123932           0.811966           0.124464   \n32           0.158120           0.166667           0.313305   \n33           0.769231           0.794872           0.815451   \n34           0.790598           0.829060           0.836910   \n35           0.769231           0.820513           0.836910   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.789700           0.764957           0.782051   \n1            0.862661           0.824786           0.846154   \n2            0.884120           0.897436           0.935897   \n3            0.858369           0.807692           0.816239   \n4            0.875536           0.833333           0.854701   \n5            0.905579           0.858974           0.905983   \n6            0.866953           0.807692           0.816239   \n7            0.858369           0.816239           0.846154   \n8            0.879828           0.841880           0.858974   \n9            0.866953           0.837607           0.901709   \n10           0.914163           0.914530           0.952991   \n11           0.948498           0.957265           0.978632   \n12           0.884120           0.816239           0.867521   \n13           0.922747           0.888889           0.910256   \n14           0.939914           0.931624           0.940171   \n15           0.849785           0.824786           0.816239   \n16           0.849785           0.841880           0.837607   \n17           0.892704           0.888889           0.905983   \n18           0.884120           0.876068           0.867521   \n19           0.927039           0.944444           0.931624   \n20           0.948498           0.944444           0.940171   \n21           0.875536           0.880342           0.888889   \n22           0.931330           0.931624           0.944444   \n23           0.905579           0.935897           0.961538   \n24           0.918455           0.901709           0.923077   \n25           0.914163           0.931624           0.927350   \n26           0.901288           0.935897           0.940171   \n27           0.072961           0.354701           0.004274   \n28           0.060086           0.294872           0.235043   \n29           0.072961           0.294872           0.004274   \n30           0.789700           0.811966           0.824786   \n31           0.806867           0.803419           0.799145   \n32           0.030043           0.816239           0.790598   \n33           0.789700           0.782051           0.782051   \n34           0.802575           0.782051           0.769231   \n35           0.755365           0.820513           0.764957   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.786325           0.755365           0.798283   \n1            0.833333           0.815451           0.841202   \n2            0.910256           0.862661           0.909871   \n3            0.833333           0.806867           0.824034   \n4            0.850427           0.802575           0.845494   \n5            0.897436           0.845494           0.884120   \n6            0.833333           0.802575           0.828326   \n7            0.841880           0.785408           0.841202   \n8            0.858974           0.836910           0.854077   \n9            0.858974           0.832618           0.871245   \n10           0.918803           0.918455           0.914163   \n11           0.970085           0.939914           0.961373   \n12           0.880342           0.819742           0.845494   \n13           0.918803           0.854077           0.888412   \n14           0.952991           0.896996           0.939914   \n15           0.841880           0.785408           0.854077   \n16           0.854701           0.832618           0.862661   \n17           0.871795           0.854077           0.896996   \n18           0.837607           0.854077           0.849785   \n19           0.927350           0.931330           0.914163   \n20           0.965812           0.939914           0.892704   \n21           0.884615           0.892704           0.884120   \n22           0.935897           0.931330           0.927039   \n23           0.944444           0.935622           0.905579   \n24           0.914530           0.879828           0.909871   \n25           0.918803           0.905579           0.944206   \n26           0.940171           0.884120           0.931330   \n27           0.106838           0.068670           0.137339   \n28           0.025641           0.081545           0.137339   \n29           0.145299           0.081545           0.072961   \n30           0.807692           0.781116           0.802575   \n31           0.820513           0.789700           0.815451   \n32           0.811966           0.772532           0.819742   \n33           0.786325           0.733906           0.793991   \n34           0.807692           0.733906           0.776824   \n35           0.786325           0.746781           0.798283   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.760684            0.756410            0.790598   \n1             0.824786            0.811966            0.867521   \n2             0.876068            0.897436            0.901709   \n3             0.816239            0.790598            0.846154   \n4             0.833333            0.846154            0.858974   \n5             0.876068            0.888889            0.867521   \n6             0.837607            0.807692            0.858974   \n7             0.850427            0.829060            0.858974   \n8             0.858974            0.854701            0.867521   \n9             0.858974            0.871795            0.893162   \n10            0.897436            0.923077            0.940171   \n11            0.940171            0.952991            0.965812   \n12            0.854701            0.876068            0.837607   \n13            0.893162            0.901709            0.910256   \n14            0.927350            0.944444            0.965812   \n15            0.820513            0.841880            0.841880   \n16            0.846154            0.850427            0.858974   \n17            0.884615            0.880342            0.893162   \n18            0.863248            0.871795            0.871795   \n19            0.918803            0.901709            0.910256   \n20            0.944444            0.935897            0.952991   \n21            0.880342            0.846154            0.893162   \n22            0.931624            0.905983            0.935897   \n23            0.940171            0.940171            0.944444   \n24            0.897436            0.893162            0.940171   \n25            0.914530            0.910256            0.944444   \n26            0.918803            0.914530            0.957265   \n27            0.217949            0.192308            0.051282   \n28            0.226496            0.064103            0.064103   \n29            0.222222            0.192308            0.064103   \n30            0.816239            0.470085            0.824786   \n31            0.790598            0.034188            0.816239   \n32            0.820513            0.025641            0.829060   \n33            0.790598            0.777778            0.829060   \n34            0.811966            0.811966            0.829060   \n35            0.782051            0.756410            0.833333   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.755365            0.759657         0.769984        0.021427   \n1             0.798283            0.815451         0.828198        0.026448   \n2             0.884120            0.841202         0.890397        0.025211   \n3             0.819742            0.811159         0.817936        0.025699   \n4             0.845494            0.828326         0.841044        0.023301   \n5             0.888412            0.879828         0.878999        0.023795   \n6             0.828326            0.785408         0.820776        0.028638   \n7             0.824034            0.815451         0.833330        0.025920   \n8             0.862661            0.845494         0.852177        0.021965   \n9             0.841202            0.845494         0.858442        0.027136   \n10            0.931330            0.896996         0.919801        0.015205   \n11            0.957082            0.944206         0.955759        0.010812   \n12            0.849785            0.854077         0.853311        0.023766   \n13            0.901288            0.892704         0.896401        0.022670   \n14            0.952790            0.944206         0.937780        0.016920   \n15            0.849785            0.862661         0.831354        0.025846   \n16            0.841202            0.871245         0.849321        0.017977   \n17            0.875536            0.909871         0.886704        0.020823   \n18            0.849785            0.815451         0.853586        0.022937   \n19            0.927039            0.922747         0.864830        0.220100   \n20            0.944206            0.944206         0.878551        0.224075   \n21            0.879828            0.866953         0.879575        0.021916   \n22            0.927039            0.931330         0.930656        0.011038   \n23            0.927039            0.914163         0.929494        0.018731   \n24            0.905579            0.914163         0.907253        0.018573   \n25            0.918455            0.927039         0.921522        0.015421   \n26            0.909871            0.927039         0.922934        0.020366   \n27            0.004292            0.278970         0.115855        0.098786   \n28            0.077253            0.266094         0.118154        0.088327   \n29            0.004292            0.266094         0.115837        0.086616   \n30            0.802575            0.845494         0.710781        0.208497   \n31            0.811159            0.815451         0.629676        0.298294   \n32            0.806867            0.815451         0.535487        0.341598   \n33            0.815451            0.798283         0.786822        0.024221   \n34            0.785408            0.785408         0.793653        0.028022   \n35            0.785408            0.776824         0.786239        0.028498   \n\n    rank_test_score  \n0                30  \n1                24  \n2                10  \n3                26  \n4                21  \n5                13  \n6                25  \n7                22  \n8                19  \n9                16  \n10                7  \n11                1  \n12               18  \n13                9  \n14                2  \n15               23  \n16               20  \n17               11  \n18               17  \n19               15  \n20               14  \n21               12  \n22                3  \n23                4  \n24                8  \n25                6  \n26                5  \n27               35  \n28               34  \n29               36  \n30               31  \n31               32  \n32               33  \n33               28  \n34               27  \n35               29  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_n_estimators</th>\n      <th>param_max_depth</th>\n      <th>param_learning_rate</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.308470</td>\n      <td>0.040786</td>\n      <td>0.000539</td>\n      <td>0.001152</td>\n      <td>5</td>\n      <td>3</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 5, 'max_depth': 3, 'learning_...</td>\n      <td>0.709402</td>\n      <td>0.777778</td>\n      <td>0.777778</td>\n      <td>0.785408</td>\n      <td>0.789700</td>\n      <td>0.764957</td>\n      <td>0.782051</td>\n      <td>0.786325</td>\n      <td>0.755365</td>\n      <td>0.798283</td>\n      <td>0.760684</td>\n      <td>0.756410</td>\n      <td>0.790598</td>\n      <td>0.755365</td>\n      <td>0.759657</td>\n      <td>0.769984</td>\n      <td>0.021427</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.300610</td>\n      <td>0.240282</td>\n      <td>0.001341</td>\n      <td>0.002407</td>\n      <td>20</td>\n      <td>3</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 20, 'max_depth': 3, 'learning...</td>\n      <td>0.756410</td>\n      <td>0.837607</td>\n      <td>0.846154</td>\n      <td>0.841202</td>\n      <td>0.862661</td>\n      <td>0.824786</td>\n      <td>0.846154</td>\n      <td>0.833333</td>\n      <td>0.815451</td>\n      <td>0.841202</td>\n      <td>0.824786</td>\n      <td>0.811966</td>\n      <td>0.867521</td>\n      <td>0.798283</td>\n      <td>0.815451</td>\n      <td>0.828198</td>\n      <td>0.026448</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.980673</td>\n      <td>0.652208</td>\n      <td>0.005532</td>\n      <td>0.004006</td>\n      <td>100</td>\n      <td>3</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.841880</td>\n      <td>0.901709</td>\n      <td>0.901709</td>\n      <td>0.909871</td>\n      <td>0.884120</td>\n      <td>0.897436</td>\n      <td>0.935897</td>\n      <td>0.910256</td>\n      <td>0.862661</td>\n      <td>0.909871</td>\n      <td>0.876068</td>\n      <td>0.897436</td>\n      <td>0.901709</td>\n      <td>0.884120</td>\n      <td>0.841202</td>\n      <td>0.890397</td>\n      <td>0.025211</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.447546</td>\n      <td>0.004478</td>\n      <td>0.001485</td>\n      <td>0.002704</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 5, 'max_depth': 5, 'learning_...</td>\n      <td>0.747863</td>\n      <td>0.807692</td>\n      <td>0.846154</td>\n      <td>0.836910</td>\n      <td>0.858369</td>\n      <td>0.807692</td>\n      <td>0.816239</td>\n      <td>0.833333</td>\n      <td>0.806867</td>\n      <td>0.824034</td>\n      <td>0.816239</td>\n      <td>0.790598</td>\n      <td>0.846154</td>\n      <td>0.819742</td>\n      <td>0.811159</td>\n      <td>0.817936</td>\n      <td>0.025699</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.809340</td>\n      <td>0.021084</td>\n      <td>0.002540</td>\n      <td>0.002706</td>\n      <td>20</td>\n      <td>5</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 20, 'max_depth': 5, 'learning...</td>\n      <td>0.782051</td>\n      <td>0.833333</td>\n      <td>0.858974</td>\n      <td>0.866953</td>\n      <td>0.875536</td>\n      <td>0.833333</td>\n      <td>0.854701</td>\n      <td>0.850427</td>\n      <td>0.802575</td>\n      <td>0.845494</td>\n      <td>0.833333</td>\n      <td>0.846154</td>\n      <td>0.858974</td>\n      <td>0.845494</td>\n      <td>0.828326</td>\n      <td>0.841044</td>\n      <td>0.023301</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>9.112545</td>\n      <td>0.125521</td>\n      <td>0.007518</td>\n      <td>0.003388</td>\n      <td>100</td>\n      <td>5</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.820513</td>\n      <td>0.867521</td>\n      <td>0.914530</td>\n      <td>0.884120</td>\n      <td>0.905579</td>\n      <td>0.858974</td>\n      <td>0.905983</td>\n      <td>0.897436</td>\n      <td>0.845494</td>\n      <td>0.884120</td>\n      <td>0.876068</td>\n      <td>0.888889</td>\n      <td>0.867521</td>\n      <td>0.888412</td>\n      <td>0.879828</td>\n      <td>0.878999</td>\n      <td>0.023795</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.610575</td>\n      <td>0.009771</td>\n      <td>0.001074</td>\n      <td>0.002739</td>\n      <td>5</td>\n      <td>7</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 5, 'max_depth': 7, 'learning_...</td>\n      <td>0.756410</td>\n      <td>0.807692</td>\n      <td>0.863248</td>\n      <td>0.811159</td>\n      <td>0.866953</td>\n      <td>0.807692</td>\n      <td>0.816239</td>\n      <td>0.833333</td>\n      <td>0.802575</td>\n      <td>0.828326</td>\n      <td>0.837607</td>\n      <td>0.807692</td>\n      <td>0.858974</td>\n      <td>0.828326</td>\n      <td>0.785408</td>\n      <td>0.820776</td>\n      <td>0.028638</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2.648563</td>\n      <td>0.128171</td>\n      <td>0.002425</td>\n      <td>0.003307</td>\n      <td>20</td>\n      <td>7</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 20, 'max_depth': 7, 'learning...</td>\n      <td>0.777778</td>\n      <td>0.824786</td>\n      <td>0.867521</td>\n      <td>0.862661</td>\n      <td>0.858369</td>\n      <td>0.816239</td>\n      <td>0.846154</td>\n      <td>0.841880</td>\n      <td>0.785408</td>\n      <td>0.841202</td>\n      <td>0.850427</td>\n      <td>0.829060</td>\n      <td>0.858974</td>\n      <td>0.824034</td>\n      <td>0.815451</td>\n      <td>0.833330</td>\n      <td>0.025920</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>13.397420</td>\n      <td>0.283995</td>\n      <td>0.013849</td>\n      <td>0.003415</td>\n      <td>100</td>\n      <td>7</td>\n      <td>0.01</td>\n      <td>{'n_estimators': 100, 'max_depth': 7, 'learnin...</td>\n      <td>0.786325</td>\n      <td>0.833333</td>\n      <td>0.880342</td>\n      <td>0.862661</td>\n      <td>0.879828</td>\n      <td>0.841880</td>\n      <td>0.858974</td>\n      <td>0.858974</td>\n      <td>0.836910</td>\n      <td>0.854077</td>\n      <td>0.858974</td>\n      <td>0.854701</td>\n      <td>0.867521</td>\n      <td>0.862661</td>\n      <td>0.845494</td>\n      <td>0.852177</td>\n      <td>0.021965</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.272154</td>\n      <td>0.002916</td>\n      <td>0.000800</td>\n      <td>0.001601</td>\n      <td>5</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 5, 'max_depth': 3, 'learning_...</td>\n      <td>0.786325</td>\n      <td>0.854701</td>\n      <td>0.880342</td>\n      <td>0.875536</td>\n      <td>0.866953</td>\n      <td>0.837607</td>\n      <td>0.901709</td>\n      <td>0.858974</td>\n      <td>0.832618</td>\n      <td>0.871245</td>\n      <td>0.858974</td>\n      <td>0.871795</td>\n      <td>0.893162</td>\n      <td>0.841202</td>\n      <td>0.845494</td>\n      <td>0.858442</td>\n      <td>0.027136</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1.097921</td>\n      <td>0.009783</td>\n      <td>0.002022</td>\n      <td>0.003126</td>\n      <td>20</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 20, 'max_depth': 3, 'learning...</td>\n      <td>0.897436</td>\n      <td>0.931624</td>\n      <td>0.918803</td>\n      <td>0.927039</td>\n      <td>0.914163</td>\n      <td>0.914530</td>\n      <td>0.952991</td>\n      <td>0.918803</td>\n      <td>0.918455</td>\n      <td>0.914163</td>\n      <td>0.897436</td>\n      <td>0.923077</td>\n      <td>0.940171</td>\n      <td>0.931330</td>\n      <td>0.896996</td>\n      <td>0.919801</td>\n      <td>0.015205</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>5.621758</td>\n      <td>0.248152</td>\n      <td>0.008494</td>\n      <td>0.002681</td>\n      <td>100</td>\n      <td>3</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.961538</td>\n      <td>0.952991</td>\n      <td>0.944444</td>\n      <td>0.961373</td>\n      <td>0.948498</td>\n      <td>0.957265</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.939914</td>\n      <td>0.961373</td>\n      <td>0.940171</td>\n      <td>0.952991</td>\n      <td>0.965812</td>\n      <td>0.957082</td>\n      <td>0.944206</td>\n      <td>0.955759</td>\n      <td>0.010812</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.435682</td>\n      <td>0.005271</td>\n      <td>0.000810</td>\n      <td>0.002058</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 5, 'max_depth': 5, 'learning_...</td>\n      <td>0.807692</td>\n      <td>0.854701</td>\n      <td>0.884615</td>\n      <td>0.866953</td>\n      <td>0.884120</td>\n      <td>0.816239</td>\n      <td>0.867521</td>\n      <td>0.880342</td>\n      <td>0.819742</td>\n      <td>0.845494</td>\n      <td>0.854701</td>\n      <td>0.876068</td>\n      <td>0.837607</td>\n      <td>0.849785</td>\n      <td>0.854077</td>\n      <td>0.853311</td>\n      <td>0.023766</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.802385</td>\n      <td>0.015655</td>\n      <td>0.001626</td>\n      <td>0.002685</td>\n      <td>20</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 20, 'max_depth': 5, 'learning...</td>\n      <td>0.837607</td>\n      <td>0.897436</td>\n      <td>0.918803</td>\n      <td>0.909871</td>\n      <td>0.922747</td>\n      <td>0.888889</td>\n      <td>0.910256</td>\n      <td>0.918803</td>\n      <td>0.854077</td>\n      <td>0.888412</td>\n      <td>0.893162</td>\n      <td>0.901709</td>\n      <td>0.910256</td>\n      <td>0.901288</td>\n      <td>0.892704</td>\n      <td>0.896401</td>\n      <td>0.022670</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>9.086724</td>\n      <td>0.095253</td>\n      <td>0.009298</td>\n      <td>0.002065</td>\n      <td>100</td>\n      <td>5</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.905983</td>\n      <td>0.948718</td>\n      <td>0.940171</td>\n      <td>0.935622</td>\n      <td>0.939914</td>\n      <td>0.931624</td>\n      <td>0.940171</td>\n      <td>0.952991</td>\n      <td>0.896996</td>\n      <td>0.939914</td>\n      <td>0.927350</td>\n      <td>0.944444</td>\n      <td>0.965812</td>\n      <td>0.952790</td>\n      <td>0.944206</td>\n      <td>0.937780</td>\n      <td>0.016920</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.615474</td>\n      <td>0.013758</td>\n      <td>0.000003</td>\n      <td>0.000009</td>\n      <td>5</td>\n      <td>7</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 5, 'max_depth': 7, 'learning_...</td>\n      <td>0.773504</td>\n      <td>0.803419</td>\n      <td>0.854701</td>\n      <td>0.849785</td>\n      <td>0.849785</td>\n      <td>0.824786</td>\n      <td>0.816239</td>\n      <td>0.841880</td>\n      <td>0.785408</td>\n      <td>0.854077</td>\n      <td>0.820513</td>\n      <td>0.841880</td>\n      <td>0.841880</td>\n      <td>0.849785</td>\n      <td>0.862661</td>\n      <td>0.831354</td>\n      <td>0.025846</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2.602337</td>\n      <td>0.037224</td>\n      <td>0.003369</td>\n      <td>0.004017</td>\n      <td>20</td>\n      <td>7</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 20, 'max_depth': 7, 'learning...</td>\n      <td>0.807692</td>\n      <td>0.837607</td>\n      <td>0.888889</td>\n      <td>0.858369</td>\n      <td>0.849785</td>\n      <td>0.841880</td>\n      <td>0.837607</td>\n      <td>0.854701</td>\n      <td>0.832618</td>\n      <td>0.862661</td>\n      <td>0.846154</td>\n      <td>0.850427</td>\n      <td>0.858974</td>\n      <td>0.841202</td>\n      <td>0.871245</td>\n      <td>0.849321</td>\n      <td>0.017977</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>11.715912</td>\n      <td>0.530508</td>\n      <td>0.010460</td>\n      <td>0.001444</td>\n      <td>100</td>\n      <td>7</td>\n      <td>0.1</td>\n      <td>{'n_estimators': 100, 'max_depth': 7, 'learnin...</td>\n      <td>0.833333</td>\n      <td>0.897436</td>\n      <td>0.914530</td>\n      <td>0.901288</td>\n      <td>0.892704</td>\n      <td>0.888889</td>\n      <td>0.905983</td>\n      <td>0.871795</td>\n      <td>0.854077</td>\n      <td>0.896996</td>\n      <td>0.884615</td>\n      <td>0.880342</td>\n      <td>0.893162</td>\n      <td>0.875536</td>\n      <td>0.909871</td>\n      <td>0.886704</td>\n      <td>0.020823</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.279137</td>\n      <td>0.007278</td>\n      <td>0.000534</td>\n      <td>0.001360</td>\n      <td>5</td>\n      <td>3</td>\n      <td>1</td>\n      <td>{'n_estimators': 5, 'max_depth': 3, 'learning_...</td>\n      <td>0.799145</td>\n      <td>0.871795</td>\n      <td>0.858974</td>\n      <td>0.832618</td>\n      <td>0.884120</td>\n      <td>0.876068</td>\n      <td>0.867521</td>\n      <td>0.837607</td>\n      <td>0.854077</td>\n      <td>0.849785</td>\n      <td>0.863248</td>\n      <td>0.871795</td>\n      <td>0.871795</td>\n      <td>0.849785</td>\n      <td>0.815451</td>\n      <td>0.853586</td>\n      <td>0.022937</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1.119136</td>\n      <td>0.040264</td>\n      <td>0.003233</td>\n      <td>0.003514</td>\n      <td>20</td>\n      <td>3</td>\n      <td>1</td>\n      <td>{'n_estimators': 20, 'max_depth': 3, 'learning...</td>\n      <td>0.897436</td>\n      <td>0.944444</td>\n      <td>0.042735</td>\n      <td>0.931330</td>\n      <td>0.927039</td>\n      <td>0.944444</td>\n      <td>0.931624</td>\n      <td>0.927350</td>\n      <td>0.931330</td>\n      <td>0.914163</td>\n      <td>0.918803</td>\n      <td>0.901709</td>\n      <td>0.910256</td>\n      <td>0.927039</td>\n      <td>0.922747</td>\n      <td>0.864830</td>\n      <td>0.220100</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2.990290</td>\n      <td>1.345340</td>\n      <td>0.003707</td>\n      <td>0.003615</td>\n      <td>100</td>\n      <td>3</td>\n      <td>1</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.901709</td>\n      <td>0.952991</td>\n      <td>0.927350</td>\n      <td>0.042918</td>\n      <td>0.948498</td>\n      <td>0.944444</td>\n      <td>0.940171</td>\n      <td>0.965812</td>\n      <td>0.939914</td>\n      <td>0.892704</td>\n      <td>0.944444</td>\n      <td>0.935897</td>\n      <td>0.952991</td>\n      <td>0.944206</td>\n      <td>0.944206</td>\n      <td>0.878551</td>\n      <td>0.224075</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.476734</td>\n      <td>0.066961</td>\n      <td>0.002155</td>\n      <td>0.002805</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'n_estimators': 5, 'max_depth': 5, 'learning_...</td>\n      <td>0.837607</td>\n      <td>0.850427</td>\n      <td>0.923077</td>\n      <td>0.909871</td>\n      <td>0.875536</td>\n      <td>0.880342</td>\n      <td>0.888889</td>\n      <td>0.884615</td>\n      <td>0.892704</td>\n      <td>0.884120</td>\n      <td>0.880342</td>\n      <td>0.846154</td>\n      <td>0.893162</td>\n      <td>0.879828</td>\n      <td>0.866953</td>\n      <td>0.879575</td>\n      <td>0.021916</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1.254965</td>\n      <td>0.166223</td>\n      <td>0.002283</td>\n      <td>0.003513</td>\n      <td>20</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'n_estimators': 20, 'max_depth': 5, 'learning...</td>\n      <td>0.910256</td>\n      <td>0.927350</td>\n      <td>0.935897</td>\n      <td>0.952790</td>\n      <td>0.931330</td>\n      <td>0.931624</td>\n      <td>0.944444</td>\n      <td>0.935897</td>\n      <td>0.931330</td>\n      <td>0.927039</td>\n      <td>0.931624</td>\n      <td>0.905983</td>\n      <td>0.935897</td>\n      <td>0.927039</td>\n      <td>0.931330</td>\n      <td>0.930656</td>\n      <td>0.011038</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2.468417</td>\n      <td>1.223435</td>\n      <td>0.003650</td>\n      <td>0.004384</td>\n      <td>100</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.884615</td>\n      <td>0.931624</td>\n      <td>0.935897</td>\n      <td>0.935622</td>\n      <td>0.905579</td>\n      <td>0.935897</td>\n      <td>0.961538</td>\n      <td>0.944444</td>\n      <td>0.935622</td>\n      <td>0.905579</td>\n      <td>0.940171</td>\n      <td>0.940171</td>\n      <td>0.944444</td>\n      <td>0.927039</td>\n      <td>0.914163</td>\n      <td>0.929494</td>\n      <td>0.018731</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.628991</td>\n      <td>0.047277</td>\n      <td>0.001825</td>\n      <td>0.003183</td>\n      <td>5</td>\n      <td>7</td>\n      <td>1</td>\n      <td>{'n_estimators': 5, 'max_depth': 7, 'learning_...</td>\n      <td>0.863248</td>\n      <td>0.910256</td>\n      <td>0.905983</td>\n      <td>0.931330</td>\n      <td>0.918455</td>\n      <td>0.901709</td>\n      <td>0.923077</td>\n      <td>0.914530</td>\n      <td>0.879828</td>\n      <td>0.909871</td>\n      <td>0.897436</td>\n      <td>0.893162</td>\n      <td>0.940171</td>\n      <td>0.905579</td>\n      <td>0.914163</td>\n      <td>0.907253</td>\n      <td>0.018573</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1.070145</td>\n      <td>0.051903</td>\n      <td>0.001085</td>\n      <td>0.002075</td>\n      <td>20</td>\n      <td>7</td>\n      <td>1</td>\n      <td>{'n_estimators': 20, 'max_depth': 7, 'learning...</td>\n      <td>0.880342</td>\n      <td>0.927350</td>\n      <td>0.927350</td>\n      <td>0.931330</td>\n      <td>0.914163</td>\n      <td>0.931624</td>\n      <td>0.927350</td>\n      <td>0.918803</td>\n      <td>0.905579</td>\n      <td>0.944206</td>\n      <td>0.914530</td>\n      <td>0.910256</td>\n      <td>0.944444</td>\n      <td>0.918455</td>\n      <td>0.927039</td>\n      <td>0.921522</td>\n      <td>0.015421</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1.610570</td>\n      <td>0.057966</td>\n      <td>0.001896</td>\n      <td>0.002998</td>\n      <td>100</td>\n      <td>7</td>\n      <td>1</td>\n      <td>{'n_estimators': 100, 'max_depth': 7, 'learnin...</td>\n      <td>0.884615</td>\n      <td>0.927350</td>\n      <td>0.927350</td>\n      <td>0.944206</td>\n      <td>0.901288</td>\n      <td>0.935897</td>\n      <td>0.940171</td>\n      <td>0.940171</td>\n      <td>0.884120</td>\n      <td>0.931330</td>\n      <td>0.918803</td>\n      <td>0.914530</td>\n      <td>0.957265</td>\n      <td>0.909871</td>\n      <td>0.927039</td>\n      <td>0.922934</td>\n      <td>0.020366</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.290363</td>\n      <td>0.008679</td>\n      <td>0.001211</td>\n      <td>0.002732</td>\n      <td>5</td>\n      <td>3</td>\n      <td>10</td>\n      <td>{'n_estimators': 5, 'max_depth': 3, 'learning_...</td>\n      <td>0.042735</td>\n      <td>0.059829</td>\n      <td>0.055556</td>\n      <td>0.090129</td>\n      <td>0.072961</td>\n      <td>0.354701</td>\n      <td>0.004274</td>\n      <td>0.106838</td>\n      <td>0.068670</td>\n      <td>0.137339</td>\n      <td>0.217949</td>\n      <td>0.192308</td>\n      <td>0.051282</td>\n      <td>0.004292</td>\n      <td>0.278970</td>\n      <td>0.115855</td>\n      <td>0.098786</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>1.195566</td>\n      <td>0.029424</td>\n      <td>0.001475</td>\n      <td>0.002377</td>\n      <td>20</td>\n      <td>3</td>\n      <td>10</td>\n      <td>{'n_estimators': 20, 'max_depth': 3, 'learning...</td>\n      <td>0.068376</td>\n      <td>0.059829</td>\n      <td>0.012821</td>\n      <td>0.098712</td>\n      <td>0.060086</td>\n      <td>0.294872</td>\n      <td>0.235043</td>\n      <td>0.025641</td>\n      <td>0.081545</td>\n      <td>0.137339</td>\n      <td>0.226496</td>\n      <td>0.064103</td>\n      <td>0.064103</td>\n      <td>0.077253</td>\n      <td>0.266094</td>\n      <td>0.118154</td>\n      <td>0.088327</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>6.155422</td>\n      <td>0.297763</td>\n      <td>0.003648</td>\n      <td>0.004386</td>\n      <td>100</td>\n      <td>3</td>\n      <td>10</td>\n      <td>{'n_estimators': 100, 'max_depth': 3, 'learnin...</td>\n      <td>0.111111</td>\n      <td>0.059829</td>\n      <td>0.055556</td>\n      <td>0.090129</td>\n      <td>0.072961</td>\n      <td>0.294872</td>\n      <td>0.004274</td>\n      <td>0.145299</td>\n      <td>0.081545</td>\n      <td>0.072961</td>\n      <td>0.222222</td>\n      <td>0.192308</td>\n      <td>0.064103</td>\n      <td>0.004292</td>\n      <td>0.266094</td>\n      <td>0.115837</td>\n      <td>0.086616</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.303961</td>\n      <td>0.058215</td>\n      <td>0.001486</td>\n      <td>0.002706</td>\n      <td>5</td>\n      <td>5</td>\n      <td>10</td>\n      <td>{'n_estimators': 5, 'max_depth': 5, 'learning_...</td>\n      <td>0.222222</td>\n      <td>0.230769</td>\n      <td>0.811966</td>\n      <td>0.819742</td>\n      <td>0.789700</td>\n      <td>0.811966</td>\n      <td>0.824786</td>\n      <td>0.807692</td>\n      <td>0.781116</td>\n      <td>0.802575</td>\n      <td>0.816239</td>\n      <td>0.470085</td>\n      <td>0.824786</td>\n      <td>0.802575</td>\n      <td>0.845494</td>\n      <td>0.710781</td>\n      <td>0.208497</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.987141</td>\n      <td>0.465329</td>\n      <td>0.001884</td>\n      <td>0.003177</td>\n      <td>20</td>\n      <td>5</td>\n      <td>10</td>\n      <td>{'n_estimators': 20, 'max_depth': 5, 'learning...</td>\n      <td>0.282051</td>\n      <td>0.123932</td>\n      <td>0.811966</td>\n      <td>0.124464</td>\n      <td>0.806867</td>\n      <td>0.803419</td>\n      <td>0.799145</td>\n      <td>0.820513</td>\n      <td>0.789700</td>\n      <td>0.815451</td>\n      <td>0.790598</td>\n      <td>0.034188</td>\n      <td>0.816239</td>\n      <td>0.811159</td>\n      <td>0.815451</td>\n      <td>0.629676</td>\n      <td>0.298294</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>4.853859</td>\n      <td>3.138086</td>\n      <td>0.004193</td>\n      <td>0.004646</td>\n      <td>100</td>\n      <td>5</td>\n      <td>10</td>\n      <td>{'n_estimators': 100, 'max_depth': 5, 'learnin...</td>\n      <td>0.055556</td>\n      <td>0.158120</td>\n      <td>0.166667</td>\n      <td>0.313305</td>\n      <td>0.030043</td>\n      <td>0.816239</td>\n      <td>0.790598</td>\n      <td>0.811966</td>\n      <td>0.772532</td>\n      <td>0.819742</td>\n      <td>0.820513</td>\n      <td>0.025641</td>\n      <td>0.829060</td>\n      <td>0.806867</td>\n      <td>0.815451</td>\n      <td>0.535487</td>\n      <td>0.341598</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.165441</td>\n      <td>0.019251</td>\n      <td>0.001354</td>\n      <td>0.002728</td>\n      <td>5</td>\n      <td>7</td>\n      <td>10</td>\n      <td>{'n_estimators': 5, 'max_depth': 7, 'learning_...</td>\n      <td>0.743590</td>\n      <td>0.769231</td>\n      <td>0.794872</td>\n      <td>0.815451</td>\n      <td>0.789700</td>\n      <td>0.782051</td>\n      <td>0.782051</td>\n      <td>0.786325</td>\n      <td>0.733906</td>\n      <td>0.793991</td>\n      <td>0.790598</td>\n      <td>0.777778</td>\n      <td>0.829060</td>\n      <td>0.815451</td>\n      <td>0.798283</td>\n      <td>0.786822</td>\n      <td>0.024221</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.287946</td>\n      <td>0.042637</td>\n      <td>0.000678</td>\n      <td>0.002052</td>\n      <td>20</td>\n      <td>7</td>\n      <td>10</td>\n      <td>{'n_estimators': 20, 'max_depth': 7, 'learning...</td>\n      <td>0.752137</td>\n      <td>0.790598</td>\n      <td>0.829060</td>\n      <td>0.836910</td>\n      <td>0.802575</td>\n      <td>0.782051</td>\n      <td>0.769231</td>\n      <td>0.807692</td>\n      <td>0.733906</td>\n      <td>0.776824</td>\n      <td>0.811966</td>\n      <td>0.811966</td>\n      <td>0.829060</td>\n      <td>0.785408</td>\n      <td>0.785408</td>\n      <td>0.793653</td>\n      <td>0.028022</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.923908</td>\n      <td>0.128783</td>\n      <td>0.002957</td>\n      <td>0.003449</td>\n      <td>100</td>\n      <td>7</td>\n      <td>10</td>\n      <td>{'n_estimators': 100, 'max_depth': 7, 'learnin...</td>\n      <td>0.760684</td>\n      <td>0.769231</td>\n      <td>0.820513</td>\n      <td>0.836910</td>\n      <td>0.755365</td>\n      <td>0.820513</td>\n      <td>0.764957</td>\n      <td>0.786325</td>\n      <td>0.746781</td>\n      <td>0.798283</td>\n      <td>0.782051</td>\n      <td>0.756410</td>\n      <td>0.833333</td>\n      <td>0.785408</td>\n      <td>0.776824</td>\n      <td>0.786239</td>\n      <td>0.028498</td>\n      <td>29</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "GB_model = GradientBoostingClassifier()\n",
    "GB_params = dict()\n",
    "GB_params['n_estimators'] = [5, 20, 100]\n",
    "GB_params['max_depth'] = [3, 5, 7]\n",
    "GB_params['learning_rate'] = [0.01, 0.1, 1, 10]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(GB_model, GB_params, n_iter=36, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "GB = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(GB.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[3]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification par voisin le plus proche"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        0.010582      0.016659         0.029114        0.010256   \n1        0.013095      0.005380         0.019936        0.003961   \n2        0.016431      0.012549         0.021038        0.002051   \n3        0.023009      0.003866         0.054271        0.006044   \n4        0.023631      0.003800         0.063375        0.003246   \n5        0.011969      0.003165         0.022104        0.003717   \n6        0.006074      0.004188         0.019416        0.002293   \n7        0.007269      0.003274         0.012586        0.005154   \n8        0.000607      0.002013         0.015324        0.004237   \n9        0.000952      0.001830         0.017695        0.002880   \n10       0.011469      0.002737         0.034259        0.003786   \n11       0.008503      0.002682         0.019297        0.004362   \n12       0.014564      0.004457         0.044748        0.004437   \n13       0.001888      0.003168         0.014433        0.004539   \n14       0.018290      0.003621         0.026335        0.003570   \n15       0.008483      0.002246         0.020169        0.005083   \n16       0.028934      0.003181         0.194576        0.006286   \n17       0.001215      0.002742         0.013829        0.003467   \n18       0.008293      0.002266         0.018221        0.003178   \n19       0.017775      0.003620         0.022615        0.003620   \n20       0.006201      0.004324         0.016690        0.004195   \n21       0.008350      0.003193         0.016694        0.005341   \n22       0.016566      0.003837         0.026795        0.004050   \n23       0.015629      0.003933         0.015245        0.003822   \n24       0.001754      0.003206         0.007036        0.003893   \n25       0.012548      0.003619         0.025083        0.004165   \n26       0.000677      0.002049         0.007847        0.003032   \n27       0.005929      0.004390         0.021825        0.003054   \n28       0.009416      0.000964         0.024266        0.004152   \n29       0.008217      0.002900         0.017508        0.004985   \n30       0.000410      0.000820         0.026853        0.008783   \n31       0.001891      0.003807         0.016334        0.006071   \n32       0.008769      0.003102         0.022125        0.003805   \n33       0.009592      0.002014         0.016699        0.003643   \n34       0.022192      0.016156         0.046045        0.022245   \n35       0.018202      0.003777         0.028999        0.003133   \n36       0.000137      0.000513         0.022249        0.004044   \n37       0.021766      0.017920         0.036914        0.017205   \n38       0.009202      0.002073         0.025025        0.004361   \n39       0.016096      0.006301         0.045065        0.014773   \n40       0.001900      0.003528         0.006909        0.005729   \n41       0.008667      0.004586         0.018095        0.006720   \n42       0.006627      0.003939         0.028805        0.008777   \n43       0.003649      0.005844         0.018539        0.007179   \n44       0.000285      0.000701         0.009491        0.002049   \n45       0.002158      0.003578         0.006375        0.004440   \n46       0.028219      0.007803         0.159026        0.025515   \n47       0.026561      0.004729         0.094954        0.011838   \n48       0.000143      0.000514         0.013123        0.005524   \n49       0.000814      0.002066         0.014420        0.004176   \n\n   param_weights param_p param_n_neighbors param_leaf_size param_algorithm  \\\n0        uniform       2                16              30         kd_tree   \n1       distance       2                11              30         kd_tree   \n2        uniform       1                 6              30         kd_tree   \n3        uniform       1                 1               1         kd_tree   \n4       distance       1                 6               1         kd_tree   \n5       distance       1                 6               1       ball_tree   \n6        uniform       2                11              30       ball_tree   \n7       distance       1                 1              30       ball_tree   \n8        uniform       1                 1              10           brute   \n9        uniform       1                16               1           brute   \n10       uniform       2                16               1       ball_tree   \n11       uniform       1                 1              10       ball_tree   \n12       uniform       2                11              10         kd_tree   \n13       uniform       1                 1              30           brute   \n14      distance       1                 6              10         kd_tree   \n15      distance       1                16              10       ball_tree   \n16       uniform       2                11               1         kd_tree   \n17       uniform       2                11              30           brute   \n18      distance       2                16              10       ball_tree   \n19      distance       1                 1              10         kd_tree   \n20      distance       1                16              30       ball_tree   \n21      distance       2                 1              10       ball_tree   \n22      distance       1                16              10         kd_tree   \n23      distance       2                 1              30         kd_tree   \n24      distance       2                 6              10           brute   \n25       uniform       2                11              30         kd_tree   \n26      distance       2                11              10           brute   \n27       uniform       1                16              30       ball_tree   \n28       uniform       1                16              10       ball_tree   \n29      distance       2                 1              30       ball_tree   \n30       uniform       1                 6               1           brute   \n31      distance       2                16               1           brute   \n32       uniform       1                11              30       ball_tree   \n33      distance       1                 1              10       ball_tree   \n34       uniform       2                 1              10         kd_tree   \n35      distance       1                11              10         kd_tree   \n36       uniform       1                11               1           brute   \n37      distance       1                16              30         kd_tree   \n38       uniform       2                 1              10       ball_tree   \n39       uniform       1                 6               1       ball_tree   \n40      distance       2                 1              30           brute   \n41      distance       2                 6              30       ball_tree   \n42       uniform       1                 6              30       ball_tree   \n43      distance       1                16               1           brute   \n44      distance       2                11              30           brute   \n45      distance       2                 1               1           brute   \n46       uniform       2                 6               1         kd_tree   \n47      distance       2                 1               1         kd_tree   \n48       uniform       2                 1               1           brute   \n49      distance       1                 6              10           brute   \n\n                                               params  split0_test_score  \\\n0   {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.944444   \n1   {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n2   {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.982906   \n3   {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n4   {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.987179   \n5   {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.987179   \n6   {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n7   {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n8   {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n9   {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.948718   \n10  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.940171   \n11  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n12  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n13  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n14  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.982906   \n15  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.961538   \n16  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n17  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n18  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.957265   \n19  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n20  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.961538   \n21  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n22  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.961538   \n23  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n24  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.982906   \n25  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.961538   \n26  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n27  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.948718   \n28  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.944444   \n29  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n30  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.978632   \n31  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.957265   \n32  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.965812   \n33  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.978632   \n34  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.987179   \n35  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.970085   \n36  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.965812   \n37  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.961538   \n38  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.987179   \n39  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.982906   \n40  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n41  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.982906   \n42  {'weights': 'uniform', 'p': 1, 'n_neighbors': ...           0.982906   \n43  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.961538   \n44  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.970085   \n45  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n46  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.982906   \n47  {'weights': 'distance', 'p': 2, 'n_neighbors':...           0.987179   \n48  {'weights': 'uniform', 'p': 2, 'n_neighbors': ...           0.987179   \n49  {'weights': 'distance', 'p': 1, 'n_neighbors':...           0.982906   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.970085           0.965812           0.969957   \n1            0.982906           0.965812           0.978541   \n2            0.974359           0.965812           0.969957   \n3            0.978632           0.970085           0.987124   \n4            0.970085           0.974359           0.982833   \n5            0.970085           0.974359           0.982833   \n6            0.978632           0.965812           0.974249   \n7            0.978632           0.970085           0.987124   \n8            0.978632           0.970085           0.987124   \n9            0.961538           0.970085           0.969957   \n10           0.970085           0.965812           0.969957   \n11           0.978632           0.970085           0.987124   \n12           0.978632           0.965812           0.974249   \n13           0.978632           0.970085           0.987124   \n14           0.970085           0.970085           0.982833   \n15           0.961538           0.970085           0.974249   \n16           0.978632           0.965812           0.974249   \n17           0.978632           0.965812           0.974249   \n18           0.970085           0.970085           0.974249   \n19           0.978632           0.970085           0.987124   \n20           0.961538           0.970085           0.974249   \n21           0.987179           0.978632           0.978541   \n22           0.961538           0.970085           0.974249   \n23           0.987179           0.978632           0.978541   \n24           0.991453           0.970085           0.974249   \n25           0.978632           0.965812           0.974249   \n26           0.982906           0.965812           0.978541   \n27           0.961538           0.970085           0.969957   \n28           0.961538           0.970085           0.974249   \n29           0.987179           0.978632           0.978541   \n30           0.974359           0.965812           0.969957   \n31           0.970085           0.970085           0.974249   \n32           0.974359           0.957265           0.965665   \n33           0.978632           0.970085           0.987124   \n34           0.987179           0.978632           0.978541   \n35           0.974359           0.965812           0.974249   \n36           0.970085           0.957265           0.965665   \n37           0.961538           0.970085           0.974249   \n38           0.987179           0.978632           0.978541   \n39           0.974359           0.965812           0.969957   \n40           0.987179           0.978632           0.978541   \n41           0.991453           0.970085           0.974249   \n42           0.974359           0.965812           0.969957   \n43           0.961538           0.970085           0.974249   \n44           0.982906           0.965812           0.978541   \n45           0.987179           0.978632           0.978541   \n46           0.991453           0.970085           0.965665   \n47           0.987179           0.978632           0.978541   \n48           0.987179           0.978632           0.978541   \n49           0.970085           0.974359           0.982833   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.957082           0.970085           0.974359   \n1            0.969957           0.974359           0.982906   \n2            0.982833           0.965812           0.987179   \n3            0.982833           0.974359           0.982906   \n4            0.982833           0.982906           0.991453   \n5            0.982833           0.982906           0.991453   \n6            0.965665           0.965812           0.978632   \n7            0.982833           0.974359           0.982906   \n8            0.982833           0.974359           0.982906   \n9            0.957082           0.961538           0.970085   \n10           0.957082           0.970085           0.974359   \n11           0.982833           0.974359           0.982906   \n12           0.965665           0.965812           0.978632   \n13           0.982833           0.974359           0.982906   \n14           0.982833           0.982906           0.995726   \n15           0.965665           0.974359           0.978632   \n16           0.965665           0.965812           0.978632   \n17           0.965665           0.965812           0.978632   \n18           0.957082           0.978632           0.978632   \n19           0.982833           0.974359           0.982906   \n20           0.965665           0.974359           0.978632   \n21           0.991416           0.978632           1.000000   \n22           0.965665           0.974359           0.978632   \n23           0.991416           0.978632           1.000000   \n24           0.995708           0.982906           1.000000   \n25           0.965665           0.965812           0.978632   \n26           0.969957           0.974359           0.982906   \n27           0.957082           0.961538           0.970085   \n28           0.957082           0.961538           0.970085   \n29           0.991416           0.978632           1.000000   \n30           0.978541           0.965812           0.982906   \n31           0.957082           0.978632           0.978632   \n32           0.952790           0.961538           0.974359   \n33           0.982833           0.974359           0.982906   \n34           0.991416           0.978632           1.000000   \n35           0.965665           0.957265           0.982906   \n36           0.957082           0.961538           0.974359   \n37           0.965665           0.974359           0.978632   \n38           0.991416           0.978632           1.000000   \n39           0.978541           0.965812           0.982906   \n40           0.991416           0.978632           1.000000   \n41           0.995708           0.982906           1.000000   \n42           0.982833           0.965812           0.982906   \n43           0.965665           0.974359           0.978632   \n44           0.969957           0.974359           0.982906   \n45           0.991416           0.978632           1.000000   \n46           0.995708           0.978632           0.995726   \n47           0.991416           0.978632           1.000000   \n48           0.991416           0.978632           1.000000   \n49           0.982833           0.982906           0.991453   \n\n    split7_test_score  split8_test_score  split9_test_score  \\\n0            0.952991           0.957082           0.965665   \n1            0.974359           0.965665           0.982833   \n2            0.970085           0.948498           0.982833   \n3            0.991453           0.965665           0.982833   \n4            0.991453           0.957082           0.991416   \n5            0.991453           0.957082           0.991416   \n6            0.974359           0.961373           0.974249   \n7            0.991453           0.965665           0.982833   \n8            0.995726           0.965665           0.982833   \n9            0.944444           0.952790           0.969957   \n10           0.952991           0.957082           0.965665   \n11           0.991453           0.965665           0.982833   \n12           0.974359           0.961373           0.974249   \n13           0.995726           0.965665           0.982833   \n14           0.991453           0.957082           0.991416   \n15           0.961538           0.961373           0.974249   \n16           0.974359           0.961373           0.974249   \n17           0.974359           0.965665           0.974249   \n18           0.974359           0.961373           0.969957   \n19           0.991453           0.965665           0.982833   \n20           0.965812           0.961373           0.974249   \n21           1.000000           0.978541           0.987124   \n22           0.961538           0.961373           0.978541   \n23           1.000000           0.978541           0.987124   \n24           0.995726           0.969957           0.978541   \n25           0.974359           0.961373           0.974249   \n26           0.974359           0.969957           0.982833   \n27           0.948718           0.952790           0.969957   \n28           0.948718           0.952790           0.969957   \n29           1.000000           0.978541           0.987124   \n30           0.974359           0.948498           0.982833   \n31           0.974359           0.961373           0.969957   \n32           0.961538           0.957082           0.978541   \n33           0.991453           0.965665           0.982833   \n34           1.000000           0.978541           0.987124   \n35           0.961538           0.961373           0.978541   \n36           0.961538           0.961373           0.978541   \n37           0.961538           0.961373           0.974249   \n38           1.000000           0.978541           0.987124   \n39           0.974359           0.948498           0.982833   \n40           1.000000           0.978541           0.987124   \n41           0.995726           0.969957           0.978541   \n42           0.974359           0.948498           0.982833   \n43           0.961538           0.961373           0.978541   \n44           0.974359           0.969957           0.982833   \n45           1.000000           0.978541           0.987124   \n46           0.974359           0.969957           0.978541   \n47           1.000000           0.978541           0.987124   \n48           1.000000           0.978541           0.987124   \n49           0.991453           0.957082           0.991416   \n\n    split10_test_score  split11_test_score  split12_test_score  \\\n0             0.965812            0.970085            0.965812   \n1             0.982906            0.974359            0.982906   \n2             0.961538            0.961538            0.974359   \n3             0.974359            0.974359            0.974359   \n4             0.965812            0.974359            0.978632   \n5             0.965812            0.974359            0.978632   \n6             0.965812            0.974359            0.978632   \n7             0.974359            0.974359            0.974359   \n8             0.974359            0.974359            0.974359   \n9             0.948718            0.965812            0.961538   \n10            0.965812            0.970085            0.965812   \n11            0.974359            0.974359            0.974359   \n12            0.965812            0.974359            0.978632   \n13            0.974359            0.974359            0.974359   \n14            0.965812            0.974359            0.978632   \n15            0.961538            0.965812            0.974359   \n16            0.965812            0.974359            0.978632   \n17            0.965812            0.974359            0.982906   \n18            0.974359            0.974359            0.974359   \n19            0.974359            0.974359            0.974359   \n20            0.961538            0.965812            0.974359   \n21            0.987179            0.987179            1.000000   \n22            0.961538            0.965812            0.974359   \n23            0.987179            0.987179            1.000000   \n24            0.991453            0.982906            0.991453   \n25            0.965812            0.974359            0.978632   \n26            0.982906            0.974359            0.987179   \n27            0.948718            0.965812            0.961538   \n28            0.948718            0.965812            0.965812   \n29            0.987179            0.987179            1.000000   \n30            0.965812            0.961538            0.974359   \n31            0.974359            0.974359            0.974359   \n32            0.957265            0.961538            0.965812   \n33            0.974359            0.974359            0.974359   \n34            0.987179            0.987179            1.000000   \n35            0.970085            0.965812            0.978632   \n36            0.957265            0.961538            0.965812   \n37            0.961538            0.965812            0.974359   \n38            0.987179            0.987179            1.000000   \n39            0.961538            0.961538            0.974359   \n40            0.987179            0.987179            1.000000   \n41            0.991453            0.982906            0.991453   \n42            0.965812            0.961538            0.974359   \n43            0.961538            0.965812            0.974359   \n44            0.982906            0.974359            0.987179   \n45            0.987179            0.987179            1.000000   \n46            0.978632            0.978632            0.978632   \n47            0.987179            0.987179            1.000000   \n48            0.987179            0.987179            1.000000   \n49            0.965812            0.974359            0.978632   \n\n    split13_test_score  split14_test_score  mean_test_score  std_test_score  \\\n0             0.974249            0.961373         0.964326        0.008067   \n1             0.978541            0.978541         0.976312        0.006031   \n2             0.978541            0.974249         0.972033        0.009996   \n3             0.978541            0.987124         0.978884        0.006710   \n4             0.982833            0.987124         0.980024        0.009722   \n5             0.982833            0.987124         0.980024        0.009722   \n6             0.974249            0.969957         0.970889        0.005895   \n7             0.978541            0.987124         0.978884        0.006710   \n8             0.978541            0.987124         0.979169        0.007303   \n9             0.969957            0.957082         0.960620        0.008569   \n10            0.974249            0.961373         0.964041        0.008806   \n11            0.978541            0.987124         0.978884        0.006710   \n12            0.974249            0.969957         0.970889        0.005895   \n13            0.978541            0.987124         0.979169        0.007303   \n14            0.982833            0.987124         0.979739        0.010186   \n15            0.974249            0.961373         0.968037        0.006221   \n16            0.974249            0.969957         0.970889        0.005895   \n17            0.974249            0.969957         0.971460        0.005987   \n18            0.978541            0.969957         0.970886        0.006869   \n19            0.978541            0.987124         0.978884        0.006710   \n20            0.974249            0.961373         0.968322        0.006011   \n21            0.991416            0.991416         0.988296        0.007410   \n22            0.974249            0.961373         0.968323        0.006588   \n23            0.991416            0.991416         0.988296        0.007410   \n24            0.982833            0.991416         0.985440        0.009099   \n25            0.974249            0.969957         0.970889        0.005895   \n26            0.978541            0.978541         0.976883        0.006013   \n27            0.969957            0.957082         0.960905        0.008084   \n28            0.969957            0.952790         0.960905        0.009215   \n29            0.991416            0.991416         0.988296        0.007410   \n30            0.982833            0.974249         0.972033        0.009103   \n31            0.978541            0.969957         0.970886        0.006869   \n32            0.974249            0.969957         0.965185        0.007471   \n33            0.978541            0.987124         0.978884        0.006710   \n34            0.991416            0.991416         0.988296        0.007410   \n35            0.974249            0.969957         0.970035        0.006982   \n36            0.974249            0.969957         0.965472        0.006516   \n37            0.974249            0.961373         0.968037        0.006221   \n38            0.991416            0.991416         0.988296        0.007410   \n39            0.978541            0.974249         0.971747        0.009358   \n40            0.991416            0.991416         0.988296        0.007410   \n41            0.982833            0.991416         0.985440        0.009099   \n42            0.974249            0.974249         0.972032        0.009238   \n43            0.974249            0.961373         0.968323        0.006588   \n44            0.978541            0.978541         0.976883        0.006013   \n45            0.991416            0.991416         0.988296        0.007410   \n46            0.974249            0.978541         0.979448        0.008604   \n47            0.991416            0.991416         0.988296        0.007410   \n48            0.991416            0.991416         0.988296        0.007410   \n49            0.982833            0.987124         0.979739        0.009570   \n\n    rank_test_score  \n0                46  \n1                26  \n2                28  \n3                19  \n4                12  \n5                12  \n6                32  \n7                19  \n8                17  \n9                50  \n10               47  \n11               19  \n12               32  \n13               17  \n14               14  \n15               42  \n16               32  \n17               31  \n18               36  \n19               19  \n20               41  \n21                1  \n22               39  \n23                1  \n24               10  \n25               32  \n26               24  \n27               48  \n28               49  \n29                1  \n30               27  \n31               36  \n32               45  \n33               19  \n34                1  \n35               38  \n36               44  \n37               42  \n38                1  \n39               30  \n40                1  \n41               10  \n42               29  \n43               39  \n44               24  \n45                1  \n46               16  \n47                1  \n48                1  \n49               14  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_weights</th>\n      <th>param_p</th>\n      <th>param_n_neighbors</th>\n      <th>param_leaf_size</th>\n      <th>param_algorithm</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>split10_test_score</th>\n      <th>split11_test_score</th>\n      <th>split12_test_score</th>\n      <th>split13_test_score</th>\n      <th>split14_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.010582</td>\n      <td>0.016659</td>\n      <td>0.029114</td>\n      <td>0.010256</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>16</td>\n      <td>30</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.944444</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964326</td>\n      <td>0.008067</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.013095</td>\n      <td>0.005380</td>\n      <td>0.019936</td>\n      <td>0.003961</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>30</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976312</td>\n      <td>0.006031</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.016431</td>\n      <td>0.012549</td>\n      <td>0.021038</td>\n      <td>0.002051</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>30</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.974249</td>\n      <td>0.972033</td>\n      <td>0.009996</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.023009</td>\n      <td>0.003866</td>\n      <td>0.054271</td>\n      <td>0.006044</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.023631</td>\n      <td>0.003800</td>\n      <td>0.063375</td>\n      <td>0.003246</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.991453</td>\n      <td>0.957082</td>\n      <td>0.991416</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.987124</td>\n      <td>0.980024</td>\n      <td>0.009722</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.011969</td>\n      <td>0.003165</td>\n      <td>0.022104</td>\n      <td>0.003717</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.991453</td>\n      <td>0.957082</td>\n      <td>0.991416</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.987124</td>\n      <td>0.980024</td>\n      <td>0.009722</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.006074</td>\n      <td>0.004188</td>\n      <td>0.019416</td>\n      <td>0.002293</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.970889</td>\n      <td>0.005895</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.007269</td>\n      <td>0.003274</td>\n      <td>0.012586</td>\n      <td>0.005154</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.000607</td>\n      <td>0.002013</td>\n      <td>0.015324</td>\n      <td>0.004237</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.979169</td>\n      <td>0.007303</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.000952</td>\n      <td>0.001830</td>\n      <td>0.017695</td>\n      <td>0.002880</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.944444</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960620</td>\n      <td>0.008569</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.011469</td>\n      <td>0.002737</td>\n      <td>0.034259</td>\n      <td>0.003786</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>16</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.940171</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.952991</td>\n      <td>0.957082</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.964041</td>\n      <td>0.008806</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.008503</td>\n      <td>0.002682</td>\n      <td>0.019297</td>\n      <td>0.004362</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.014564</td>\n      <td>0.004457</td>\n      <td>0.044748</td>\n      <td>0.004437</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.970889</td>\n      <td>0.005895</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.001888</td>\n      <td>0.003168</td>\n      <td>0.014433</td>\n      <td>0.004539</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>1</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.979169</td>\n      <td>0.007303</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.018290</td>\n      <td>0.003621</td>\n      <td>0.026335</td>\n      <td>0.003570</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>6</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.982906</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.995726</td>\n      <td>0.991453</td>\n      <td>0.957082</td>\n      <td>0.991416</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.987124</td>\n      <td>0.979739</td>\n      <td>0.010186</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.008483</td>\n      <td>0.002246</td>\n      <td>0.020169</td>\n      <td>0.005083</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>16</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.968037</td>\n      <td>0.006221</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.028934</td>\n      <td>0.003181</td>\n      <td>0.194576</td>\n      <td>0.006286</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.970889</td>\n      <td>0.005895</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.001215</td>\n      <td>0.002742</td>\n      <td>0.013829</td>\n      <td>0.003467</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965665</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.971460</td>\n      <td>0.005987</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.008293</td>\n      <td>0.002266</td>\n      <td>0.018221</td>\n      <td>0.003178</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>16</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.970886</td>\n      <td>0.006869</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.017775</td>\n      <td>0.003620</td>\n      <td>0.022615</td>\n      <td>0.003620</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.006201</td>\n      <td>0.004324</td>\n      <td>0.016690</td>\n      <td>0.004195</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>16</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.968322</td>\n      <td>0.006011</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.008350</td>\n      <td>0.003193</td>\n      <td>0.016694</td>\n      <td>0.005341</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.016566</td>\n      <td>0.003837</td>\n      <td>0.026795</td>\n      <td>0.004050</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>16</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.978541</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.968323</td>\n      <td>0.006588</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.015629</td>\n      <td>0.003933</td>\n      <td>0.015245</td>\n      <td>0.003822</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>30</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.001754</td>\n      <td>0.003206</td>\n      <td>0.007036</td>\n      <td>0.003893</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>6</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.995708</td>\n      <td>0.982906</td>\n      <td>1.000000</td>\n      <td>0.995726</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.991453</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.982833</td>\n      <td>0.991416</td>\n      <td>0.985440</td>\n      <td>0.009099</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.012548</td>\n      <td>0.003619</td>\n      <td>0.025083</td>\n      <td>0.004165</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>11</td>\n      <td>30</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.961538</td>\n      <td>0.978632</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.970889</td>\n      <td>0.005895</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.000677</td>\n      <td>0.002049</td>\n      <td>0.007847</td>\n      <td>0.003032</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976883</td>\n      <td>0.006013</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.005929</td>\n      <td>0.004390</td>\n      <td>0.021825</td>\n      <td>0.003054</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.948718</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.948718</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.969957</td>\n      <td>0.957082</td>\n      <td>0.960905</td>\n      <td>0.008084</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.009416</td>\n      <td>0.000964</td>\n      <td>0.024266</td>\n      <td>0.004152</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>16</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.944444</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.948718</td>\n      <td>0.952790</td>\n      <td>0.969957</td>\n      <td>0.948718</td>\n      <td>0.965812</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.952790</td>\n      <td>0.960905</td>\n      <td>0.009215</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.008217</td>\n      <td>0.002900</td>\n      <td>0.017508</td>\n      <td>0.004985</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.000410</td>\n      <td>0.000820</td>\n      <td>0.026853</td>\n      <td>0.008783</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.974249</td>\n      <td>0.972033</td>\n      <td>0.009103</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.001891</td>\n      <td>0.003807</td>\n      <td>0.016334</td>\n      <td>0.006071</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>16</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.957265</td>\n      <td>0.970085</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.957082</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974359</td>\n      <td>0.961373</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.970886</td>\n      <td>0.006869</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.008769</td>\n      <td>0.003102</td>\n      <td>0.022125</td>\n      <td>0.003805</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>11</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.952790</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.961538</td>\n      <td>0.957082</td>\n      <td>0.978541</td>\n      <td>0.957265</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.965185</td>\n      <td>0.007471</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.009592</td>\n      <td>0.002014</td>\n      <td>0.016699</td>\n      <td>0.003643</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.970085</td>\n      <td>0.987124</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.965665</td>\n      <td>0.982833</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.978884</td>\n      <td>0.006710</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.022192</td>\n      <td>0.016156</td>\n      <td>0.046045</td>\n      <td>0.022245</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>1</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.018202</td>\n      <td>0.003777</td>\n      <td>0.028999</td>\n      <td>0.003133</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>11</td>\n      <td>10</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.957265</td>\n      <td>0.982906</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.978541</td>\n      <td>0.970085</td>\n      <td>0.965812</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.970035</td>\n      <td>0.006982</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.000137</td>\n      <td>0.000513</td>\n      <td>0.022249</td>\n      <td>0.004044</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>11</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.965812</td>\n      <td>0.970085</td>\n      <td>0.957265</td>\n      <td>0.965665</td>\n      <td>0.957082</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.978541</td>\n      <td>0.957265</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974249</td>\n      <td>0.969957</td>\n      <td>0.965472</td>\n      <td>0.006516</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.021766</td>\n      <td>0.017920</td>\n      <td>0.036914</td>\n      <td>0.017205</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>16</td>\n      <td>30</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.974249</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.968037</td>\n      <td>0.006221</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.009202</td>\n      <td>0.002073</td>\n      <td>0.025025</td>\n      <td>0.004361</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>1</td>\n      <td>10</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.016096</td>\n      <td>0.006301</td>\n      <td>0.045065</td>\n      <td>0.014773</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.978541</td>\n      <td>0.974249</td>\n      <td>0.971747</td>\n      <td>0.009358</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.001900</td>\n      <td>0.003528</td>\n      <td>0.006909</td>\n      <td>0.005729</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>0.008667</td>\n      <td>0.004586</td>\n      <td>0.018095</td>\n      <td>0.006720</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>6</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.995708</td>\n      <td>0.982906</td>\n      <td>1.000000</td>\n      <td>0.995726</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.991453</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.982833</td>\n      <td>0.991416</td>\n      <td>0.985440</td>\n      <td>0.009099</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>0.006627</td>\n      <td>0.003939</td>\n      <td>0.028805</td>\n      <td>0.008777</td>\n      <td>uniform</td>\n      <td>1</td>\n      <td>6</td>\n      <td>30</td>\n      <td>ball_tree</td>\n      <td>{'weights': 'uniform', 'p': 1, 'n_neighbors': ...</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.965812</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.948498</td>\n      <td>0.982833</td>\n      <td>0.965812</td>\n      <td>0.961538</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.974249</td>\n      <td>0.972032</td>\n      <td>0.009238</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>0.003649</td>\n      <td>0.005844</td>\n      <td>0.018539</td>\n      <td>0.007179</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>16</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.970085</td>\n      <td>0.974249</td>\n      <td>0.965665</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.961538</td>\n      <td>0.961373</td>\n      <td>0.978541</td>\n      <td>0.961538</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.974249</td>\n      <td>0.961373</td>\n      <td>0.968323</td>\n      <td>0.006588</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>0.000285</td>\n      <td>0.000701</td>\n      <td>0.009491</td>\n      <td>0.002049</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>11</td>\n      <td>30</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.970085</td>\n      <td>0.982906</td>\n      <td>0.965812</td>\n      <td>0.978541</td>\n      <td>0.969957</td>\n      <td>0.974359</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.974359</td>\n      <td>0.987179</td>\n      <td>0.978541</td>\n      <td>0.978541</td>\n      <td>0.976883</td>\n      <td>0.006013</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>0.002158</td>\n      <td>0.003578</td>\n      <td>0.006375</td>\n      <td>0.004440</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>0.028219</td>\n      <td>0.007803</td>\n      <td>0.159026</td>\n      <td>0.025515</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>6</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.970085</td>\n      <td>0.965665</td>\n      <td>0.995708</td>\n      <td>0.978632</td>\n      <td>0.995726</td>\n      <td>0.974359</td>\n      <td>0.969957</td>\n      <td>0.978541</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.978632</td>\n      <td>0.974249</td>\n      <td>0.978541</td>\n      <td>0.979448</td>\n      <td>0.008604</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>0.026561</td>\n      <td>0.004729</td>\n      <td>0.094954</td>\n      <td>0.011838</td>\n      <td>distance</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>kd_tree</td>\n      <td>{'weights': 'distance', 'p': 2, 'n_neighbors':...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>0.000143</td>\n      <td>0.000514</td>\n      <td>0.013123</td>\n      <td>0.005524</td>\n      <td>uniform</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>brute</td>\n      <td>{'weights': 'uniform', 'p': 2, 'n_neighbors': ...</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>0.978632</td>\n      <td>0.978541</td>\n      <td>0.991416</td>\n      <td>0.978632</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.978541</td>\n      <td>0.987124</td>\n      <td>0.987179</td>\n      <td>0.987179</td>\n      <td>1.000000</td>\n      <td>0.991416</td>\n      <td>0.991416</td>\n      <td>0.988296</td>\n      <td>0.007410</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>0.000814</td>\n      <td>0.002066</td>\n      <td>0.014420</td>\n      <td>0.004176</td>\n      <td>distance</td>\n      <td>1</td>\n      <td>6</td>\n      <td>10</td>\n      <td>brute</td>\n      <td>{'weights': 'distance', 'p': 1, 'n_neighbors':...</td>\n      <td>0.982906</td>\n      <td>0.970085</td>\n      <td>0.974359</td>\n      <td>0.982833</td>\n      <td>0.982833</td>\n      <td>0.982906</td>\n      <td>0.991453</td>\n      <td>0.991453</td>\n      <td>0.957082</td>\n      <td>0.991416</td>\n      <td>0.965812</td>\n      <td>0.974359</td>\n      <td>0.978632</td>\n      <td>0.982833</td>\n      <td>0.987124</td>\n      <td>0.979739</td>\n      <td>0.009570</td>\n      <td>14</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "KNN_model = KNeighborsClassifier()\n",
    "\n",
    "KNN_params = dict()\n",
    "KNN_params['n_neighbors'] = [i for i in range(1, 20, 5)]\n",
    "KNN_params['weights'] = [\"uniform\", \"distance\"]\n",
    "KNN_params['algorithm'] = [\"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "KNN_params['leaf_size'] = [1, 10, 30]\n",
    "KNN_params['p'] = [1, 2]\n",
    "\n",
    "kFold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(KNN_model, KNN_params, n_iter=50, scoring='accuracy', n_jobs=-1, cv=kFold, random_state=1)\n",
    "\n",
    "KNN = RCV.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(KNN.cv_results_)\n",
    "\n",
    "# Stockage du meilleur résultat dans le tableau des résultats\n",
    "Evaluation_Results.iloc[4]['Best Mean Test Score'] = max(cv_results['mean_test_score'])\n",
    "\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                               Best Mean Test Score\nLogistic Regression (LR)                   0.972313\nNaïve Bayes Classifier (NB)                0.864722\nDecision Tree Classifier (DT)              0.834181\nGradient Boosting (GB)                     0.955759\nK Nearest Neighbours (KNN)                 0.988296",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Best Mean Test Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression (LR)</th>\n      <td>0.972313</td>\n    </tr>\n    <tr>\n      <th>Naïve Bayes Classifier (NB)</th>\n      <td>0.864722</td>\n    </tr>\n    <tr>\n      <th>Decision Tree Classifier (DT)</th>\n      <td>0.834181</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting (GB)</th>\n      <td>0.955759</td>\n    </tr>\n    <tr>\n      <th>K Nearest Neighbours (KNN)</th>\n      <td>0.988296</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(Evaluation_Results)\n",
    "df.plot(kind = 'bar')\n",
    "plt.show()\n",
    "\n",
    "Evaluation_Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Citation :\n",
    "Stephanie Glen. \"Regularization: Simple Definition, L1 & L2 Penalties\" From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/regularization/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}